[{"Title":"An Augmented Reality System with Advanced User Interfaces for Image-Guided Intervention Applications","DOI":"10.1117\/12.2653952","Publication year":2023,"Abstract":"Augmented Reality (AR) is becoming a more common addition to physicians' repertoire for aiding in resident training and patient interactions. However, the use of augmented reality in clinical settings is still beset with many complications, including the lack of physician control over the systems, set modes of interactions within the system, and physician's lack of familiarity with such AR systems. In this paper, we plan to expand on our previous prostate biopsy AR system by adding in improved user interface systems within the virtual world in order to allow the user to more accurately visualize only parts of the system which they consider to be useful at that time. To accomplish this, we have incorporated three-dimensional virtual sliders built from the ground up, using Unity to afford control over each model's RGB values, as well as their transparency. This means that the user would be able to fully edit the color, and transparency of each individual model in real time as they see fit quickly and easily while still being immersed in the augmented space. This would allow users to view internal holograms while not sacrificing the capability to view the external structure. Such leeway could be invaluable when visualizing a tumor within a prostate and would provide the physician with the capability to view as much or as little of the surrounded virtual models as desired, while providing the option to reinstate the surrounding models at will. The AR system can provide a new approach for potential uses in image-guided interventions including targeted biopsy of the prostate. &copy; 2023 SPIE.","LowLevel":"biopsy;transparency;urology;user interfaces","MidLevel":"graphics;human-computer interaction;medical","HighLevel":"end users and user experience;technology;industries","Venue":"Progr. Biomed. Opt. Imaging Proc. SPIE","Top20AbsAndTags":[432,45,79,54,444,276,175,267,138,174,43,118,296,340,208,463,139,323,142,317],"Top20Abs":[432,45,276,79,54,175,174,267,118,296,208,38,82,139,142,229,340,440,419,10],"Top20Tags":[43,138,470,432,169,74,453,170,207,323,488,431,285,317,223,464,463,332,444,201],"Citation":"Bettati, P., & Fei, B. (2023). An advanced system with advanced user interfaces for image-guided intervention applications. Medical Imaging 2023: Image-Guided Procedures, Robotic Interventions, and Modeling. https:\/\/doi.org\/10.1117\/12.2653952\n"},{"Title":"UnMapped: Leveraging Experts' Situated Experiences to Ease Remote Guidance in Collaborative Mixed Reality","DOI":"10.1145\/3544548.3581444","Publication year":2023,"Abstract":"Collaborative Mixed Reality (MR) systems that help extend expertise for physical tasks to remote environments often situate experts in an immersive view of the task environment to bring the collaboration closer to collocated settings. In this paper, we design UnMapped, an alternative interface for remote experts that combines a live 3D view of the active space within the novice's environment with a static 3D recreation of the expert's own workspace to leverage their existing spatial memories within it. We evaluate the impact of this approach on single and repeated use of collaborative MR systems for remote guidance through a comparative study. Our results indicate that despite having a limited understanding of the novice's environment, using an UnMapped interface increased performance and communication efficiency while reducing experts' task load. We also outline the various affordances of providing remote experts with a familiar and spatially-stable environment to assist novices.","LowLevel":"groupware","MidLevel":"collaboration","HighLevel":"use cases","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[366,419,104,440,203,69,365,234,304,54,360,397,329,411,224,189,418,239,131,133],"Top20Abs":[366,419,234,440,104,54,365,360,411,69,397,304,203,439,133,418,329,399,189,10],"Top20Tags":[224,29,17,304,203,279,131,199,239,104,267,69,337,329,125,240,193,429,219,164],"Citation":"Johnson, J. G., Sharkey, T., Butarbutar, I. C., Xiong, D., Huang, R., Sy, L., & Weibel, N. (2023). UnMapped: Leveraging Experts\u2019 Situated Experiences to Ease Remote Guidance in Collaborative Mixed Reality. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581444\n"},{"Title":"HMDspeller: Fast and Hands-free Text Entry System for Head Mount Displays using Silent Spelling Recognition","DOI":"10.1145\/3544549.3583910","Publication year":2023,"Abstract":"As virtual reality and augmented reality technology become more popular in office and communication contexts, the need for simple and efficient text input methods for these devices becomes more evident. We propose the use of an HMDspeller, which allows for text input at a speed of more than 30 words per minute without the need of using hands. Our method utilizes a technique called \"silent spelling,\" which serves as a compromise between text entry and silent speech. The HMDspeller uses an infrared camera to capture the user's silent spelling and decode the words, achieving a character error rate of 13.9% for seen words and 26.6% for unseen words without using language models or dictionaries.","LowLevel":"helmet mounted displays;natural language processing;text analysis","MidLevel":"artificial intelligence;data;wearables;input;display technology","HighLevel":"displays;technology","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[400,75,353,348,485,249,203,208,363,48,419,3,309,411,79,183,259,80,424,77],"Top20Abs":[400,75,348,353,249,485,203,363,208,48,3,183,419,77,367,200,80,79,46,219],"Top20Tags":[394,352,75,353,238,422,455,424,301,400,80,167,440,419,21,32,165,126,224,206],"Citation":"Asano, K., Kimura, N., & Rekimoto, J. (2023). HMDspeller: Fast and Hands-free Text Entry System for Head Mount Displays using Silent Spelling Recognition. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583910\n"},{"Title":"Educational Robotics in Teaching Programming in Primary School","DOI":"10.1007\/978-981-19-9876-8_51","Publication year":2023,"Abstract":"Learning at Slovak primary schools in the last school years takes place in a combined form due to the COVID-19 pandemic. With the distance form of teaching, it was possible to reduce the teaching hours and omit some parts of the study plan. Parts of the subjects often omitted were tied to practical lessons in special facilities, possibly to the use of specific equipment or tools, such as laboratory exercises and experiments, building and construction of robots, programming and physical education. Teachers opted the path of least resistance and often preferred to skip such topics. According to the state educational program in elementary school, it is necessary to develop algorithmic and programming thinking. For this purpose, thematic units focused on algorithmic problem solving and programming are used. The article brings the experience of the authors from the programming of robots in elementary school, which can be equally successfully implemented face-to-face as well as remotely using real or virtual robots. To revive learning and increase its effectiveness, the authors used modern teaching aid and digital educational technologies, which have also proven themselves in the distance form of education. We used virtual and augmented reality, worked in remote laboratories and used implementation of laboratory experiments using visualized simulation models and environments and emulation techniques. The authors report on their experience of teaching programming in primary schools using programmable toys, robots and microcontrollers. Simple tools (buttons marked with symbols) and an interactive environment that offers block or icon programming are used to program the movement and control the activities of such objects&mdash;robot toys and own constructed robots. Compiling a functional program does not require high analytical and abstract thinking. The program can be easily and interactively assembled from the offered building elements. This method makes programming more fun for elementary school students and ensures their rapid progress and the joy of success. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"abstracting;construction equipment;controllers;educational robots;microcontrollers","MidLevel":"education;video;robotics;construction;input;human-computer interaction","HighLevel":"industries;technology;end users and user experience","Venue":"Lect. Notes Electr. Eng.","Top20AbsAndTags":[185,174,19,342,196,414,232,410,320,128,136,163,161,369,184,241,56,154,454,207],"Top20Abs":[185,174,19,342,414,196,410,232,128,163,241,320,136,184,161,88,369,457,454,56],"Top20Tags":[435,291,307,487,361,247,290,369,482,161,466,18,468,192,393,433,463,128,2,188],"Citation":"Stoffov\u00e1, V., & Zboran, M. (2023). Educational Robotics in Teaching Programming in Primary School. Proceedings of International Conference on Recent Innovations in Computing, 669\u2013682. https:\/\/doi.org\/10.1007\/978-981-19-9876-8_51\n"},{"Title":"A petri net architecture for real-time human activity recognition in work systems","DOI":"10.1016\/j.procs.2022.12.317","Publication year":2023,"Abstract":"Real-time human-centered assistance in industrial processes depends on the individual history of the work person's activities in the work system and requires adequate methods for tracking the person's actions. Most research in human activity recognition is based on recognizing actions from video data using computer vision methods. Digital equipment, standardized machine data interfaces, and smart wearable devices extend the possibilities to describe the current state of the work system. Petri nets have already been applied to human activity recognition, however, without the requirement of detecting actions in real-time. This paper proposes a Petri net architecture that enables hierarchical description-based human activity recognition in industrial work processes. We present an extension, a Partitioned Colored Petri Net, based on the colored Petri net formalism that infers activities from state transitions of the work system in real-time. In a case study, we demonstrate the Petri net's application for an error-based learning system that visualizes error consequences in augmented reality using experimentable digital twins. All rights reserved Elsevier.","LowLevel":"computer vision;digital twins;human activity recognition;petri nets","MidLevel":"smart cities;computer vision;human factors;other","HighLevel":"end users and user experience;technology;use cases;other","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[12,387,23,182,45,174,205,352,157,84,412,486,122,144,411,54,451,137,179,452],"Top20Abs":[12,387,205,174,23,352,45,157,182,486,84,412,122,180,137,452,370,178,54,144],"Top20Tags":[407,411,53,220,412,374,115,28,122,110,377,180,198,190,310,212,182,266,387,362],"Citation":"Herrmann, J.-P., Atanasyan, A., Casser, F., & Tackenberg, S. (2023). A Petri Net Architecture for Real-Time Human Activity Recognition in Work Systems. Procedia Computer Science, 217, 1188\u20131199. https:\/\/doi.org\/10.1016\/j.procs.2022.12.317\n"},{"Title":"Build a Smart Sustainable Windhoek: An AR game","DOI":"10.1145\/3544549.3583841","Publication year":2023,"Abstract":"The City of Windhoek is dedicated to become a Smart, Sustainable City, in alignment with sustainable development goal 11. Local officials strive to include residents to guide smart and sustainable initiatives and strategies, but lack tools and techniques to engage those unfamiliar with smart city concepts. We proposed a Build a Smart Sustainable Windhoek game, which educates and raises awareness while sharing citizen needs and desires to inform smart and sustainable city strategies. It is an interactive hybrid game combining physical interactions with boxes and augmented reality. The game was co-designed with unemployed youth from an informal settlement in the capital. It was played by 8 teams, each consisting of three Windhoek residents from different backgrounds, during a half-day smart city event. The survey results show that participants enjoyed the role of a town planner and building their own city having been teamed up with fellow residents from different suburbs. Moreover, they considered the game to be most informative in creating awareness on Smart City topics.","LowLevel":"smart cities;sustainable development;town and country planning","MidLevel":"smart cities;business planning and management;policy;liberal arts","HighLevel":"industries;business;use cases","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[89,376,236,38,224,373,412,111,83,25,79,113,352,164,423,26,389,281,282,269],"Top20Abs":[89,376,224,38,373,236,111,412,352,113,83,79,281,25,26,389,164,282,269,261],"Top20Tags":[418,412,236,410,374,38,67,377,22,164,14,204,373,407,147,311,266,97,129,111],"Citation":"Makosa, I., Nuunyango, C., & Uchezuba, K. C. (2023). Build a Smart Sustainable Windhoek: An AR game. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583841\n"},{"Title":"StandARone: Infrared-Watermarked Documents as Portable Containers of AR Interaction and Personalization","DOI":"10.1145\/3544549.3585905","Publication year":2023,"Abstract":"Hybrid paper interfaces leverage augmented reality (AR) to combine the desired tangibility of paper documents with the affordances of interactive digital media. Typically, the instructions for how the virtual content should be generated are not an intrinsic part of the document but rather accessed through a link to remote resources. To enable hybrid documents to be portable containers of also the AR content, we introduce StandARone documents. Using our system, a document author can define AR content and embed it invisibly on the document using a standard inkjet printer and infrared-absorbing ink. A document consumer can interact with the embedded content using a smartphone with a NIR camera without requiring a network connection. We demonstrate several use cases of StandARone including personalized offline menus, interactive visualizations, and location-aware packaging.","LowLevel":"data visualization;image watermarking;smartphones;user interfaces;watermarking","MidLevel":"telecommunication;data;liberal arts;human-computer interaction;other","HighLevel":"end users and user experience;technology;industries;other","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[365,8,267,419,153,248,53,178,302,263,209,424,123,48,173,27,77,121,4,82],"Top20Abs":[365,8,153,267,419,429,123,302,53,263,209,178,275,229,155,303,48,231,77,135],"Top20Tags":[221,339,424,129,248,195,82,93,105,419,27,348,410,42,72,101,376,216,365,36],"Citation":"Dogan, M. D., Siu, A. F., Healey, J., Wigington, C., Xiao, C., & Sun, T. (2023). StandARone: Infrared-Watermarked Documents as Portable Containers of AR Interaction and Personalization. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585905\n"},{"Title":"When XR and AI Meet - A Scoping Review on Extended Reality and Artificial Intelligence","DOI":"10.1145\/3544548.3581072","Publication year":2023,"Abstract":"Research on Extended Reality (XR) and Artificial Intelligence (AI) is booming, which has led to an emerging body of literature in their intersection. However, the main topics in this intersection are unclear, as are the benefits of combining XR and AI. This paper presents a scoping review that highlights how XR is applied in AI research and vice versa. We screened 2619 publications from 203 international venues published between 2017 and 2021, followed by an in-depth review of 311 papers. Based on our review, we identify five main topics at the intersection of XR and AI, showing how research at the intersection can benefit each other. Furthermore, we present a list of commonly used datasets, software, libraries, and models to help researchers interested in this intersection. Finally, we present 13 research opportunities and recommendations for future work in XR and AI research.","LowLevel":"artificial intelligence","MidLevel":"artificial intelligence;liberal arts","HighLevel":"industries;technology","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[314,391,67,257,327,306,58,305,193,353,396,251,117,296,359,168,389,372,237,289],"Top20Abs":[314,67,257,327,306,391,193,396,353,305,58,168,359,389,251,278,372,279,376,281],"Top20Tags":[391,18,58,79,251,339,254,372,353,289,273,160,368,352,296,95,415,326,428,378],"Citation":"Hirzle, T., M\u00fcller, F., Draxler, F., Schmitz, M., Knierim, P., & Hornb\u00e6k, K. (2023). When XR and AI Meet - A Scoping Review on Extended Reality and Artificial Intelligence. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581072\n"},{"Title":"A First-Priority Set of&nbsp;Telepresence Services and&nbsp;a&nbsp;Model Network for&nbsp;Research and&nbsp;Education","DOI":"10.1007\/978-3-031-30648-8_17","Publication year":2023,"Abstract":"A first-priority set of telepresence services is proposed, and the delay requirements and fault probabilities for these services are defined. The end-to-end latency and reliability requirements are derived from analysis of ITU-T, 3GPP, ETSI standards and recommendations. The characteristics of a next-generation model network for research and education in the field of telepresence services are discussed. The model network is based on a DWDM core, a variety of server equipment, holographic fans, 3D cameras and projectors, avatar robots and multifunctional robots, and augmented reality terminal devices. The results of the first tests on the model network are presented. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"holography;mobile telecommunication systems;reliability analysis;three dimensional computer graphics","MidLevel":"education;telecommunication;data;inspection, safety and quality;graphics;input;display technology","HighLevel":"displays;technology;industries;use cases","Venue":"Commun. Comput. Info. Sci.","Top20AbsAndTags":[157,239,447,335,268,458,454,467,457,427,269,312,443,265,381,278,486,322,355,1],"Top20Abs":[335,268,458,447,427,269,454,239,355,157,467,426,19,1,265,312,453,448,404,486],"Top20Tags":[457,157,312,488,443,433,183,464,278,413,438,446,291,290,475,467,74,435,283,431],"Citation":"Koucheryavy, A. E., Makolkina, M. A., Paramonov, A. I., Vybornova, A. I., Muthanna, A. S. A., Dunaytsev, R. A., Vladimirov, S. S., Elagin, V. S., Markelov, O. A., Vorozheykina, O. I., Marochkina, A. V., Gorbacheva, L. S., Pankov, B. O., & Anvarzhonov, B. N. (2023). A First-Priority Set of\u00a0Telepresence Services and\u00a0a\u00a0Model Network for\u00a0Research and\u00a0Education. Distributed Computer and Communication Networks, 208\u2013219. https:\/\/doi.org\/10.1007\/978-3-031-30648-8_17\n"},{"Title":"Experiences of web-based extended reality technologies for physics education","DOI":"10.1002\/cae.22571","Publication year":2023,"Abstract":"Extended reality (XR) technologies such as augmented reality (AR) and virtual reality (VR) are being increasingly used for education and skill development through the development of interactive learning environments (ILEs) with XR implementations. For physics education, these ILEs with XR are capable of improving the visual representation of educational content, as well as stimulating the cognitive process of learning with interactive experiences. In this study, we focused on evaluating students' perceptions of the usage of web-based XR technologies for education, through a learning tool that implements virtual reality and augmented reality environments, with the purpose of providing students the necessary didactic material to learn physics. An experimental process was carried out with a sample of 70 undergrad students who used FisicARtivo, a web-based learning tool on kinematics and dynamics concepts that incorporates XR. After working with the tool, students completed a comprehensive survey about their motivational perceptions regarding the use of XR technologies for physics education. According to the results, both VR and AR showed significant effects on motivation. However, there was a higher positive impact on students' learning motivation when they used FisicARtivo in AR mode in comparison to VR mode. &#169; 2023 Wiley Periodicals LLC.","LowLevel":"cognition;computer aided instruction;human factors;physics education","MidLevel":"education;human factors;training;engineering","HighLevel":"industries;technology;use cases;end users and user experience","Venue":"Comput. Appl. Eng. Educ. (USA)","Top20AbsAndTags":[327,163,87,320,296,114,128,314,369,244,117,257,57,88,124,82,308,134,185,46],"Top20Abs":[327,163,87,88,314,369,257,128,82,320,46,185,124,114,244,363,62,67,391,136],"Top20Tags":[52,89,342,244,145,128,87,196,154,117,204,114,33,147,195,51,57,320,339,62],"Citation":"Zatarain\u2010Cabada, R., Barr\u00f3n\u2010Estrada, M. L., C\u00e1rdenas\u2010Sainz, B. A., & Chavez\u2010Echeagaray, M. E. (2022). Experiences of web\u2010based extended reality technologies for physics education. Computer Applications in Engineering Education, 31(1), 63\u201382. Portico. https:\/\/doi.org\/10.1002\/cae.22571\n"},{"Title":"Memory Manipulations in Extended Reality","DOI":"10.1145\/3544548.3580988","Publication year":2023,"Abstract":"Human memory has notable limitations (e.g., forgetting) which have necessitated a variety of memory aids (e.g., calendars). As we grow closer to mass adoption of everyday Extended Reality (XR), which is frequently leveraging perceptual limitations (e.g., redirected walking), it becomes pertinent to consider how XR could leverage memory limitations (forgetting, distorting, persistence) to induce memory manipulations. As memories highly impact our self-perception, social interactions, and behaviors, there is a pressing need to understand XR Memory Manipulations (XRMMs). We ran three speculative design workshops (n=12), with XR and memory researchers creating 48 XRMM scenarios. Through thematic analysis, we define XRMMs, present a framework of their core components and reveal three classes (at encoding, pre-retrieval, at retrieval). Each class differs in terms of technology (AR, VR) and impact on memory (influencing quality of memories, inducing forgetting, distorting memories). We raise ethical concerns and discuss opportunities of perceptual and memory manipulations in XR.","LowLevel":"ethical aspects","MidLevel":"policy","HighLevel":"business","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[314,306,257,308,327,195,67,130,117,237,303,396,391,399,248,193,349,296,447,389],"Top20Abs":[306,257,327,314,130,67,195,308,248,396,349,303,399,193,237,117,447,391,389,62],"Top20Tags":[78,67,314,308,459,236,202,195,391,204,410,257,396,389,354,100,104,33,310,372],"Citation":"Bonnail, E., Tseng, W.-J., Mcgill, M., Lecolinet, E., Huron, S., & Gugenheimer, J. (2023). Memory Manipulations in Extended Reality. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580988\n"},{"Title":"Partially Blended Realities: Aligning Dissimilar Spaces for Distributed Mixed Reality Meetings","DOI":"10.1145\/3544548.3581515","Publication year":2023,"Abstract":"Mixed Reality allows for distributed meetings where people's local physical spaces are virtually aligned into blended interaction spaces. In many cases, people's physical rooms are dissimilar, making it challenging to design a coherent blended space. We introduce the concept of Partially Blended Realities (PBR) - using Mixed Reality to support remote collaborators in partially aligning their physical spaces. As physical surfaces are central in collaborative work, PBR supports users in transitioning between different configurations of tables and whiteboard surfaces. In this paper, we 1) describe the design space of PBR, 2) present RealityBlender to explore interaction techniques for how users may configure and transition between blended spaces, and 3) provide insights from a study on how users experience transitions in a remote collaboration task. With this work, we demonstrate new potential for using partial solutions to tackle the alignment problem of dissimilar spaces in distributed Mixed Reality meetings.","LowLevel":"groupware","MidLevel":"collaboration","HighLevel":"use cases","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[203,64,440,279,239,197,303,199,27,224,248,267,366,240,80,126,208,365,69,15],"Top20Abs":[64,203,440,197,279,239,27,199,303,248,411,224,80,208,365,366,419,126,424,118],"Top20Tags":[303,29,17,203,279,314,131,199,239,104,267,69,337,329,125,240,365,126,224,193],"Citation":"Gr\u00f8nb\u00e6k, J. E. S., Pfeuffer, K., Velloso, E., Astrup, M., Pedersen, M. I. S., Kj\u00e6r, M., Leiva, G., & Gellersen, H. (2023). Partially Blended Realities: Aligning Dissimilar Spaces for Distributed Mixed Reality Meetings. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581515\n"},{"Title":"Serverless Functions in the Cloud-Edge Continuum: Challenges and Opportunities","DOI":"10.1109\/PDP59025.2023.00056","Publication year":2023,"Abstract":"The Function-as-a-Service (FaaS) paradigm is increasingly adopted for the development of Cloud-native applications, which especially benefit from the seamless scalability and attractive pricing models of serverless deployments. With the continuous emergence of latency-sensitive applications and services, including Internet-of-Things and augmented reality, it is now natural to wonder whether and how the FaaS paradigm can be efficiently exploited in the Cloud-Edge Continuum, where serverless functions may benefit from reduced network delay between their invoking users and the FaaS platform. In this paper, we illustrate the key challenges that must be faced to effectively deploy serverless functions in the Cloud-Edge Continuum and review recent contributions proposed by the research community towards overcoming those challenges. We also discuss the key issues that currently remain unsolved and highlight a few research opportunities for better support of FaaS in the Compute Continuum.","LowLevel":"cloud computing;internet;internet of things","MidLevel":"internet of things;networks","HighLevel":"technology","Venue":"2023 31st Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)","Top20AbsAndTags":[427,5,271,448,451,159,350,268,315,335,395,359,447,92,289,111,346,110,8,429],"Top20Abs":[335,427,448,271,268,5,346,307,350,451,447,355,159,203,111,92,395,279,315,426],"Top20Tags":[271,427,258,129,5,123,315,289,395,159,175,359,38,220,110,286,121,378,266,31],"Citation":"Russo, G. R., Cardellini, V., & Presti, F. L. (2023). Serverless Functions in the Cloud-Edge Continuum: Challenges and Opportunities. 2023 31st Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP). https:\/\/doi.org\/10.1109\/pdp59025.2023.00056\n"},{"Title":"Digital Transformation, Applications, and Vulnerabilities in Maritime and Shipbuilding Ecosystems","DOI":"10.1016\/j.procs.2022.12.338","Publication year":2023,"Abstract":"The evolution of maritime and shipbuilding supply chains toward digital ecosystems increases operational complexity and needs reliable communication and coordination. As labor and suppliers shift to digital platforms, interconnection, information transparency, and decentralized choices become ubiquitous. In this sense, Industry 4.0 enables \"smart digitalization\" in these environments. Many applications exist in two distinct but interrelated areas related to shipbuilding design and shipyard operational performance. New digital tools, such as virtual prototypes and augmented reality, begin to be used in the design phases, during the commissioning\/quality control activities, and for training workers and crews. An application relates to using Virtual Prototypes and Augmented Reality during all the design and construction phases. Another application relates to the cybersecurity protection of operational networks that support shipbuilding supply chains that ensures the flow of material and labor to the shipyards. This protection requires a holistic approach to evaluate their vulnerability and understand ripple effects. This paper presents the applications of Industry 4.0 for the areas mentioned above. The first case in shipbuilding design is an example of how the virtual prototype of a ship, together with wearable devices enabling augmented reality, can be used for the quality control of the construction of ship systems. For the second case, we propose developing an artificial intelligence-based cybersecurity supply network framework that characterizes and monitors shipbuilding supply networks and determines ripple effects from disruptions caused by cyberattacks. This framework extends a novel risk management framework developed by Diaz and Smith and Smith and Diaz that considers complex tiered networks. All rights reserved Elsevier.","LowLevel":"artificial intelligence;computer crime;design engineering;industrial training;marine engineering;personnel;production engineering computing;quality control;risk management;shipbuilding industry;supply chain management","MidLevel":"training;artificial intelligence;human resources;engineering;farming and natural science;marine;inspection, safety and quality;security;liberal arts;manufacturing;human-computer interaction;logistics","HighLevel":"end users and user experience;use cases;business;industries;technology","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[378,334,2,268,447,436,179,44,12,412,456,414,458,181,377,45,289,35,431,159],"Top20Abs":[378,2,268,35,44,458,447,414,456,86,146,334,181,436,179,431,395,377,224,412],"Top20Tags":[378,308,289,54,388,18,482,58,370,360,334,84,94,412,352,339,377,205,159,179],"Citation":"Diaz, R., Smith, K., Bertagna, S., & Bucci, V. (2023). Digital Transformation, Applications, and Vulnerabilities in Maritime and Shipbuilding Ecosystems. Procedia Computer Science, 217, 1396\u20131405. https:\/\/doi.org\/10.1016\/j.procs.2022.12.338\n"},{"Title":"Unreality- Tangible learning anytime, anywhere","DOI":"10.1145\/3569009.3576221","Publication year":2023,"Abstract":"This article intends to explain the project \"Unreality\", which is a system developed to make learning remotely and physically more tangibly, socially, and emotionally efficient. Using Augmented Reality (AR) and Extended Reality (XR), interactive holograms will be introduced, and will enable the remotely and physically present students to interact as they normally would. Emotion Tree and Emotion bracelet are designed to encourage more discussions pertaining to their emotional health. Lastly, the Note-Making system takes into account the different learning styles of every student who may not thrive under the traditional learning techniques. The use of Unreality application to input the required data increases ease of access. The report would also explain each element used in the system and its process of how they would be used while learning.","LowLevel":"computer aided instruction;learning algorithms","MidLevel":"training;artificial intelligence;medical","HighLevel":"industries;technology;use cases","Venue":"TEI '23: Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction","Top20AbsAndTags":[163,306,87,244,39,82,320,262,369,257,114,57,124,207,134,145,352,150,70,314],"Top20Abs":[163,87,306,39,244,82,369,150,262,114,207,320,70,124,0,352,77,145,184,128],"Top20Tags":[308,93,391,9,376,3,82,204,163,320,117,115,164,253,222,262,56,339,410,89],"Citation":"Thomas, C., Koshy, A., Samal, A., & Hanspal, A. K. (2023). Unreality\u2013 Tangible learning anytime, anywhere. Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction. https:\/\/doi.org\/10.1145\/3569009.3576221\n"},{"Title":"Large-Area Scatterometry for Nanoscale Metrology","DOI":"10.1117\/12.2649342","Publication year":2023,"Abstract":"Many applications across photonics and semiconductor industries require the fabrication of nanostructures with non-trivial geometries with a precision and reproducibility down to the nanometer scale. Slanted gratings and metamaterials are examples of such designs that have vast applications in Augmented Reality and LiDAR. State-of-the-art lithography techniques, such as nanoimprint lithography or UV lithography, can provide such levels of fabrication precision for high-volume production. However, a rapid in-line quality inspection method for such complex patterns is required to monitor the fabrication process, verify the sample quality, and to ensure reproducibility. Here, we demonstrate a novel technique that allows us to inspect the quality of the samples in a non-destructive and fast manner, and to extract geometrical parameters of the nanostructures over large areas, generating spatial variations maps across wafers. &copy; 2023 SPIE.","LowLevel":"geometry;nanoimprint lithography;semiconductor device manufacture","MidLevel":"graphics;chemical;semiconductors;manufacturing;other","HighLevel":"industries;technology;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[407,317,444,12,475,158,413,99,300,247,2,235,270,345,194,434,223,370,361,290],"Top20Abs":[407,12,317,444,413,300,345,259,364,99,223,235,194,331,322,270,285,227,273,158],"Top20Tags":[466,475,212,332,481,463,141,461,194,344,361,477,223,187,137,35,247,169,158,434],"Citation":"G\u00f3mez Rivas, J., Ramezani, M., Verschuuren, M. A., & Castellanos Gonzalez, G. (2023). Large-area scatterometry for nanoscale metrology. Advanced Fabrication Technologies for Micro\/Nano Optics and Photonics XVI. https:\/\/doi.org\/10.1117\/12.2649342\n"},{"Title":"Designing Immersive, Narrative-Based Interfaces to Guide Outdoor Learning","DOI":"10.1145\/3544548.3581365","Publication year":2023,"Abstract":"Outdoor learning experiences, such as field trips, can improve children's science achievement and engagement, but these experiences are often difficult to deliver without extensive support. Narrative in educational experiences can provide needed structure, while also increasing engagement. We created a narrative-based, mobile application to investigate how to guide young learners in interacting with their local, outdoor environment. In a second variant, we added augmented reality and image classification to explore the value of these features. A study (n = 44) found that participants using our system demonstrated learning gains and found the experience engaging. Our findings identified several major themes, including participant excitement for hands-on interactions with nature, curiosity about the characters, and enthusiasm toward typing their thoughts and observations. We offer a set of design implications for supporting narrative-based, outdoor learning with immersive technology.","LowLevel":"computer aided instruction;image classification","MidLevel":"training;computer vision","HighLevel":"technology;use cases","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[14,115,6,229,20,83,114,163,400,180,320,383,57,442,136,16,219,79,125,88],"Top20Abs":[115,14,20,83,6,163,229,114,88,400,180,442,383,136,320,369,188,79,440,87],"Top20Tags":[34,204,33,387,405,9,47,172,139,115,255,145,480,196,93,154,147,114,331,367],"Citation":"Cheng, A. Y., Ritchie, J., Agrawal, N., Childs, E., DeVeaux, C., Jee, Y., Leon, T., Maples, B., Cuadra, A., & Landay, J. A. (2023). Designing Immersive, Narrative-Based Interfaces to Guide Outdoor Learning. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581365\n"},{"Title":"Robotics and sensing technologies in red meat processing: A review","DOI":"10.1016\/j.tifs.2023.05.015","Publication year":2023,"Abstract":"Background: The red meat processing industry has a harsh work environment where tasks performed in abattoirs are physically and mentally demanding. In addition, the high financial costs associated with employing skilled labour, the shortage of such workers, and the rise in worldwide meat consumption, there has been a growing push towards integrating automation as a potential solution for the industry. Scope and approach: This paper describes the complexities of implementing robotics technology in red meat processing. The complexity when processing deformable natural meat mediums is significantly sensitive to the variations of workpieces caused by mechanical properties, physical shape and the position of tissues. These differences hinder conventional robotic systems from succeeding. Experimental and commercial robotic systems in red meat processing are shown to perform cutting tasks in the deboning room, whose systems capabilities are limited by executing cuts requiring little to no adaptability during the process. The review shows that X-ray, optical probes, and ultrasonic are the most effective sensing technologies in determining the cutting trajectories prior to the task. Some experimental systems utilised tactile sensing to follow more complex cutting paths but have not yet produced a commercially viable product. The evaluation of these sensing technologies' applicability to guide a robotic system in real-time is critical to tackling more complex cuts. Key findings and conclusions: A combination of preoperative scanning and real-time perception for adaptive control is recommended to automate tasks in red meat cutting. Also, it is recommended that to fully automate the meat cutting process, a gradual approach should be taken by shifting abattoirs by first utilising assistive technologies such as cobots, exoskeletons augmented reality, and virtual reality. &copy; 2023 The Authors","LowLevel":"cutting;exoskeleton;meats","MidLevel":"manufacturing;robotics;other","HighLevel":"industries;technology;other","Venue":"Trends Food Sci. Technol.","Top20AbsAndTags":[343,401,76,463,70,2,407,233,138,361,101,371,174,179,360,109,370,269,20,45],"Top20Abs":[401,76,343,70,463,371,407,101,2,233,179,158,360,269,174,370,107,261,45,20],"Top20Tags":[361,312,435,487,307,393,463,441,247,187,2,459,177,407,24,137,109,343,1,344],"Citation":"Aly, B. A., Low, T., Long, D., Baillie, C., & Brett, P. (2023). Robotics and sensing technologies in red meat processing: A review. Trends in Food Science &amp; Technology, 137, 142\u2013155. https:\/\/doi.org\/10.1016\/j.tifs.2023.05.015\n"},{"Title":"Optical Image Processing of 2-D and 3-D Objects using Digital Holography","DOI":"10.1117\/12.2660594","Publication year":2023,"Abstract":"Traditional computer vision (CV) approaches are the norm when attempting to extract edge information from an imaged object. These discrete approaches are almost always performed on 2D intensity imagery and at times can be computationally expensive depending on the algorithm. Digital holography (DH) provides access to the 3D object information. By manipulation of the Fourier transform of the hologram, which is also needed for isolating the real or virtual image in off-axis DH, edge information can be extracted by high pass spatial filtering of the pertinent cropped and centered spectrum. We show simple simulations utilizing 2-D and 3-D objects to show edge enhancement qualities using this approach, and compare its performance to conventional CV techniques. The same technique can be used to perform other image processing functions, such as image sharpening, blurring, and others so long as the correct filters are applied. Developments in ultra-high definition displays have either incorporated DH or have overlapping areas of interest currently, including 3-D television and augmented reality. &copy; 2023 SPIE.","LowLevel":"geometrical optics;holograms;information filtering","MidLevel":"graphics;developers;optics","HighLevel":"displays;technology","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[20,138,413,278,76,438,437,335,201,74,235,300,265,158,461,487,446,432,444,228],"Top20Abs":[20,76,300,335,278,437,461,487,194,271,438,432,265,201,413,189,480,138,435,447],"Top20Tags":[149,74,482,438,223,169,431,201,138,228,317,413,441,344,307,158,235,0,437,444],"Citation":"Smith, E., & Banerjee, P. P. (2023). Optical image processing of 2-D and 3-D objects using digital holography. Practical Holography XXXVII: Displays, Materials, and Applications. https:\/\/doi.org\/10.1117\/12.2660594\n"},{"Title":"Intraoperative zoom lens calibration for high magnification surgical microscope","DOI":"10.1016\/j.cmpb.2023.107618","Publication year":2023,"Abstract":"Background and objectives: An augmented reality (AR)-based surgical guidance system is often used with high-magnification zoom lens systems such as a surgical microscope, particularly in neurology or otolaryngology. To superimpose the internal structures of relevant organs on the microscopy image, an accurate calibration process to obtain the camera intrinsic and hand&ndash;eye parameters of the microscope is essential. However, conventional calibration methods are unsuitable for surgical microscopes because of their narrow depth of focus at high magnifications. To realize AR-based surgical guidance with a high-magnification surgical microscope, we herein propose a new calibration method that is applicable to the highest magnification levels as well as low magnifications. Methods: The key idea of the proposed method is to find the relationship between the focal length and the hand&ndash;eye parameters, which remains constant regardless of the magnification level. Based on this, even if the magnification changes arbitrarily during surgery, the intrinsic and hand&ndash;eye parameters are recalculated quickly and accurately with one or two pictures of the pattern. We also developed a dedicated calibration tool with a prism to take focused pattern images without interfering with the surgery. Results: The proposed calibration method ensured an AR error of &lt; 1 mm for all magnification levels. In addition, the variation of focal length was within 1% regardless of the magnification level, and the corresponding variation with the conventional calibration method exceeded 20% at high magnification levels. Conclusions: The comparative study showed that the proposed method has outstanding accuracy and reproducibility for a high-magnification surgical microscope. The proposed calibration method is applicable to various endoscope or microscope systems with zoom lens. &copy; 2023 Elsevier B.V.","LowLevel":"microscopes;prisms;surgery","MidLevel":"medical;other","HighLevel":"industries;other","Venue":"Comput. Methods Programs Biomed.","Top20AbsAndTags":[433,107,488,437,326,71,294,152,170,43,69,356,153,270,464,123,2,173,463,430],"Top20Abs":[433,107,488,437,326,71,294,152,356,43,170,123,69,270,2,295,430,345,153,393],"Top20Tags":[437,138,488,0,464,207,445,317,463,432,169,43,170,453,74,433,430,223,442,431],"Citation":"Jeung, D., Choi, H., Ha, H.-G., Oh, S.-H., & Hong, J. (2023). Intraoperative zoom lens calibration for high magnification surgical microscope. Computer Methods and Programs in Biomedicine, 238, 107618. https:\/\/doi.org\/10.1016\/j.cmpb.2023.107618\n"},{"Title":"Interactive multi-sensory and volumetric content integration for music education applications","DOI":"10.1007\/s11042-022-12314-3","Publication year":2023,"Abstract":"Taiwan's heritage in terms of the local music culture has been gradually fading in recent years. Thus, boosting and passing on the local music culture to pre-school and elementary school students are urgent requirements. This study proposes an interactive integration of multi-sensory and volumetric content for music education in Taiwan into applications for children. Further, the study introduces a technological multi-sensory pop-up sketch book created in collaboration with the National Taiwan Symphony Orchestra (NTSO) and Industrial Technology Research Institute (ITRI). Both organizations collaborate to integrate emerging media technologies, including augmented reality (AR) and volumetric capture for content production, and creative music teaching methods, derived from traditional pop-up sketch books. The final product featured 3D animated videos to achieve interactive learning with digital formation additional to the real worlds. This AR multi-sensory pop-up sketch book utilizes advanced volumetric capture technology to capture the motions of main actors in vivid 3D animation. Besides, modularized pop-up cards of musical instruments provide a haptic experience to complement the story. The book targets children aged between 3 and 12 years. An actual reading survey was conducted on 497 students from five kindergartens in Taiwan. Satisfaction with the book was rated using a five-rank scale. The cluster random sampling method was used for data analysis. Results of t-test produced an average score of 4.9980, which indicated that the target audience ranked the book with high levels of satisfaction. The results also confirmed that the story line was familiar among young children within the target age range. Additionally, using digital audio-visual augmented reality technology will be conducive to young children in terms of acceptance and recognition of traditional music culture. 3D animations of famous intellectual property characters aroused the interest of readers in learning about music through interactive contents. Moreover, this study provided evidence that the STEAM education model, which represents a cross-curricular approach that integrates Science, Technology, Engineering, Art, and Mathematics, is applicable to the inheritance and development of the local music culture.","LowLevel":"computer aided instruction;computer animation;data analysis;educational institutions;multimedia computing;music;musical instruments;sampling methods;stem;teaching","MidLevel":"education;training;audio;data;graphics","HighLevel":"industries;technology;use cases","Venue":"Multimed. Tools Appl. (Germany)","Top20AbsAndTags":[192,275,337,136,375,115,105,180,83,185,127,39,320,188,163,239,134,467,87,421],"Top20Abs":[192,275,136,115,337,180,105,375,83,188,163,39,239,185,421,77,467,16,320,88],"Top20Tags":[127,39,337,287,145,275,4,405,6,375,89,106,3,136,250,262,229,33,56,82],"Citation":"Ho, C.-L., Lin, T.-G., & Chang, C.-R. (2022). Interactive multi-sensory and volumetric content integration for music education applications. Multimedia Tools and Applications, 82(4), 4847\u20134862. https:\/\/doi.org\/10.1007\/s11042-022-12314-3\n"},{"Title":"CNN - Forest Based Person Identification and Head Pose Estimation for AI Based Applications","DOI":"10.1142\/S0218488523400044","Publication year":2023,"Abstract":"Face recognition and head posture estimation have aroused a lot of academic interest recently since the inherent information improves the performance of face-related applications such as face alignment, augmented reality, healthcare applications, and emotion detection. The proposed work explores the challenges of identifying people and determining head posture. An analysis of the features produced at intermediate layers by limiting the number of kernals is performed and improved the performance of detecting the person. In addition, the learned features are sent to forest trees in order to determine the exact head attitude of the detected person. The proposed Forest CNN (FCNN) architecture is tested for head pose estimation methods on the Pointing 04 and Facepix benchmark databases. &copy; 2023 World Scientific Publishing Company.","LowLevel":"emotion recognition;feature extraction","MidLevel":"computer vision;chemical;human factors;input","HighLevel":"industries;technology;end users and user experience","Venue":"Int. J. Uncertainty Fuzziness Knowledge Based Syst.","Top20AbsAndTags":[387,194,160,237,12,425,26,341,240,98,318,460,381,255,337,75,32,48,180,299],"Top20Abs":[387,237,160,26,32,318,12,98,194,240,341,425,84,171,337,255,48,75,299,65],"Top20Tags":[194,183,341,475,438,307,433,402,157,98,191,138,457,473,381,24,328,71,467,446],"Citation":"Anitta, D., & Annis Fathima, A. (2023). CNN \u2014 Forest Based Person Identification and Head Pose Estimation for AI Based Applications. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 31(Supp01), 47\u201363. https:\/\/doi.org\/10.1142\/s0218488523400044\n"},{"Title":"Correlation and comparison between digital twin and cyber physical systems","DOI":"10.1201\/9781003354222-35","Publication year":2023,"Abstract":"Advanced technologies such as Internet of things (IoTs), Artificial Intelligence (AI), blockchain, Augmented Reality (AR), data analytics etc., have significantly accelerated the development process of smart manufacturing. Manufacturers are promptly using cyber&ndash;physical integration as a prerequisite for smart production. Cyber&ndash;physical systems (CPS) and digital twins (DTs) are two popular technologies that are used as a preferred mean of integration. Regardless of their differences, both technologies have several commonalities that cause them to be misunderstood. Therefore, it is important to distinguish these two technologies in order to consolidate future research direction. Through a systematic analysis, this paper compares and contrasts CPS and DTs from several angles in order to highlight the differences and correlations between them. &copy; 2023 the Author(s).","LowLevel":"data analytics;embedded systems","MidLevel":"education;data;semiconductors","HighLevel":"industries;technology","Venue":"eWork eBus. Archit., Eng. Constr. - Proc. Eur. Conf. Prod. Process Model.","Top20AbsAndTags":[429,352,122,361,486,373,86,376,447,448,378,359,159,110,175,289,362,4,89,2],"Top20Abs":[352,429,361,373,376,447,486,359,110,159,2,122,448,175,377,289,89,372,315,224],"Top20Tags":[429,122,448,86,183,352,307,362,447,157,438,283,24,361,207,457,433,410,138,290],"Citation":"Khan, A. U., & Huang, L. (2023). Correlation and comparison between digital twin and cyber physical systems. ECPPM 2022 - EWork and EBusiness in Architecture, Engineering and Construction 2022, 268\u2013274. https:\/\/doi.org\/10.1201\/9781003354222-35\n"},{"Title":"Enabling Ultra-Compact, High-Quality 3D Displays with Neural Holography","DOI":"10.1117\/12.2655127","Publication year":2023,"Abstract":"Holographic near-eye displays have the potential to overcome many long-standing challenges for virtual and augmented reality (VR\/AR) systems; they can reproduce full 3D depth cues, improve power efficiency, enable compact display systems, and correct for optical aberrations. Despite these remarkable benefits, this technology has been held back from widespread usage due to the limited image quality achieved by traditional holographic displays, the slow algorithms for computer-generated holography (CGH), and current bulky optical setups. Here, we review recent advances in CGH that utilize artificial intelligence (AI) techniques to solve these challenges. &copy; 2023 SPIE.","LowLevel":"aberrations;artificial intelligence;computer-generated holography;holographic displays;three-dimensional displays","MidLevel":"artificial intelligence;graphics;liberal arts;optics;display technology","HighLevel":"displays;technology;industries","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[413,344,149,332,225,265,235,201,158,228,343,74,290,444,141,317,446,450,300,364],"Top20Abs":[413,225,344,265,235,228,158,317,141,332,343,444,300,201,432,74,364,290,248,450],"Top20Tags":[413,149,332,235,344,74,373,307,446,201,467,457,290,308,0,228,157,433,169,158],"Citation":"Gopakumar, M., Choi, S., Kim, J., Peng, E. Y., & Wetzstein, G. (2023). Enabling ultra-compact, high-quality 3D displays with neural holography. Practical Holography XXXVII: Displays, Materials, and Applications. https:\/\/doi.org\/10.1117\/12.2655127\n"},{"Title":"When Realities Interweave: Exploring the Design Space of Immersive Tangible XR","DOI":"10.1145\/3569009.3571843","Publication year":2023,"Abstract":"Tangible devices and interaction in Extended Reality (XR) increase immersion and enable users to perform tasks more intuitively, accurately and joyfully across the reality-virtuality continuum. Upon reviewing the literature, we noticed no clear trend for a publication venue, as well as no standard in evaluating the effects of tangible XR. To position the topic of tangible XR in the TEI community, we propose a hands-on studio, where participants will bring in their own ideas for tangible XR from their application fields, and develop prototypes with the cutting-edge technology and a selection of virtual assets provided. Additionally, we will collectively reflect upon evaluation methods on tangible XR, and aim to find a consensus of a core evaluation suite. With this, we aim to foster a practical understanding and spark new developments in tangible XR and its use cases within the TEI community.","LowLevel":"human computer interaction","MidLevel":"human-computer interaction","HighLevel":"end users and user experience","Venue":"TEI '23: Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction","Top20AbsAndTags":[53,308,327,257,306,67,305,117,193,396,296,237,391,389,218,83,234,202,203,279],"Top20Abs":[53,327,257,306,308,67,193,396,305,117,83,234,389,218,237,202,203,391,419,388],"Top20Tags":[211,203,126,210,339,308,304,459,302,436,66,224,338,196,58,122,242,397,319,37],"Citation":"Uhl, J. C., Schrom-Feiertag, H., Regal, G., Hirsch, L., Weiss, Y., & Tscheligi, M. (2023). When Realities Interweave: Exploring the Design Space of Immersive Tangible XR. Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction. https:\/\/doi.org\/10.1145\/3569009.3571843\n"},{"Title":"The Use of Industry 4.0 Technologies in Maintenance: A Systematic Literature Review","DOI":"10.1007\/978-3-031-29857-8_81","Publication year":2023,"Abstract":"In the Industry 4.0 era, where competitiveness in the industrial sector is increasingly tough, maintenance optimization is an undeniable tool to stand out in this fierce context. To minimize costs, increase productivity, improve quality and facilitate decision-making in maintenance activities, companies are resorting to the deployment of digital technologies of the fourth industrial revolution, including the Internet of Things (IoT), Big Data, Additive Manufacturing (AM), Augmented Reality (AR), Cloud Computing, etc. The main goal of this paper is to assess the impact of Industry 4.0 in maintenance, to identify which technologies are used by companies in maintenance, what are the reasons that push companies to use these tools, and what are their benefits. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"big data;cloud computing;competition;decision making;internet of things;maintenance","MidLevel":"internet of things;human factors;data;business performance metrics;manufacturing;networks","HighLevel":"end users and user experience;technology;business;industries","Venue":"Lect. Notes Networks Syst.","Top20AbsAndTags":[447,361,389,159,179,289,131,395,261,360,86,25,472,298,429,359,190,129,135,157],"Top20Abs":[389,159,447,179,261,131,395,86,361,360,289,25,429,190,2,454,1,135,157,449],"Top20Tags":[361,447,31,472,289,298,137,373,469,156,129,442,479,483,188,378,475,344,286,427],"Citation":"Essalih, S., Haouat, Z. E., Ramadany, M., Bennouna, F., & Amegouz, D. (2023). The Use of Industry 4.0 Technologies in Maintenance: A Systematic Literature Review. Lecture Notes in Networks and Systems, 811\u2013821. https:\/\/doi.org\/10.1007\/978-3-031-29857-8_81\n"},{"Title":"Bringing AR\/VR to everyday life - a wireless localization perspective","DOI":"10.1145\/3572864.3581589","Publication year":2023,"Abstract":"Augmented reality (AR) and virtual reality (VR) have the potential to revolutionize the way we interact with the environment around us. These technologies allow us to experience and collaborate with people in an immersive and intuitive way. Today, AR\/VR is no longer limited to gaming and entertainment, rather it's blending into our everyday lives with applications in medical fields, education, grocery shopping, virtual try-ons etc. While a lot of progress has been made in visual rendering and scene understanding, little work is done on multi-user localization. To fully realize the potential of collaboration and multi-user applications, it is important to have an accurate and real-time 3D localization. Current 3D localization frameworks use cameras to create a relative coordinate system but these visual technologies struggle in low light conditions, and also require all devices to share an overlap of visual features which limit the applications to line-of-sight and room-level.We propose a new peer-to-peer localization framework that utilizes Ultra-Wideband (UWB) radio to create a wireless network of nodes and measure the range and angle-of-arrival (AoA) between them. A common approach to this problem is Multidimensional Scaling (MDS) [1], which involves formulating a least squares problem and minimizing the difference between measured distances and euclidean distances from estimated coordinates. However, this approach becomes challenging when the network topology is highly dynamic and reconfigurable, especially in real-time applications like AR\/VR. To address this challenge, we developed a new algorithm that selects key edges to measure and incorporates angle information along with distance measurements.","LowLevel":"cameras;data visualization;rendering","MidLevel":"graphics;data;input","HighLevel":"technology","Venue":"HotMobile '23: Proceedings of the 24th International Workshop on Mobile Computing Systems and Applications","Top20AbsAndTags":[200,267,24,48,4,105,217,82,345,139,212,486,410,234,123,268,341,153,219,208],"Top20Abs":[200,267,24,48,217,82,234,341,152,123,237,105,440,268,60,139,212,372,4,410],"Top20Tags":[239,243,475,354,483,263,134,95,120,130,410,182,119,12,4,73,48,148,265,219],"Citation":"Garg, N., Shahid, I., Sankar, K., Dasari, M., Sheshadri, R. K., Sundaresan, K., & Roy, N. (2023). Bringing AR\/VR to Everyday Life - a Wireless Localization Perspective. Proceedings of the 24th International Workshop on Mobile Computing Systems and Applications. https:\/\/doi.org\/10.1145\/3572864.3581589\n"},{"Title":"Optimized silicon antennas for optical phased arrays","DOI":"10.1117\/12.2658716","Publication year":2023,"Abstract":"We demonstrate a large-scale two dimensional silicon-based optical phased array (OPA) composed of nanoantennas with circular gratings that are balanced in power and aligned in phase, required for producing desired radiation patterns in the far-field. The OPAs are numerically optimized to have an upward efficiency of up to 90%, targeting radiation concentration mainly in the field of view. We envision that our OPAs have the ability of generating complex holographic images, rendering them an attractive candidate for a wide range of applications like LiDAR sensors, optical trapping, optogenetic stimulation and augmented-reality displays. &copy; 2023 SPIE.","LowLevel":"antenna phased arrays;directional patterns;nanoantennas;optical radar;radiation efficiency","MidLevel":"optics;telecommunication;developers;geospatial;human-computer interaction;networks;other","HighLevel":"end users and user experience;displays;industries;technology;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[158,343,324,431,444,228,445,201,223,295,278,74,344,169,225,413,332,138,149,235],"Top20Abs":[158,343,278,431,225,344,228,295,444,445,223,74,332,324,235,263,413,265,450,393],"Top20Tags":[158,201,169,441,324,138,0,223,437,290,444,445,431,228,285,423,438,74,430,379],"Citation":"Farheen, H., Strauch, A., Scheytt, J. C., Myroshnychenko, V., & F\u00f6rstner, J. (2023). Optimized silicon antennas for optical phased arrays. Integrated Optics: Devices, Materials, and Technologies XXVII. https:\/\/doi.org\/10.1117\/12.2658716\n"},{"Title":"Combinatorial Auction-enabled Dependency-Aware Offloading Strategy in Mobile Edge Computing","DOI":"10.1109\/WCNC55385.2023.10118890","Publication year":2023,"Abstract":"Mobile Edge Computing (MEC) enables computation offloading from resource-constrained mobile devices to edge servers in close vicinity, effectively promoting the user experience on emerging interactive multimedia applications such as virtual\/augmented reality, mobile gaming, and mobile video editing. However, most contemporary MEC offloading research disregards the interdependencies between partitioned subtasks of application. Also, few studies focused on application topologies have neglected to design effective incentives to encourage edge servers to provide offloading services. In this paper, we propose a dependency-aware offloading algorithm based on a multi-round truthful combinatorial reverse auction (MTCRA) to address the social welfare maximization problem in the paradigm of MEC. Building on the topology of directed acyclic graphs (DAGs) modeled from applications, we discuss the complementarity and substitutability of subtasks in the context of combinatorial auction. Theoretical analysis shows that the presented auction mechanism achieves computing efficiency while maintaining desirable economic features like truthfulness, individual rationality, and budget balance. Simulation results demonstrate that the proposed algorithm achieves high social welfare regarding reduced execution time and good economic benefits for MEC servers.","LowLevel":"cloud computing;directed graphs;edge computing;electronic commerce;mobile computing;optimization;resource allocation;telecommunication computing","MidLevel":"telecommunication;business planning and management;business performance metrics;developers;input;geospatial;networks;sales and marketing","HighLevel":"industries;technology;business","Venue":"2023 IEEE Wireless Communications and Networking Conference (WCNC)","Top20AbsAndTags":[264,427,335,111,271,258,123,103,8,5,298,268,257,280,423,209,267,70,115,471],"Top20Abs":[264,335,111,427,271,103,258,316,298,194,268,115,70,123,134,76,151,210,267,280],"Top20Tags":[264,257,271,423,111,209,335,8,404,427,426,289,282,28,14,325,416,123,5,258],"Citation":"Kang, H., Li, M., Fan, S., & Cai, W. (2023). Combinatorial Auction-enabled Dependency-Aware Offloading Strategy in Mobile Edge Computing. 2023 IEEE Wireless Communications and Networking Conference (WCNC). https:\/\/doi.org\/10.1109\/wcnc55385.2023.10118890\n"},{"Title":"Practical design for holographic head-mounted display system using holographic printing technology","DOI":"10.1117\/12.2655308","Publication year":2023,"Abstract":"We propose a holographic printing technology for head-mounted display through practical design in hologram recording and reconstruction. Most head-mounted displays are designed based on waveguide type and analog holographic optical elements, resulting in disruption of the uniformity of the image because of the difference between the initial recording conditions and the source image. This problem can be solved using holographic printing technology to modulate different diffraction efficiencies for each holographic element. This study uses a digital holographic screen that can fabricate and reconstruct augmented reality images of 1.17\", 1.76\", and 2.35\" in a field of view of 28.07&deg;, 41.11&deg;, and 53.13&deg;, respectively, at a distance of 53.33 mm from the eye. Moreover, augmented images are realized with higher diffraction efficiency than conventional methods, simplifying the design and facilitating mass production of uniformed products using digital holographic printing technology. &copy; 2023 SPIE.","LowLevel":"diffraction efficiency;helmet mounted displays;holograms;holographic displays;holographic optical elements;image recording;product design","MidLevel":"optics;graphics;wearables;manufacturing;human-computer interaction;other;display technology","HighLevel":"displays;end users and user experience;industries;technology;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[265,332,225,74,278,235,228,149,444,393,450,223,317,431,413,481,237,351,158,169],"Top20Abs":[265,225,332,74,235,228,278,149,317,393,444,450,431,481,413,351,237,158,461,223],"Top20Tags":[332,450,74,169,223,444,278,149,228,393,191,187,235,290,351,440,21,0,225,80],"Citation":"Hwang, L., Hong, K., Choi, J., & Lee, S. (2023). Practical design for holographic head-mounted display system using holographic printing technology. Practical Holography XXXVII: Displays, Materials, and Applications. https:\/\/doi.org\/10.1117\/12.2655308\n"},{"Title":"A Faster, Lighter and&nbsp;Stronger Deep Learning-Based Approach for&nbsp;Place Recognition","DOI":"10.1007\/978-981-99-2385-4_34","Publication year":2023,"Abstract":"Visual Place Recognition is a vital part of image localization and loop closure detection systems, and it has attracted widespread interest in multiple domains such as computer vision, robotics and AR\/VR. In this work, we propose a faster, lighter and stronger approach that can generate models with fewer parameters and can spend less time in the inference stage. We designed RepVGG-lite as the backbone network in our architecture, it is more discriminative than other general networks in the Place Recognition task. RepVGG-lite has more speed advantages while achieving higher performance. We extract only one scale patch-level descriptors from global descriptors in the feature extraction stage. Then we design a trainable feature matcher to exploit both the space relationships and the visual appearance of the features, which is based on the attention mechanism. Extensive experiments on difficult datasets show that the proposed approach outperforming previous other advanced learning approaches, and achieving even higher inference speed. Our system has 14 times less params than Patch-NetVLAD, 6.8 times lower theoretical FLOPs, and run faster 21 and 33 times in feature extraction and feature matching. Moreover, the performance of our approach is 0.5% better than Patch-NetVLAD in Recall@1. We used subsets of Mapillary Street Level Sequences dataset to conduct experiments for all other challenging conditions. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"computer vision;deep learning;e-learning;extraction","MidLevel":"education;artificial intelligence;computer vision;medical;chemical","HighLevel":"industries;technology","Venue":"Commun. Comput. Info. Sci.","Top20AbsAndTags":[318,480,435,323,453,483,98,76,381,369,416,280,446,283,476,316,451,144,402,473],"Top20Abs":[318,480,435,98,76,323,316,280,458,381,402,476,451,107,416,144,486,48,438,271],"Top20Tags":[473,453,446,483,323,480,223,413,431,459,484,207,369,456,283,488,443,115,138,161],"Citation":"Huang, R., Huang, Z., & Su, S. (2023). A Faster, Lighter and\u00a0Stronger Deep Learning-Based Approach for\u00a0Place Recognition. Communications in Computer and Information Science, 453\u2013463. https:\/\/doi.org\/10.1007\/978-981-99-2385-4_34\n"},{"Title":"A Method of Touchable 3d Model Reconstruction based on Mixed Reality -A Case Study of Medical Training Applications","DOI":"10.1145\/3582649.3582679","Publication year":2023,"Abstract":"The application of trauma and hemorrhage medical treatment training has generated higher and higher requirements for the authenticity of the medical model. With the increasing integration of mixed reality technology and the medical field, multi-view-based 3D reconstruction technology can generate trauma medical object models not limited to specific targets, with low cost and high precision. However, the current reconstruction methods have weak points such as the inability to build the object surface beyond the direct view of humans, the uneven distribution of reconstructed point clouds, many holes, and reconstructed triangles. And it is time-consuming and laborious to fill and modify manually and requires highly skilled modeling expertise. Therefore, the models generated by traditional 3D reconstruction methods cannot meet the needs of interaction and touch in mixed reality applications. In order to solve these problems, we propose a new model generation method based on multi-view and point cloud processing, which enables the model to reduce the computational cost under the premise of meeting the need for direct interaction and touch in mixed reality applications. Finally, based on the reconstruction method proposed in this paper, a new medical trauma hemorrhage control and rescue training system is designed and implemented, which proves the effectiveness of the method proposed in this paper.","LowLevel":"image reconstruction;solid modelling","MidLevel":"manufacturing;construction;computer vision","HighLevel":"industries;technology","Venue":"ICIGP '23: Proceedings of the 2023 6th International Conference on Image and Graphics Processing","Top20AbsAndTags":[467,98,488,321,318,350,451,294,397,263,189,112,116,486,208,178,10,225,409,5],"Top20Abs":[112,467,350,98,488,318,451,321,263,397,234,294,486,208,301,463,189,117,439,92],"Top20Tags":[370,130,318,321,73,345,418,225,10,186,178,356,379,234,72,397,71,227,172,220],"Citation":"Liu, M., Guo, D., & Zhang, Z. (2023). A Method of Touchable 3d Model Reconstruction based on Mixed Reality \u2013A Case Study of Medical Training Applications. Proceedings of the 2023 6th International Conference on Image and Graphics Processing. https:\/\/doi.org\/10.1145\/3582649.3582679\n"},{"Title":"Design and user experience analysis of AR intelligent virtual agents on smartphones","DOI":"10.1016\/j.cogsys.2022.11.007","Publication year":2023,"Abstract":"Intelligent Virtual Agents (IVAs) can provide users with a friendly experience and have a wide range of applications in the era of artificial intelligence. However, most of existing IVAs are designed for personal computers. Design and user studies of IVAs on smartphones are uncommon. Therefore, developing IVAs for smartphones is an interesting topic. Considering Augmented Reality (AR) technology can provide more potential application value for IVAs, we mainly investigate users' experiences of AR IVAs on smartphones in this paper. To make an IVA more suitable for a smartphone, a lightweight IVA's cognitive architecture is proposed. To find out the factors that affect users' interaction experiences, the effects of humanoid embodiment and emotional expressions of IVAs on users' perceptions and experiences are explored. A museum is used as a specific task scenario to measure users' experiences. Three forms of AR agents are evaluated in this scenario: a voice assistant without an entity, a humanoid IVA without emotional expressions, and a humanoid IVA with emotional expressions. The results show that compared with the voice assistant, a humanoid embodiment can significantly improve the user's experience, and compared with humanoid IVA without emotional expressions, a humanoid IVA with emotional expressions is more welcome. Moreover, we use the cloud model to describe the uncertainty of IVAs' actions (blinking and body orientation). The results show that the uncertainty of actions can increase the believability of IVAs. All rights reserved Elsevier.","LowLevel":"artificial intelligence;cognition;human computer interaction;humanoid robots;smartphones","MidLevel":"artificial intelligence;human factors;telecommunication;robotics;liberal arts;human-computer interaction","HighLevel":"end users and user experience;technology;industries","Venue":"Cogn. Syst. Res. (Netherlands)","Top20AbsAndTags":[70,182,79,419,82,248,237,27,195,187,51,53,216,87,176,175,189,267,348,410],"Top20Abs":[70,79,237,182,248,419,187,82,195,176,53,87,51,189,175,208,410,27,102,109],"Top20Tags":[308,248,221,419,254,58,129,352,348,142,93,415,70,29,27,387,359,216,302,95],"Citation":"Gan, Q., Liu, Z., Liu, T., Zhao, Y., & Chai, Y. (2023). Design and user experience analysis of AR intelligent virtual agents on smartphones. Cognitive Systems Research, 78, 33\u201347. https:\/\/doi.org\/10.1016\/j.cogsys.2022.11.007\n"},{"Title":"InExChange: Fostering Genuine Social Connection through Embodied Breath Sharing in Mixed Reality","DOI":"10.1145\/3544549.3583917","Publication year":2023,"Abstract":"InExChange is an interactive mixed reality experience centering around an inflatable vest which conveys a physical sense of shared breathing on the diaphragm between two or more participants. The experience is composed of three acts in which the participants' breaths are transformed into metaphorical projected representations: expansive waves, flowing light trails, and growing tree branches. The inflatable wearable devices physically enact in near real-time the inhale\/exhale pattern of the other person's breath, varying in intensity level to create an attention interplay between the embodied sensation and the projection. Through this embodied sense of playful shared breathing, we aim to cultivate a genuine feeling of connection and contribute to the integration of somaesthetic design principles in mixed reality HCI.","LowLevel":"human computer interaction;pneumodynamics","MidLevel":"computer vision;human-computer interaction;medical","HighLevel":"end users and user experience;technology;industries","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[26,251,440,237,200,270,29,439,153,399,240,239,219,111,170,419,336,123,366,60],"Top20Abs":[26,251,440,200,237,270,153,29,240,399,336,123,419,239,219,60,411,14,69,324],"Top20Tags":[314,210,436,339,53,415,302,217,397,144,66,32,58,122,242,126,394,319,286,37],"Citation":"Morris, C., Liu, P., Riecke, B. E., & Maes, P. (2023). InExChange: Fostering Genuine Social Connection through Embodied Breath Sharing in Mixed Reality. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583917\n"},{"Title":"Wish You Were Here: Mental and Physiological Effects of Remote Music Collaboration in Mixed Reality","DOI":"10.1145\/3544548.3581162","Publication year":2023,"Abstract":"With face-to-face music collaboration being severely limited during the recent pandemic, mixed reality technologies and their potential to provide musicians a feeling of \"being there\" with their musical partner can offer tremendous opportunities. In order to assess this potential, we conducted a laboratory study in which musicians made music together in real-time while simultaneously seeing their jamming partner's mixed reality point cloud via a head-mounted display and compared mental effects such as flow, affect, and co-presence to an audio-only baseline. In addition, we tracked the musicians' physiological signals and evaluated their features during times of self-reported flow. For users jamming in mixed reality, we observed a significant increase in co-presence. Regardless of the condition (mixed reality or audio-only), we observed an increase in positive affect after jamming remotely. Furthermore, we identified heart rate and HF\/LF as promising features for classifying the flow state musicians experienced while making music together.","LowLevel":"groupware;helmet mounted displays;jamming;music","MidLevel":"collaboration;audio;wearables;other;display technology","HighLevel":"displays;technology;use cases;other","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[284,239,206,375,386,283,240,127,237,318,106,279,203,444,251,371,69,440,125,217],"Top20Abs":[239,284,206,318,106,375,251,283,98,371,386,240,237,127,171,451,340,350,444,411],"Top20Tags":[217,279,127,239,206,203,284,69,375,240,21,303,29,80,237,17,301,304,199,180],"Citation":"Schlagowski, R., Nazarenko, D., Can, Y., Gupta, K., Mertes, S., Billinghurst, M., & Andr\u00e9, E. (2023). Wish You Were Here: Mental and Physiological Effects of Remote Music Collaboration in Mixed Reality. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581162\n"},{"Title":"Playful co-design: creating an AR-prototype with nurses in interlocking remote and on-site workshops","DOI":"10.1145\/3544549.3573869","Publication year":2023,"Abstract":"Deeply engaging nurses in a participatory co-design process, especially in times of COVID-19, is challenging. In this case study, we shed light on the process of developing a prototype for AR-glasses in nursing. We show the challenges we faced, the methods we used and how they contribute to the core principles of participatory design. A special focus is laid on small-scale interventions with high-impact, that helped us to truly engage users. We introduce empathetic ways to connect contrasting work environments, establish mutual understanding, make the abstract more graspable with playful tools like PLAYMOBIL&#174;, and support co-design development with online formats. Finally, we discuss the transferability to other projects.","LowLevel":"diseases;epidemics;health care;medical computing","MidLevel":"medical","HighLevel":"industries","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[391,390,125,171,145,27,25,68,135,329,418,114,363,46,134,388,184,357,124,178],"Top20Abs":[125,391,27,68,145,114,46,135,388,134,418,390,184,236,179,252,251,34,370,338],"Top20Tags":[390,391,363,61,230,329,171,42,140,376,108,124,145,262,418,168,210,117,25,357],"Citation":"Albrecht-Gansohr, C., Geisler, S., & Eimler, S. C. (2023). Playful Co-Design: Creating an AR-Prototype with Nurses in Interlocking Remote and On-Site Workshops. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3573869\n"},{"Title":"Collaborative Edge Caching with Multiple Virtual Reality Service Providers Using Coalition Games","DOI":"10.1109\/WCNC55385.2023.10118763","Publication year":2023,"Abstract":"Mobile edge computing (MEC) and 5G networks can provide ultra-low latency connections. Combining the two, caching services at the network edge can greatly reduce the delay of virtual reality (VR) and augmented reality (AR) services, enhancing the Quality of Service (QoS) for users. In this paper, we investigate an efficient collaborative service caching scheme between multiple service providers (SPs) with a game-theoretical approach. We model SPs as players who care about nothing but their profits and can form coalitions by sharing edge server resources as well as costs with other members. More than one coalition can be formed in an edge server. Our algorithm guarantees to reach a Nash equilibrium, where no one has the incentive to deviate. Simulation results show that through the proposed collaboration scheme, SPs can reach a higher profit compared to several baselines as well as previously proposed schemes.","LowLevel":"5g mobile communication;edge computing;game theory;mobile computing;quality of service","MidLevel":"human resources;telecommunication;business performance metrics;input;networks","HighLevel":"industries;technology;business","Venue":"2023 IEEE Wireless Communications and Networking Conference (WCNC)","Top20AbsAndTags":[346,111,271,427,268,307,355,290,264,298,258,447,117,404,86,256,426,448,209,194],"Top20Abs":[346,307,111,271,268,427,355,298,447,117,290,258,194,86,264,404,256,448,426,403],"Top20Tags":[111,426,209,346,264,123,427,256,258,147,266,423,448,129,93,326,376,360,8,428],"Citation":"Lin, C.-C., Chiang, Y., & Wei, H.-Y. (2023). Collaborative Edge Caching with Multiple Virtual Reality Service Providers Using Coalition Games. 2023 IEEE Wireless Communications and Networking Conference (WCNC). https:\/\/doi.org\/10.1109\/wcnc55385.2023.10118763\n"},{"Title":"Machine Learning and Computer Vision for the automation of processes in advanced logistics: the Integrated Logistic Platform (ILP) 4.0","DOI":"10.1016\/j.procs.2022.12.228","Publication year":2023,"Abstract":"The growing complexity of the logistics chain, the need to take on an ever longer and distributed logistic chain, which in some cases even reaches the final stages of the production of the same goods, highlights the problem of managing the post-production phases of products and goods, but also of the personnel who work on these activities, charged to the companies that deal with logistics. We present in this paper the Integrated Logistic Platform (ILP 4.0), a software architectural model, whose purpose is to lead warehouse logistics to new levels of efficiency, not reachable as the sum of the operational functions within a single logistic company, but reachable only through a unique and cross-functional system, which surpasses the usual fragmented vision and consequent approaches, in favor a strategic coordination of all activities. In this paper it is described the general architecture of the realized platform, with its research contributions and innovations, in particular related to some typical problems of warehouse environments, solved with the help of machine learning and computer vision approaches, but also with the integration of augmented reality (AR) and virtual reality (VR) devices. The aim of the integration of these technologies in the project is the definition of a \"smart\" warehouse model environment, capable of mitigate problems of warehouse logistics such as: automation of the inventory process (through an UAV), control of warehouse movements, and management of logical \/ physical security of the premises. Based on the experiences made on the project, and focusing the attention on the safety and security aspects in logistics, that the platform covers, we discuss further a possible federated learning approach in order to mitigate the lack of data in order to solve some security and safety problems in the logistic area. All rights reserved Elsevier.","LowLevel":"computer vision;learning algorithms;logistics;software architecture;warehousing","MidLevel":"artificial intelligence;computer vision;medical;construction;developers;logistics","HighLevel":"industries;technology;business","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[269,273,361,378,388,110,45,252,281,289,41,53,12,54,447,44,184,282,395,311],"Top20Abs":[269,361,388,273,45,378,395,414,54,41,184,44,281,110,252,429,12,369,442,447],"Top20Tags":[269,378,370,273,282,446,252,115,190,220,110,274,355,387,388,379,143,181,116,144],"Citation":"Capua, M. D., Ciaramella, A., & De Prisco, A. (2023). Machine Learning and Computer Vision for the automation of processes in advanced logistics: the Integrated Logistic Platform (ILP) 4.0. Procedia Computer Science, 217, 326\u2013338. https:\/\/doi.org\/10.1016\/j.procs.2022.12.228\n"},{"Title":"Method for large field of view and eye-box for holographic waveguide display based on LED illumination","DOI":"10.1117\/12.2668436","Publication year":2023,"Abstract":"Holographic waveguide display takes the advantage of pupil expansion to increase the size of eye-box which can be widely employed into augmented reality (AR) systems. However, the angular selectivity of holographic optical elements (HOEs) used as in- and out-coupler for waveguide display is very limited when reading out by coherent light source like lasers. Here, we propose a method by utilizing broadband light source like LED with multilayer HOEs or multilayer waveguides, each pair of the in- and out-coupling HOEs or waveguides will be angularly separated to construct the specific angular region. Due to its decent spectrum selectivity of holographic couplers and multilayer structure, the field of view (FOV) of the display system can be significantly increased. Large eye-box is realized by employing pupil expansion for allowing the image to interact with out-couplers multiple times. &copy; 2023 SPIE.","LowLevel":"holographic displays;holographic optical elements;light emitting diodes;multilayers;three-dimensional displays","MidLevel":"optics;input;networks;display technology","HighLevel":"displays;technology","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[225,344,393,278,235,169,228,74,413,265,223,141,364,149,481,317,444,201,287,158],"Top20Abs":[225,344,393,278,265,235,228,169,223,413,74,364,317,141,444,287,481,422,450,89],"Top20Tags":[344,278,481,235,169,149,74,223,201,225,413,393,158,0,228,141,450,343,295,290],"Citation":"Zhang, T., & Kaneda, Y. (2023). Method for large field of view and eye-box of holographic waveguide display based on LED illumination. Practical Holography XXXVII: Displays, Materials, and Applications. https:\/\/doi.org\/10.1117\/12.2668436\n"},{"Title":"The Aachen Lab Demo: From Fundamental Perception to Design Tools","DOI":"10.1145\/3544549.3583937","Publication year":2023,"Abstract":"This year, the Media Computing Group at RWTH Aachen University turns 20. We celebrate this anniversary with a Lab Interactivity Demo at CHI that showcases not past achievements, but the range of currently ongoing research at the lab. It features hands-on interactive demos ranging from fundamental research in perception and cognition with traditional devices, such as experiencing input latency and Dark Patterns, to new input and output techniques beyond the desktop, such as user-perspective rendering in handheld AR and interaction with time-based media through conducting, to physical interfaces and the tools and processes for their design and fabrication, such as textile icons and sliders, soft robotics, and 3D printing fabric-covered objects.","LowLevel":"rendering;user interfaces","MidLevel":"graphics;human-computer interaction","HighLevel":"end users and user experience;technology","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[419,114,407,107,224,403,126,105,9,77,415,219,167,410,424,259,33,279,211,301],"Top20Abs":[419,114,348,224,407,9,77,107,424,80,415,33,451,105,153,110,128,130,111,55],"Top20Tags":[238,483,36,411,72,219,167,239,126,327,267,243,15,397,100,382,221,104,211,101],"Citation":"Borchers, J., Brocker, A., Hueber, S., Nowak, O., Sch\u00e4fer, R., Wagner, A., Preuschoff, P. M., & Schirp, L. E. (2023). The Aachen Lab Demo: From Fundamental Perception to Design Tools. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583937\n"},{"Title":"Mixed Reality Ecosystem Architecture to Support Visuoconstructive Ability in Older Adults","DOI":"10.1109\/RITA.2023.3259986","Publication year":2023,"Abstract":"Nowadays, Senile Dementia is one of the most recurrent ailments related to aging as brain functions begin to deteriorate, making the elderly more dependent on others to take care of them. Using ecosystems with mixed reality allows them to have an easier way to their activities, have some independence, improve their quality of life and do exercise routines by themselves with the help of the Internet for remote control monitoring. This work proposes an architectural model for a mixed reality ecosystem to support older adults' daily activities. The work advocates the design of the ecosystem components, which are used in two scenarios for the rehabilitation of the visuo-constructive ability of patients, making a more adequate and detailed combination and implementation of connectivity, software and peripherals.","LowLevel":"brain;geriatrics;internet","MidLevel":"farming and natural science;medical;networks","HighLevel":"industries;technology","Venue":"IEEE Rev. Iberoam. Tecnol. Aprendiz. (USA)","Top20AbsAndTags":[384,448,117,417,159,184,222,234,209,447,268,465,364,251,147,311,84,61,165,273],"Top20Abs":[117,384,222,417,184,234,465,147,304,84,311,23,273,448,363,230,61,364,388,447],"Top20Tags":[251,159,163,327,209,213,244,268,376,364,242,377,10,298,384,165,423,175,455,193],"Citation":"Chaparro, E. B. M., Mu\u00f1oz-Arteaga, J., Zavala, \u00c1. E. M., Reyes, H. C., & Condori, K. O. V. (2023). Mixed Reality Ecosystem Architecture to Support Visuoconstructive Ability in Older Adults. IEEE Revista Iberoamericana de Tecnologias Del Aprendizaje, 18(2), 182\u2013189. https:\/\/doi.org\/10.1109\/rita.2023.3259986\n"},{"Title":"HybridMingler: Towards Mixed-Reality Support for Mingling at Hybrid Conferences","DOI":"10.1145\/3544549.3585806","Publication year":2023,"Abstract":"Mingling, the activity of ad-hoc, private, opportunistic conversations ahead of, during, or after breaks, is an important socializing activity for attendees at scheduled events, such as in-person conferences. The Covid-19 pandemic had a dramatic impact on the way conferences are organized, so that most of them now take place in a hybrid mode where people can either attend on-site or remotely. While on-site attendees can resume in-person mingling, hybrid modes make it challenging for remote attendees to mingle with on-site peers. In addressing this problem, we propose a collaborative mixed-reality (MR) concept, including a prototype, called HybridMingler. This is a distributed MR system supporting ambient awareness and allowing both on-site and remote conference attendees to virtually mingle. HybridMingler aims to provide both on-site and remote attendees with a spatial sense of co-location in the very same venue location, thus ultimately improving perceived presence.","LowLevel":"epidemics;groupware;human computer interaction","MidLevel":"human-computer interaction;collaboration;medical","HighLevel":"end users and user experience;industries;use cases","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[418,197,411,366,336,303,390,391,439,6,26,395,239,304,419,248,338,178,31,365],"Top20Abs":[418,197,366,6,303,411,395,26,439,419,86,390,304,391,239,31,336,365,248,64],"Top20Tags":[418,29,336,390,199,279,104,7,363,366,17,124,391,53,239,303,325,304,203,198],"Citation":"Le, K.-D., Ly, D.-N., Nguyen, H.-L., Le, Q.-T., Fjeld, M., & Tran, M.-T. (2023). HybridMingler: Towards Mixed-Reality Support for Mingling at Hybrid Conferences. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585806\n"},{"Title":"Happiness through Metaverse: Health and Innovation Relationship","DOI":"10.1109\/CSNT57126.2023.10134713","Publication year":2023,"Abstract":"Happiness is the state of emotion characterized by various aspects including satisfaction, day-to-day activities, and balance of emotions. Feeling of accomplishment and living the life you want are the common signs of happy people whereas Metaverse is the concept of the world that exists virtually, it helps in creating a virtual environment for users. Consequently, research related to the contribution of the Metaverse in the field of mental health is increasing. In this paper the relationship between innovation (Metaverse) and mental health (happiness) according to research work is analysed, the result of analysis is presented through several graphs. According to the reviews of papers, it has been observed that there is a great future and improvement in the field of psychology with the help of emerging technologies i.e. Metaverse, AR, and VR.","LowLevel":"emotion recognition;innovation management;psychology","MidLevel":"business performance metrics;medical;human factors;input","HighLevel":"end users and user experience;technology;business;industries","Venue":"2023 IEEE 12th International Conference on Communication Systems and Network Technologies (CSNT)","Top20AbsAndTags":[377,193,226,249,355,363,204,354,442,251,215,244,64,70,44,94,448,42,200,352],"Top20Abs":[377,354,226,193,249,355,363,204,442,251,215,244,70,64,448,42,440,352,93,216],"Top20Tags":[64,44,68,363,117,94,193,210,452,283,256,70,409,47,182,163,231,415,216,352],"Citation":"Bhumika, Kaur, A., & Datta, P. (2023). Happiness through Metaverse: Health and Innovation Relationship. 2023 IEEE 12th International Conference on Communication Systems and Network Technologies (CSNT). https:\/\/doi.org\/10.1109\/csnt57126.2023.10134713\n"},{"Title":"Demystifying Mobile Extended Reality in Web Browsers: How Far Can We Go?","DOI":"10.1145\/3543507.3583329","Publication year":2023,"Abstract":"Mobile extended reality (XR) has developed rapidly in recent years. Compared with the app-based XR, XR in web browsers has the advantages of being lightweight and cross-platform, providing users with a pervasive experience. Therefore, many frameworks are emerging to support the development of XR in web browsers. However, little has been known about how well these frameworks perform and how complex XR apps modern web browsers can support on mobile devices. To fill the knowledge gap, in this paper, we conduct an empirical study of mobile XR in web browsers. We select seven most popular web-based XR frameworks and investigate their runtime performance, including 3D rendering, camera capturing, and real-world understanding. We find that current frameworks have the potential to further enhance their performance by increasing GPU utilization or improving computing parallelism. Besides, for 3D scenes with good rendering performance, developers can feel free to add camera capturing with little influence on performance to support augmented reality (AR) and mixed reality (MR) applications. Based on our findings, we draw several practical implications to provide better XR support in web browsers.","LowLevel":"graphics processing units;internet;mobile computing;online front ends;rendering","MidLevel":"telecommunication;data;graphics;semiconductors;human-computer interaction;networks","HighLevel":"end users and user experience;technology;industries","Venue":"WWW '23: Proceedings of the ACM Web Conference 2023","Top20AbsAndTags":[306,314,257,308,67,403,117,193,305,396,8,244,296,5,237,391,279,439,389,301],"Top20Abs":[306,257,314,67,308,193,396,403,305,117,389,8,244,439,5,103,179,237,139,279],"Top20Tags":[376,410,330,8,219,209,13,251,42,100,175,331,159,164,73,213,89,244,82,411],"Citation":"Bi, W., Ma, Y., Tian, D., Yang, Q., Zhang, M., & Jing, X. (2023). Demystifying Mobile Extended Reality in Web Browsers: How Far Can We Go? Proceedings of the ACM Web Conference 2023. https:\/\/doi.org\/10.1145\/3543507.3583329\n"},{"Title":"Information loss challenges in surgical navigation systems: From information fusion to AI-based approaches","DOI":"10.1016\/j.inffus.2022.11.015","Publication year":2023,"Abstract":"Surgical navigation technology provides minimally invasive surgery (MIS) with the relative pose relationships amongst medical images, surgical instruments, and lesions. On the other hand, traditional operation procedures depend heavily on direct surgical field exposure. Consequently, introducing surgical navigation can enable surgeons to operate more accurately and efficiently. A tracking system is a core enabling technology of a surgical navigation system. In this paper, after reviewing the tracking technologies, we compare and analyze their pros and cons, and find that information loss is a common challenge. The information loss problem is an inherent drawback in mono-modality surgical navigation systems. It is characterized by physical constraints, attenuation, breakdown of signal, and accuracy instability of the tracking algorithms. This review focuses on the information loss problem in tracking technologies for surgical navigation systems. Furthermore, we survey the existing solutions that aim at tackling the information loss problem, especially in the information fusion of surgical tracking technologies, and we also summarize their key improvements and limitations. Particular attention has been given to the modalities, approaches, objectives, and surgical application scenarios, which can improve the accuracy, precision, and stability of surgical navigation systems. Finally, future research trends directed at improving the information loss problem are discussed, i.e., tight integration of sensing technology, augmented reality for visualization in surgical tracking, stable high-speed 5G networks for telesurgery, strong intelligence and affordable service. All rights reserved Elsevier.","LowLevel":"5g mobile communication;artificial intelligence;medical image processing;medical robotics;surgery","MidLevel":"artificial intelligence;telecommunication;computer vision;data;medical;robotics;liberal arts;input","HighLevel":"industries;technology","Venue":"Inf. Fusion (Netherlands)","Top20AbsAndTags":[152,294,356,69,139,285,432,435,425,433,464,63,70,186,408,371,234,170,175,49],"Top20Abs":[152,294,356,69,139,285,435,433,70,432,63,464,49,425,408,175,170,186,371,48],"Top20Tags":[356,294,425,234,71,69,428,152,417,308,143,186,387,339,415,168,381,79,379,220],"Citation":"Xu, L., Zhang, H., Wang, J., Li, A., Song, S., Ren, H., Qi, L., Gu, J. J., & Meng, M. Q.-H. (2023). Information loss challenges in surgical navigation systems: From information fusion to AI-based approaches. Information Fusion, 92, 13\u201336. https:\/\/doi.org\/10.1016\/j.inffus.2022.11.015\n"},{"Title":"Efficient piezoelectric gimbal-less MEMS-mirror with large design flexibility","DOI":"10.1117\/12.2647807","Publication year":2023,"Abstract":"Biaxial resonant MEMS-scanners are considered as promising core-device in state-of-the-art imaging and projection systems due to their compactness, the large field-of-view, high speed, and comparably low power consumption. However, the usage in three-dimensional LIDAR modules or projectors for industrial applications is often limited by non-optimal Lissajous-scanning patterns. To achieve dense and spatially uniform Lissajous-trajectories, a suitable frequency ratio of the two oscillation modes is essential. In previous works, the frequency ratio was either maximized or minimized, which often led either to mechanical fragility or undesirable coupling of the two normal modes. For solving the abovementioned problems, a piezoelectrically-driven biaxial MEMS-scanner exhibiting large design flexibility, enabling the individual tailoring of the two orthogonal rotational oscillation-modes and Lissajous-patterns with large fill factor, was developed. This design freedom and decoupling of two axes motions are achieved by a gimbal-less design with individual actuator systems for the two oscillatory axes. Driven by the CMOS-compatible piezoelectric Al(Sc)N, the Q-factor of the resonant mirror with large optical aperture of 5 mm is enhanced by hermetic wafer-level glass-encapsulation. A projection module, which combines the biaxial MEMS-scanner, an RGB-laser-beam combiner, and the electronics for both read-out and control, was developed in the frame of a funded research project (\"MEMS-scanner-based laser projection system for maritime augmented reality\"). The target of the project was the development of a smart window, in the sense of a MEMS-scanner-based laser projection system for maritime augmented reality, which offers the possibility to fade in safety-relevant information of navigation and ship sensors into the field-of-view of the bridge personnel on the ship&rsquo;s bridge. Such projector is promising also for further applications in industry, for instance in 3D cameras. &copy; 2023 SPIE.","LowLevel":"bridges;cameras;laser mirrors;mems;optical radar;piezoelectricity;q factor measurement;ships","MidLevel":"optics;government;transportation;inspection, safety and quality;developers;input;geospatial;other;semiconductors","HighLevel":"displays;use cases;industries;technology;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[270,317,158,49,393,201,444,141,2,456,149,273,293,433,431,228,224,323,173,343],"Top20Abs":[270,49,393,444,317,2,456,141,201,158,153,433,273,338,293,323,376,139,401,266],"Top20Tags":[201,317,441,158,74,141,149,247,215,445,169,433,379,49,482,173,437,322,223,68],"Citation":"Wysocki, L., Ratzmann, L., Sch\u00fctt, P., Albers, J., Wille, G., & Gu-Stoppel, S. (2023). Efficient piezoelectric gimbal-less MEMS-mirror with large design flexibility. MOEMS and Miniaturized Systems XXII. https:\/\/doi.org\/10.1117\/12.2647807\n"},{"Title":"Feature-aggregated spatiotemporal spine surface estimation for wearable patch ultrasound volumetric imaging","DOI":"10.1117\/12.2653114","Publication year":2023,"Abstract":"Clear identification of bone structures is crucial for ultrasound-guided lumbar interventions, but it can be challenging due to the complex shapes of the self-shadowing vertebra anatomy and the extensive background speckle noise from the surrounding soft tissue structures. Therefore, in this work, we will present our method for estimating the vertebra bone surfaces by using a spatiotemporal U-Net architecture learning from the B-Mode image and aggregated feature maps of hand-crafted filters. Additionally, we are integrating this solution with our patch-like wearable ultrasound system to capture the repeating anatomical patterns and image the bone surfaces from multiple insonification angles. 3D bone representations can then be created for interventional guidance. The methods are evaluated on spine phantom image data collected by our proposed \"Patch\" scanner, and our systematic ablation experiment shows that improved accuracy can be achieved with the proposed architecture. Equipped with this surface estimation network, our wearable ultrasound system can potentially provide intuitive and accurate interventional guidance for clinicians in an augmented reality setting. &copy; 2023 SPIE.","LowLevel":"deep learning;image enhancement;image reconstruction;medical imaging;network architecture;wearable technology","MidLevel":"artificial intelligence;computer vision;medical;inspection, safety and quality;construction;wearables;human-computer interaction;networks","HighLevel":"displays;end users and user experience;use cases;industries;technology","Venue":"Progr. Biomed. Opt. Imaging Proc. SPIE","Top20AbsAndTags":[453,480,341,43,488,432,397,112,470,483,277,438,0,345,74,191,189,20,446,76],"Top20Abs":[480,43,341,277,488,438,191,76,20,74,255,397,446,432,345,294,112,300,470,311],"Top20Tags":[453,484,341,432,431,456,470,43,170,480,488,0,483,189,138,207,433,247,95,115],"Citation":"Jiang, B., Xu, K., Moghekar, A., Kazanzides, P., & Boctor, E. M. (2023). Feature-aggregated spatiotemporal spine surface estimation for wearable patch ultrasound volumetric imaging. Medical Imaging 2023: Ultrasonic Imaging and Tomography. https:\/\/doi.org\/10.1117\/12.2653114\n"},{"Title":"The Effects of Mobile Technology on Learning Performance and Motivation in Mathematics Education","DOI":"10.1007\/s10639-022-11166-6","Publication year":2023,"Abstract":"Due to rapid developments, mobile technologies started to play an essential role in designing seamless learning environments. Due to the availability of mobile technologies, students can access learning materials without being bound by time and place. On the grounds that these applications allow information exchange, time and space limitations such as classrooms or school bells have been eliminated. Therefore, this study aims to assess mobile-assisted seamless learning environments' effects on students' success and motivation in the secondary school 7th grade mathematics class algebra unit and student opinions about the application. The research is designed using the descriptive pattern of mixed-method research. The sample of the study is 73 middle school students (30 male and 43 female) in Turkey. Augmented Reality (AR) applications developed in teaching algebra to support individual learning and to utilize mobile technologies, WhatsApp groups were created. Algebra Achievement Test (AAT), Mathematics Motivation Scale (MMS), and semi-structured interview forms were used as data collection tools in the research. The results of the study showed that there were statistically significant differences in favor of the experiment group in AAT and MMS scores. However, no significant difference was found between the groups in intrinsic goal orientation and test anxiety scores, which are motivation sub-dimensions. The findings obtained from AAT, MMS, and the students' opinions showed that mobile technology applications used in out-of-school learning environments positively affect the learning process.","LowLevel":"computer aided instruction;educational institutions;human factors;mobile learning;teaching","MidLevel":"education;medical;human factors;training","HighLevel":"industries;use cases;end users and user experience","Venue":"Educ. Inf. Technol. (Germany)","Top20AbsAndTags":[163,185,136,369,39,134,342,103,87,171,306,114,115,196,207,88,383,262,124,82],"Top20Abs":[163,185,39,369,136,342,88,134,306,115,87,196,207,103,171,114,383,124,262,161],"Top20Tags":[103,87,262,136,171,56,51,196,145,113,3,115,82,33,114,241,93,410,147,164],"Citation":"Po\u00e7an, S., Altay, B., & Ya\u015faro\u011flu, C. (2022). The Effects of Mobile Technology on Learning Performance and Motivation in Mathematics Education. Education and Information Technologies, 28(1), 683\u2013712. https:\/\/doi.org\/10.1007\/s10639-022-11166-6\n"},{"Title":"Here and Now: Creating Improvisational Dance Movements with a Mixed Reality Mirror","DOI":"10.1145\/3544548.3580666","Publication year":2023,"Abstract":"This paper explores using mixed reality (MR) mirrors for supporting improvisational dance making. Motivated by the prevalence of mirrors in dance studios and inspired by Forsythe's Improvisation Technologies, we conducted workshops with 13 dancers and choreographers to inform the design of future MR visualisation and annotation tools for dance. The workshops involved using a prototype MR mirror as a technology probe that reveals the spatial and temporal relationships between the reflected dancing body and its surroundings during improvisation; speed dating group interviews around future design ideas; follow-up surveys and extended interviews with a digital media dance artist and a dance educator. Our findings highlight how the MR mirror enriches dancers' temporal and spatial perception, creates multi-layered presence, and affords appropriation by dancers. We also discuss the unique place of MR mirrors in the theoretical context of dance and in the history of movement visualisation, and distil lessons for broader HCI research.","LowLevel":"human computer interaction;humanities","MidLevel":"liberal arts;human-computer interaction","HighLevel":"end users and user experience;industries","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[439,227,418,443,365,395,397,422,55,424,64,246,419,360,162,458,412,263,400,236],"Top20Abs":[439,227,418,365,395,397,55,424,246,419,64,360,458,162,443,412,263,400,403,357],"Top20Tags":[58,302,27,16,405,339,314,210,436,59,66,415,338,122,242,397,221,37,80,142],"Citation":"Zhou, Q., Grebel, L., Irlitti, A., Minaai, J. A., Goncalves, J., & Velloso, E. (2023). Here and Now: Creating Improvisational Dance Movements with a Mixed Reality Mirror. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580666\n"},{"Title":"Self-supervised Multi-Modal Video Forgery Attack Detection","DOI":"10.1109\/WCNC55385.2023.10118664","Publication year":2023,"Abstract":"Video forgery attacks threaten surveillance systems by replacing the video captures with unrealistic synthesis, which can be powered by the latest augmented reality and virtual reality technologies. From the machine perception aspect, visual objects often have RF signatures that are naturally synchronized with them during recording. In contrast to video captures, the RF signatures are more difficult to attack given their concealed and ubiquitous nature. In this work, we investigate multimodal video forgery attack detection methods using both visual and wireless modalities. Since wireless signal-based human perception is environmentally sensitive, we propose a self-supervised training strategy to enable the system to work without external annotation and thus adapt to different environments. Our method achieves a perfect human detection accuracy and a high forgery attack detection accuracy of 94.38% which is comparable with supervised methods. The code is publicly available at: https:\/\/github.com\/ChuiZhao\/Secure-Mask.git","LowLevel":"computer network security;feature extraction;learning algorithms;object detection;supervised learning;video signal processing","MidLevel":"artificial intelligence;computer vision;data;medical;security;chemical;sensors;semiconductors","HighLevel":"industries;technology","Venue":"2023 IEEE Wireless Communications and Networking Conference (WCNC)","Top20AbsAndTags":[116,50,485,358,130,79,215,198,263,456,144,433,345,402,294,5,470,220,98,119],"Top20Abs":[485,116,50,263,130,79,456,470,358,345,198,433,294,215,311,402,435,452,174,45],"Top20Tags":[358,130,116,98,300,50,76,5,379,143,71,381,387,81,79,268,347,99,172,23],"Citation":"Zhao, C., Li, X., & Younes, R. (2023). Self-supervised Multi-Modal Video Forgery Attack Detection. 2023 IEEE Wireless Communications and Networking Conference (WCNC). https:\/\/doi.org\/10.1109\/wcnc55385.2023.10118664\n"},{"Title":"Examining the Impact of Teaching Electronics Fundamentals in different Learning Environments on Student's Conceptual Knowledge","DOI":"10.1109\/DELCON57910.2023.10127276","Publication year":2023,"Abstract":"Electronics Engineering is impeded when fundamental concepts are taught without practical experience. Teachers of electronics lab courses had to deal with this difficulty during the pandemic. Although live simulations and video capture were alternatives to this problem, practical expertise was lacking. In order to close this gap, engineering education's technical advancement serves as a catalyst. In order to keep students engaged even when they are studying at home, engineering education today utilizes labs, augmented reality, and virtual reality applications. The implications of teaching a basic electronics course to students in various learning contexts are presented in this study. One group took the course online, while another group studied in a hybrid format. While online students have studied only in a virtual setting, blended learning students have exposure to subject through online mode, face-to-face instruction, virtual laboratories, and practical kits. According to the findings, students who studied in a hybrid mode had a greater conceptual grasp than those who studied online. The crucial aspect to note in this scenario is that, in order to increase learning motivation and engagement, students who took a basic electronics course online completed hands-on projects while at home.","LowLevel":"blended learning;educational courses;electronic engineering computing;electronic engineering education;engineering education;teaching","MidLevel":"education;engineering;medical;optics;other","HighLevel":"industries;technology;displays;other","Venue":"2023 2nd Edition of IEEE Delhi Section Flagship Conference (DELCON)","Top20AbsAndTags":[47,171,82,134,163,306,383,124,88,185,369,342,39,320,112,196,103,204,246,391],"Top20Abs":[47,82,171,163,88,306,124,383,369,134,185,196,39,342,391,204,320,103,50,112],"Top20Tags":[171,454,342,88,154,134,246,136,232,82,287,33,241,406,185,363,103,17,52,128],"Citation":"Kaur, R., Mantri, A., Nagabhushan, P., & Singh, G. (2023). Examining the Impact of Teaching Electronics Fundamentals in different Learning Environments on Student\u2019s Conceptual Knowledge. 2023 2nd Edition of IEEE Delhi Section Flagship Conference (DELCON). https:\/\/doi.org\/10.1109\/delcon57910.2023.10127276\n"},{"Title":"Flying Names: Manipulating Name Tags in XR Social Environments for Target Selection Tasks","DOI":"10.1145\/3544549.3585677","Publication year":2023,"Abstract":"Recent developments in Extended Reality (XR) have led to the creation of various Virtual Reality (VR) social applications that allow small groups of people to interact and participate in various activities. However, these applications are not suitable for larger-scale social scenarios. One major challenge is the lacking of effective solutions for target selection because existing methods for occluded-target selection do not apply well to humans. In this paper, we propose a method called \"Flying Names\" to improve decision-making accuracy during target selection for both VR and Augmented Reality (AR) social environments by manipulating the heights of name tags on the fly and connecting an auxiliary line from a name tag to its highlighted owner. Our user study results indicate that this proposal effectively improves the accuracy of identifying the actual target person and is better than using either one or none of manipulating heights and connecting lines.","LowLevel":"decision making","MidLevel":"human factors","HighLevel":"end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[32,456,95,436,345,210,402,116,414,76,75,200,61,368,86,314,306,166,177,193],"Top20Abs":[32,456,95,345,75,116,210,414,76,61,200,250,399,193,306,165,402,177,368,436],"Top20Tags":[436,179,314,308,459,206,54,305,67,368,195,382,156,166,86,373,391,455,257,396],"Citation":"Wang, F., Chang, C.-M., & Igarashi, T. (2023). Flying Names: Manipulating Name Tags in XR Social Environments for Target Selection Tasks. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585677\n"},{"Title":"Technologies for the preservation of cultural heritage-a systematic review of the literature","DOI":"10.3390\/su15021059","Publication year":2023,"Abstract":"This work establishes the technological elements that have enabled the preservation, promotion, and dissemination of tangible and intangible cultural heritage in the period from 2018 to 2022. For this, a Systematic Literature Review (SLR) was conducted in the scientific databases Scopus, Science Direct, IEEE and Web of Science, which facilitated the identification of 146 articles related to the topic. A quantitative and qualitative analysis of the journals, authors and topics was carried out, detailing the important variables required to establish the sought-out elements; for this purpose, the following were quantified in the papers: type, topic, categorization, country, and language; in the publications, the type of heritage chosen, the place of the heritage and the type of intervention were investigated. The number of publications reporting the use of some type of technology was also identified, finding that 70% of them show a technological approach to preserve cultural heritage, while 30% refer to other types of interventions. The technologies reported to be used the most are 3D digital technologies (44% of those showing technological applications), augmented reality or virtual reality, henceforth AR\/VR (15%).","LowLevel":"electronic publishing;history;three-dimensional displays","MidLevel":"education;liberal arts;developers;display technology","HighLevel":"displays;technology;industries","Venue":"Sustainability (Switzerland)","Top20AbsAndTags":[6,105,49,193,236,16,222,372,487,168,314,31,302,392,52,109,30,349,46,196],"Top20Abs":[6,49,193,105,222,349,487,31,314,392,52,168,372,30,16,109,302,196,236,46],"Top20Tags":[16,6,302,109,343,105,405,236,332,197,364,10,278,201,391,275,235,158,354,289],"Citation":"Mendoza, M. A. D., De La Hoz Franco, E., & G\u00f3mez, J. E. G. (2023). Technologies for the Preservation of Cultural Heritage\u2014A Systematic Review of the Literature. Sustainability, 15(2), 1059. https:\/\/doi.org\/10.3390\/su15021059\n"},{"Title":"Versatile Immersive Virtual and Augmented Tangible OR - Using VR, AR and Tangibles to Support Surgical Practice","DOI":"10.1145\/3544549.3583895","Publication year":2023,"Abstract":"Immersive technologies such as virtual reality (VR) and augmented reality (AR), in combination with advanced image segmentation and visualization, have considerable potential to improve and support a surgeon's work. We demonstrate a solution to help surgeons plan and perform surgeries and educate future medical staff using VR, AR, and tangibles. A VR planning tool improves spatial understanding of an individual's anatomy, a tangible organ model allows for intuitive interaction, and AR gives contactless access to medical images in the operating room. Additionally, we present improvements regarding point cloud representations to provide detailed visual information to a remote expert and about the remote expert. Therefore, we give an exemplary setup showing how recent interaction techniques and modalities benefit an area that can positively change the life of patients.","LowLevel":"data visualization;image segmentation;medical image processing;surgery","MidLevel":"computer vision;medical;data","HighLevel":"industries;technology","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[69,170,200,178,152,356,294,371,210,4,303,168,53,326,76,366,316,244,80,340],"Top20Abs":[69,200,152,210,303,170,53,340,76,316,366,112,104,178,92,356,244,47,60,330],"Top20Tags":[356,326,71,69,178,294,186,376,148,152,112,42,425,143,371,280,72,318,4,95],"Citation":"Reinschluessel, A. V., Muender, T., Fischer, R., Kraft, V., Uslar, V. N., Weyhe, D., Schenk, A., Zachmann, G., D\u00f6ring, T., & Malaka, R. (2023). Versatile Immersive Virtual and Augmented Tangible OR \u2013 Using VR, AR and Tangibles to Support Surgical Practice. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583895\n"},{"Title":"AR-Based Resources to Train Computational Thinking Skills","DOI":"10.1007\/978-981-19-6585-2_61","Publication year":2023,"Abstract":"Learning and teaching computer programming is a challenge for everyone because it requires persistence and dedication. Nowadays, Computational Thinking is accepted as an essential skill to overcome those challenges. In this paper, we propose two Augmented Reality (AR) environments that create representations of complex programming constructs displaying engageable and playful activities that can be executed fast and do not require a great mental load. They are intended to be used as Learning Resources to train people in CT. The artifacts enable people to explore CT concepts and diverse problem-solving approaches in a subtle way. These artifacts include two simple AR-based activities, easy to handle and visualize based on \"see-through video\". We argue that by interacting with these artifacts, users can acquire CT problem-solving skills. Those two learning activities supported by AR provide visual representations and interactivity to engage students while training CT. We also describe an experiment to test both artifacts. The experiment was carried out with 12 participants of different ages and education levels. The results of feedback collected was positive.","LowLevel":"computer aided instruction;computer science education;teaching","MidLevel":"education;training;developers","HighLevel":"industries;technology;use cases","Venue":"Perspectives and Trends in Education and Technology: Selected Papers from ICITED 2022. Smart Innovation, Systems and Technologies (320)","Top20AbsAndTags":[128,185,163,414,312,154,363,87,114,136,88,369,82,115,103,306,410,164,342,186],"Top20Abs":[128,185,163,312,414,154,363,369,87,115,306,114,483,410,103,88,136,82,244,135],"Top20Tags":[128,154,88,185,140,33,363,246,136,55,56,3,82,17,405,87,9,204,164,372],"Citation":"Lima, L., Saraiva, F., aes, L. G. M., Henriques, P. R., & Cardoso, A. (2023). AR-Based Resources to Train Computational Thinking Skills. Smart Innovation, Systems and Technologies, 691\u2013702. https:\/\/doi.org\/10.1007\/978-981-19-6585-2_61\n"},{"Title":"Performance of 802.11be Wi-Fi 7 with Multi-Link Operation on AR Applications","DOI":"10.1109\/WCNC55385.2023.10118866","Publication year":2023,"Abstract":"Since its first release in the late 1990s, Wi-Fi has been updated to keep up with evolving user needs. Recently, Wi-Fi and other radio access technologies have been pushed to their edge when serving Augmented Reality (AR) applications. AR applications require high throughput, low latency, and high reliability to ensure a high-quality user experience. The 802.11be amendment - which will be marketed as Wi-Fi 7 - introduces several features that aim to enhance its capabilities to support challenging applications like AR. One of the main features introduced in this amendment is Multi-Link Operation (MLO) which allows nodes to transmit and receive over multiple links concurrently. When using MLO, traffic is distributed among links using an implementation-specific traffic-to-link allocation policy. This paper aims to evaluate the performance of MLO, using different policies, in serving AR applications compared to Single-Link (SL). Experimental simulations using an event-based Wi-Fi simulator have been conducted. Our results show the general superiority of MLO when serving AR applications. MLO achieves lower latency and serves a higher number of AR users compared to SL with the same frequency resources. In addition, increasing the number of links can improve the performance of MLO. Regarding traffic-to-link allocation policies, we found that policies can be more susceptible to channel blocking, resulting in possible performance degradation.","LowLevel":"next generation networks;radio access networks;wireless lan","MidLevel":"telecommunication;networks","HighLevel":"industries;technology","Venue":"2023 IEEE Wireless Communications and Networking Conference (WCNC)","Top20AbsAndTags":[416,257,404,466,265,271,229,84,385,102,111,53,197,163,159,309,166,151,439,268],"Top20Abs":[416,257,404,466,271,265,84,159,229,102,151,197,111,53,166,163,165,309,49,439],"Top20Tags":[416,423,82,409,385,458,328,268,266,114,265,307,209,404,239,240,258,87,298,353],"Citation":"Alsakati, M., Pettersson, C., Max, S., Moothedath, V. N., & Gross, J. (2023). Performance of 802.11be Wi-Fi 7 with Multi-Link Operation on AR Applications. 2023 IEEE Wireless Communications and Networking Conference (WCNC). https:\/\/doi.org\/10.1109\/wcnc55385.2023.10118866\n"},{"Title":"Evaluation of a pixelated holographic display concept for a Near Eye Display, recent results and technological developments","DOI":"10.1117\/12.2650111","Publication year":2023,"Abstract":"The development of an ideal optical system to support Mixed Reality and Augmented Reality (AR) applications has raised a lot of interest in the scientific community in the last decades. The perfect device remains an inaccessible target and researchers have to focus on the optimization of some specific behaviors. Several years ago, we introduced a disruptive display concept to push the device integration to the limit, with the suppression of the optical system. This allows the imaging process to be considered in a different way with a specific monitoring of the field of view. With this &lsquo;smart glass&rsquo; concept, the glass is the display and the image is formed directly onto the retina with a combination of refractive and diffractive effects. This conceptual target allowed us to define a technological roadmap to support our development. Technologies involved in this concept concern principally the field of Photonic Integrated Circuits in the visible range, digital\/analogic holography and Liquid Crystal devices. We will present the current state of our research with a particular focus on the holographic display element. Recent results related to analogic pixelated hologram recording validate and question both our technological and conceptual approach. We will show images formed by sparse holographic pixel distributions with controlled angular characteristics that demonstrate the mix of refractive and diffractive effects. The transmission behavior of this holographic device will also be analyzed. &copy; 2023 SPIE.","LowLevel":"digital devices;glass;holograms;liquid crystal displays;liquid crystals;optical systems","MidLevel":"education;graphics;chemical;optics;display technology","HighLevel":"industries;technology;displays","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[74,235,344,223,265,225,332,278,444,376,317,169,162,141,149,252,158,470,431,129],"Top20Abs":[74,225,344,235,265,278,444,223,376,332,129,317,252,169,162,80,184,220,431,432],"Top20Tags":[223,235,74,169,156,149,162,393,290,201,344,141,470,317,431,437,332,158,441,0],"Citation":"Martinez, C., Colard, M., Fowler, D., Lee, Y., Meunier-Della-Gatta, S., Millard, K., Rainouard, F., & Toubi, S. (2023). Evaluation of a pixelated holographic display concept for a near-eye display: recent results and technological developments. Advances in Display Technologies XIII. https:\/\/doi.org\/10.1117\/12.2650111\n"},{"Title":"An Innovative Development with Multidisciplinary Perspective in Metaverse Integrating with Blockchain Technology with Cloud Computing Techniques","DOI":"10.1109\/ICICT57646.2023.10134108","Publication year":2023,"Abstract":"The metaverse is a concept of creating an innovative virtual 3D environment immersed with social interconnections. This is the process of establishing the real world into a virtual environment through virtual and augmented reality. The metaverse is considered as the advanced recapitulation of the online platform. The virtual platform adopted through the digital environment helps in improvement with dilute in everyday lives. Thus the future revolution is done through the various advances and challenges in the virtual world. Hence the metaverse is the amalgamation of numerous technologies. The interconnections with the physical environment with the virtual world helps in the sharing of information in diverse field. The development of the digital environment through the exact replica of the real world helps in the overall development in social, economic and political fields. The important requirements of metaverse includes sensors, smart glasses with headsets. The necessary parameter needed to adopt is the privacy of the users in the metaverse environment. This is done through the blockchain technology with cloud computing techniques.","LowLevel":"blockchains;cloud computing","MidLevel":"security;networks;other","HighLevel":"technology;other","Venue":"2023 International Conference on Inventive Computation Technologies (ICICT)","Top20AbsAndTags":[249,377,355,193,328,440,363,442,204,251,354,244,215,372,439,436,365,374,289,2],"Top20Abs":[249,377,355,354,193,328,440,442,363,204,251,244,215,372,439,207,436,365,374,486],"Top20Tags":[289,359,377,378,8,5,7,193,208,17,159,64,298,268,271,129,111,355,163,374],"Citation":"Mandala, V., Jeyarani, M. A. R., Kousalya, A., M, P., & Arumugam, M. (2023). An Innovative Development with Multidisciplinary Perspective in Metaverse Integrating with Blockchain Technology with Cloud Computing Techniques. 2023 International Conference on Inventive Computation Technologies (ICICT). https:\/\/doi.org\/10.1109\/icict57646.2023.10134108\n"},{"Title":"Dual-view holographic AR display based on Bragg mismatched reconstruction of the holographic optical element","DOI":"10.1002\/jsid.1188","Publication year":2023,"Abstract":"A method for dual-view holographic display based on Bragg mismatched reconstruction of holographic optical element (HOE) is proposed. Under the Bragg mismatched condition, the reconstructed images are guided into two separated viewing zones to realize dual-view holographic display. Meanwhile, the viewing angle of each perspective is increased to 11.2&#176;, which is almost 2.5 times as large as the traditional holographic display system. The design process of HOE is simple only by interference of plane reference wave and converging spherical signal wave, which has high practicability. Furthermore, the HOE can mix the virtual 3D image with real-world scenes, which could implement augmented reality (AR) display. Experiments validate that the proposed system can achieve dual-view holographic AR three-dimensional (3D) display with accommodation effect. &#169; 2023 Society for Information Display.","LowLevel":"holographic displays;holographic optical elements;holography;image reconstruction","MidLevel":"optics;construction;computer vision;display technology","HighLevel":"industries;technology;displays","Venue":"J. Soc. Inf. Disp. (USA)","Top20AbsAndTags":[332,265,344,235,278,228,149,317,74,73,243,340,343,393,98,364,432,130,169,69],"Top20Abs":[332,265,344,235,278,228,317,149,243,74,432,364,98,248,340,130,444,343,393,169],"Top20Tags":[370,265,332,343,73,450,178,318,201,379,344,98,69,130,422,340,393,220,132,278],"Citation":"Qin, X., Sang, X., Li, H., Xiao, R., Zhong, C., Yan, B., Sun, Z., & Dong, Y. (2022). Dual\u2010view holographic AR display based on Bragg mismatched reconstruction of the holographic optical element. Journal of the Society for Information Display, 31(1), 46\u201356. Portico. https:\/\/doi.org\/10.1002\/jsid.1188\n"},{"Title":"ProObjAR: Prototyping Spatially-aware Interactions of Smart Objects with AR-HMD","DOI":"10.1145\/3544548.3580750","Publication year":2023,"Abstract":"The rapid advances in technologies have brought new interaction paradigms of smart objects (e.g., digital devices) beyond digital device screens. By utilizing spatial properties, configurations, and movements of smart objects, designing spatial interaction, which is one of the emerging interaction paradigms, efficiently promotes engagement with digital content and physical facility. However, as an important phase of design, prototyping such interactions still remains challenging, since there is no ad-hoc approach for this emerging paradigm. Designers usually rely on methods that require fixed hardware setup and advanced coding skills to script and validate early-stage concepts. These requirements restrict the design process to a limited group of users in indoor scenes. To facilitate the prototyping to general usages, we aim to figure out the design difficulties and underlying needs of current design processes for spatially-aware object interactions by empirical studies. Besides, we explore the design space of the spatial interaction for smart objects and discuss the design space in an input-output spatial interaction model. Based on these findings, we present ProObjAR, an all-in-one novel prototyping system with an Augmented Reality Head Mounted Display (AR-HMD). Our system allows designers to easily obtain the spatial data of smart objects being prototyped, specify spatially-aware interactive behaviors from an input-output event triggering workflow, and test the prototyping results in situ. From the user study, we find that ProObjAR simplifies the design procedure and increases design efficiency to a large extent and thus advancing the development of spatially-aware applications in smart ecosystems.","LowLevel":"helmet mounted displays;interactive systems","MidLevel":"education;wearables;input;display technology","HighLevel":"displays;technology;industries","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[53,418,80,55,415,38,142,79,412,180,365,114,27,407,92,419,153,175,89,424],"Top20Abs":[418,415,80,53,55,142,412,79,38,419,153,407,92,27,365,114,89,424,376,331],"Top20Tags":[303,75,80,58,364,415,314,210,180,217,38,301,238,440,313,21,126,206,279,69],"Citation":"Ye, H., Leng, J., Xiao, C., Wang, L., & Fu, H. (2023). ProObjAR: Prototyping Spatially-aware Interactions of Smart Objects with AR-HMD. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580750\n"},{"Title":"Study and validation of switchable grating using liquid crystal for active waveguide addressing","DOI":"10.1117\/12.2649749","Publication year":2023,"Abstract":"In our Augmented Reality (AR) project, we are investigating the use of a retinal projection display based on the association of pixelated holograms and a dense distribution of waveguides. We study the use of gratings impregnated with liquid crystal to actively extract light from waveguides. We explore two extraction strategies: tuning the refractive index contrast between the grating teeth and grooves to erase the grating diffraction effect, and changing the index of the waveguide cladding to tune the evanescence of the guided mode. Firstly, we present and discuss the measurements of the diffraction efficiency of nano-imprint gratings impregnated with liquid crystal and refractive liquid index. Secondly, we discuss the results of integrated switchable extraction grating of the second strategy. &copy; 2023 SPIE.","LowLevel":"diffraction;diffraction gratings;extraction;holograms;liquid crystal displays;liquid crystals;refractive index;waveguides","MidLevel":"graphics;chemical;input;optics;display technology","HighLevel":"industries;technology;displays","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[228,332,235,317,344,169,158,441,393,74,141,138,149,295,444,341,413,290,445,278],"Top20Abs":[228,332,317,344,169,295,235,393,71,270,138,141,444,445,74,324,465,215,49,197],"Top20Tags":[169,228,235,393,344,74,441,290,149,201,431,332,158,341,437,0,317,141,444,438],"Citation":"Toubi, S., Colard, M., Lee, Y., Racine, B., Suhm, A., Grosso, D., Kerzabi, B., & Martinez, C. (2023). Study and validation of switchable grating using liquid crystal for active waveguide addressing. Emerging Liquid Crystal Technologies XVIII. https:\/\/doi.org\/10.1117\/12.2649749\n"},{"Title":"E-Learning Ecosystems for People With Autism Spectrum Disorder: A Systematic Review","DOI":"10.1109\/ACCESS.2023.3277819","Publication year":2023,"Abstract":"E-Learning Ecosystems (ELE) offer excellent opportunities to manage teaching activities by incorporating state-of-the-art technologies, practices, and professional support, as well as learning and assessment resources that can be adaptive. Therefore, it can help people with disabilities or conditions such as Autism Spectrum Disorder (ASD) to develop skills. However, some technological factors prevent this population's implementation of support scenarios and hinder the proper learning process. This paper systematically reviews relevant studies on E-Learning Ecosystems for people with ASD, identifying the influence of Information and Communication Technologies (ICT) on forming ELE and the technological barriers that affect their development and appropriate use on people with ASD. This work conducted a systematic review using the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) methodology, including a search of five scientific literature databases from 2017 to 2022. The main aspects identified were 1) a shortage in design guides for the implementation of e-learning ecosystems adapted for people with ASD, 2) technological barriers that prevent the development of ELE, and 3) recommendations that help to mitigate the limitations of this field. In addition, the authors identified that the skills with the most significant focus of interest were social, communicative, and cognitive. The most implemented technologies include virtual and augmented reality or mobile applications. Most studies involved children with ASD between 8 and 15 years, followed by works with children between 5 to 8 years. Very few researches linked adults with ASD. Very few studies mention the ASD level of the participants, but most highlight the positive results of implementing ICT in training processes.","LowLevel":"computer aided instruction;medical disorders;mobile computing;reviews;teaching","MidLevel":"education;training;telecommunication;medical;standards","HighLevel":"industries;standards;use cases","Venue":"IEEE Access (USA)","Top20AbsAndTags":[253,443,394,115,46,87,93,168,363,114,414,184,218,34,88,136,330,103,163,320],"Top20Abs":[253,443,394,115,87,93,330,184,168,218,46,363,163,88,114,414,351,388,83,34],"Top20Tags":[33,253,93,46,82,34,164,56,51,89,168,363,87,3,103,376,115,88,320,17],"Citation":"Contreras-Ortiz, M. S., Marrugo, P. P., & Cesar Rodr\u00edguez Rib\u00f3n, J. (2023). E-Learning Ecosystems for People With Autism Spectrum Disorder: A Systematic Review. IEEE Access, 11, 49819\u201349832. https:\/\/doi.org\/10.1109\/access.2023.3277819\n"},{"Title":"Exploring Immersive Interpersonal Communication via AR","DOI":"10.1145\/3579483","Publication year":2023,"Abstract":"A central challenge of social computing research is to enable people to communicate expressively with each other remotely. Augmented reality has great promise for expressive communication since it enables communication beyond texts and photos and towards immersive experiences rendered in recipients' physical environments. Little research, however, has explored AR's potential for everyday interpersonal communication. In this work, we prototype an AR messaging system, ARwand, to understand people's behaviors and perceptions around communicating with friends via AR messaging. We present our findings under four themes observed from a user study with 24 participants, including the types of immersive messages people choose to send to each other, which factors contribute to a sense of immersiveness, and what concerns arise over this new form of messaging. We discuss important implications of our findings on the design of future immersive communication systems.","LowLevel":"rendering;social networking","MidLevel":"graphics;collaboration","HighLevel":"technology;use cases","Venue":"Proc. ACM Hum.-Comput. Interact. (USA)","Top20AbsAndTags":[102,368,93,27,286,440,209,251,203,240,60,16,126,365,316,351,253,447,69,62],"Top20Abs":[209,27,440,102,93,286,253,368,251,60,240,16,75,447,216,159,365,110,351,292],"Top20Tags":[193,363,164,368,239,327,130,267,331,367,167,213,249,377,195,126,402,410,17,100],"Citation":"Lee, K., Li, H., Wellyanto, M. R., Tham, Y. J., Monroy-Hern\u00e1ndez, A., Liu, F., Smith, B. A., & Vaish, R. (2023). Exploring Immersive Interpersonal Communication via AR. Proceedings of the ACM on Human-Computer Interaction, 7(CSCW1), 1\u201325. https:\/\/doi.org\/10.1145\/3579483\n"},{"Title":"Impact of wearing a head-mounted display on localization accuracy of real sound sources","DOI":"10.1051\/aacus\/2022055","Publication year":2023,"Abstract":"For augmented reality experiences, users wear head-mounted displays (HMD) while listening to real and virtual sound sources. This paper assesses the impact of wearing an HMD on localization accuracy of real sources. Eighteen blindfolded participants completed a localization task on 32 loudspeakers while wearing either no HMD, a bulky visor HMD, or a glass visor HMD. Results demonstrate that the HMDs had a significantly impact on participants' localization performance, increasing local great circle angle error by 0.9&#176;, and that the glass visor HMD demonstrably increased the rate of up-down confusions in the responses by 0.9-1.1%. These results suggest that wearing an HMD has a sufficiently small impact on real source localization that it can safely be considered as an HMD-free condition in most but the most demanding AR auditory localization studies.","LowLevel":"acoustic signal processing;helmet mounted displays;loudspeakers","MidLevel":"audio;data;wearables;sensors;display technology","HighLevel":"displays;technology","Venue":"Acta Acust. (France)","Top20AbsAndTags":[364,180,69,38,425,301,80,316,440,243,224,386,267,75,182,191,450,220,299,237],"Top20Abs":[180,364,69,425,38,316,301,440,267,243,80,220,386,75,182,376,191,8,450,43],"Top20Tags":[206,337,386,299,80,21,301,364,279,165,180,69,238,75,243,38,224,288,371,230],"Citation":"Poirier-Quinot, D., & Lawless, M. S. (2023). Impact of wearing a head-mounted display on localization accuracy of real sound sources. Acta Acustica, 7, 3. https:\/\/doi.org\/10.1051\/aacus\/2022055\n"},{"Title":"Model-driven Cluster Resource Management for AI Workloads in Edge Clouds","DOI":"10.1145\/3582080","Publication year":2023,"Abstract":"Since emerging edge applications such as Internet of Things (IoT) analytics and augmented reality have tight latency constraints, hardware AI accelerators have been recently proposed to speed up deep neural network (DNN) inference run by these applications. Resource-constrained edge servers and accelerators tend to be multiplexed across multiple IoT applications, introducing the potential for performance interference between latency-sensitive workloads. In this article, we design analytic models to capture the performance of DNN inference workloads on shared edge accelerators, such as GPU and edgeTPU, under different multiplexing and concurrency behaviors. After validating our models using extensive experiments, we use them to design various cluster resource management algorithms to intelligently manage multiple applications on edge accelerators while respecting their latency constraints. We implement a prototype of our system in Kubernetes and show that our system can host 2.3&#215; more DNN applications in heterogeneous multi-tenant edge clusters with no latency violations when compared to traditional knapsack hosting algorithms.","LowLevel":"cloud computing;deep learning (artificial intelligence);inference mechanisms;internet of things;resource allocation;telecommunication computing","MidLevel":"internet of things;artificial intelligence;telecommunication;medical;business planning and management;liberal arts;input;geospatial;networks;other","HighLevel":"industries;technology;business;other","Venue":"ACM Trans. Auton. Adapt. Syst. (USA)","Top20AbsAndTags":[280,325,346,335,298,111,427,209,266,123,359,404,447,129,395,301,423,15,341,95],"Top20Abs":[280,325,335,346,111,427,298,15,447,290,341,231,467,266,20,470,301,194,129,95],"Top20Tags":[298,289,129,423,257,258,280,359,346,378,123,209,428,38,220,18,175,266,286,160],"Citation":"Liang, Q., Hanafy, W. A., Ali-Eldin, A., & Shenoy, P. (2023). Model-driven Cluster Resource Management for AI Workloads in Edge Clouds. ACM Transactions on Autonomous and Adaptive Systems, 18(1), 1\u201326. https:\/\/doi.org\/10.1145\/3582080\n"},{"Title":"Multi-functional triboelectric nanogenerators on printed circuit board for metaverse sport interactive system","DOI":"10.1016\/j.nanoen.2023.108520","Publication year":2023,"Abstract":"Metaverse is the society of future, which merges the physical and digital worlds by utilizing sophisticated human-machine interfaces (HMIs). In this study, a Metaverse sport interactive system based on triboelectric nanogenerators has been developed, which realizes a real-time interaction among human beings, devices, and the internet. This Metaverse sport interactive system is composed of a self-powered anaerobic power meter (APM), a wireless transmission module, personalized data analysis based on the professional Wingate anaerobic test (WAnT) method, and an augmented reality (AR) application. By leveraging the WAnT method, the power (PP), mean power (MP), and fatigue index (FI) are tested, which indeed reflect the explosive power, speed endurance, and endurance. Meanwhile, the data is transmitted to the cloud via the wireless transmission module, and the users can freely explore the Metaverse through AR application. Notably, through machine learning, the professional WAnT can not only reflect the anaerobic capacity but can also be used for athlete selection. Essentially, this work puts forward a new approach to design online competitions, sport training, and athlete selection, and in general, provides a new strategy to build the Metaverse. &copy; 2023 Elsevier Ltd","LowLevel":"electric power transmission;human computer interaction;nanogenerators;printed circuit boards;sports;triboelectricity","MidLevel":"cultural heritage;human-computer interaction;other;semiconductors","HighLevel":"industries;technology;other;end users and user experience","Venue":"Nano Energy","Top20AbsAndTags":[377,249,193,355,363,328,442,119,226,204,251,244,486,354,153,448,274,411,345,44],"Top20Abs":[377,249,354,193,355,328,363,226,442,119,204,251,244,486,153,411,448,345,479,159],"Top20Tags":[74,486,23,162,247,441,324,137,20,155,302,274,119,66,1,192,187,279,157,443],"Citation":"Zhu, Y., Zhao, T., Sun, F., Jia, C., Ye, H., Jiang, Y., Wang, K., Huang, C., Xie, Y., & Mao, Y. (2023). Multi-functional triboelectric nanogenerators on printed circuit board for metaverse sport interactive system. Nano Energy, 113, 108520. https:\/\/doi.org\/10.1016\/j.nanoen.2023.108520\n"},{"Title":"A Dataset and Machine Learning Approach to Classify and Augment Interface Elements of Household Appliances to Support People with Visual Impairment","DOI":"10.1145\/3581641.3584038","Publication year":2023,"Abstract":"Many modern household appliances are challenging to operate for people with visual impairment. Low-contrast designs and insufficient tactile feedback make it difficult to distinguish interface elements and to recognize their function. Augmented reality (AR) can be used to visually highlight such elements and provide assistance to people with residual vision. To realize this goal, we (1) created a dataset consisting of 13,702 images of interfaces from household appliances and manually labeled control elements; (2) trained a neural network to recognize control elements and to distinguish between PushButton, TouchButton, Knob, Slider, and Toggle; and (3) designed various contrast-rich and visually simple AR augmentations for these elements. The results were implemented as a screen-based assistive AR application, which we tested in a user study with six individuals with visual impairment. Participants were able to recognize control elements that were imperceptible without the assistive application. The approach was well received, especially for the potential of familiarizing oneself with novel devices. The automatic parsing and augmentation of interfaces provide an important step toward the independent interaction of people with visual impairments with their everyday environment.","LowLevel":"domestic appliances;handicapped aids;learning algorithms","MidLevel":"artificial intelligence;consumer products;collaboration;medical","HighLevel":"industries;technology;use cases","Venue":"IUI '23: Proceedings of the 28th International Conference on Intelligent User Interfaces","Top20AbsAndTags":[288,93,148,419,27,19,72,118,116,53,382,465,316,461,351,219,60,396,144,344],"Top20Abs":[288,419,19,27,93,465,148,382,316,72,118,53,461,157,60,85,74,0,344,165],"Top20Tags":[393,93,351,277,394,63,144,116,415,274,326,370,79,234,198,367,69,72,387,255],"Citation":"Tschakert, H., Lang, F., Wieland, M., Schmidt, A., & Machulla, T.-K. (2023). A Dataset and Machine Learning Approach to Classify and Augment Interface Elements of Household Appliances to Support People with Visual Impairment. Proceedings of the 28th International Conference on Intelligent User Interfaces. https:\/\/doi.org\/10.1145\/3581641.3584038\n"},{"Title":"\"Picture the Audience...\": Exploring Private AR Face Filters for Online Public Speaking","DOI":"10.1145\/3544548.3581039","Publication year":2023,"Abstract":"Faced with public speaking anxiety, one common piece of advice is to picture the audience in a new light, using your mind's eye. With Augmented Reality (AR) face filters, it becomes possible to literally change how one sees oneself or others. In this paper, we explore privately applied AR filters during online public speaking. Private means that these effects are only visible to the speaker. To investigate this possibly controversial concept, we conducted an online survey with 100 respondents to gather a diverse set of initial impressions, possible boundaries, and guidelines. Following this, we built a prototype of a private AR web-based video-calling application, and pilot-tested it with 16 participants to gain more in-depth insights. Based on our results, we outline key user perspectives and opportunities for the private application of AR face filters during online public speaking and discuss them in the context of previous literature on this topic.","LowLevel":"human computer interaction;human factors;internet;teleconferencing;video communication","MidLevel":"human factors;video;collaboration;input;human-computer interaction;networks","HighLevel":"end users and user experience;technology;use cases","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[170,42,236,272,195,261,171,53,159,448,50,229,57,125,49,237,391,382,110,62],"Top20Abs":[170,42,261,236,49,272,171,229,391,195,125,237,53,135,50,110,336,187,102,27],"Top20Tags":[251,242,195,330,249,29,339,66,219,365,244,327,198,13,377,144,159,32,142,193],"Citation":"Leong, J., Perteneder, F., Rajvee, M. R., & Maes, P. (2023). \u201cPicture the Audience...\u201d: Exploring Private AR Face Filters for Online Public Speaking. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581039\n"},{"Title":"Digital twin framework for real-time dynamic analysis visualization with detecting dynamic changes in structures properties using PINN","DOI":"10.1117\/12.2658640","Publication year":2023,"Abstract":"This study developed a framework for real-time dynamic analysis of structural members using physics-informed neural networks (PINN). The interest in the use of augmented reality (AR) and virtual reality (VR) technologies to visualize the results of simulations is now increasing, and many researches are taking efforts to make these simulations more interactive and real-time. However, the application of structural dynamic simulations is limited due to its high computational cost. In this study, the Physics-informed neural networks (PINN) was used to conduct the real-time vibration analysis of a cantilever beam as a basic investigation. Prior to the real-time simulation, a PINN model for solving the cantilever beam undamped free vibration problem was constructed. Sequential trainings and predictions for the real-time simulation were then implemented at fine increment time steps by PINN. The distributions of displacement and bending moment, which were the outputs of the PINN simulations were visualized in AR on the real beam with converting the outputs to color contour for intuitive understanding. The RS framework based on PINN simulation and AR was then recognized to lead to the RS with data assimilation for real-time evaluation of structural condition using measurement data. &copy; 2023 SPIE.","LowLevel":"cantilever beams;nanocantilevers;structural analysis;structural dynamics","MidLevel":"construction;computer vision;other","HighLevel":"industries;technology;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[120,428,148,486,152,301,316,201,2,182,439,293,463,134,294,131,194,452,306,458],"Top20Abs":[428,152,148,120,486,301,201,293,294,463,316,439,157,131,2,182,458,134,464,306],"Top20Tags":[295,470,120,440,43,2,431,176,239,194,488,191,182,280,243,311,317,187,53,486],"Citation":"Okuda, T., Saida, T., Matono, G., & Nishio, M. (2023). Digital twin framework for real-time dynamic analysis visualization with detecting dynamic changes in structures properties using PINN. Sensors and Smart Structures Technologies for Civil, Mechanical, and Aerospace Systems 2023. https:\/\/doi.org\/10.1117\/12.2658640\n"},{"Title":"Collaborating Across Realities: Analytical Lenses for Understanding Dyadic Collaboration in Transitional Interfaces","DOI":"10.1145\/3544548.3580879","Publication year":2023,"Abstract":"Transitional Interfaces are a yet underexplored, emerging class of cross-reality user interfaces that enable users to freely move along the reality-virtuality continuum during collaboration. To analyze and understand how such collaboration unfolds, we propose four analytical lenses derived from an exploratory study of transitional collaboration with 15 dyads. While solving a complex spatial optimization task, participants could freely switch between three contexts, each with different displays (desktop screens, tablet-based augmented reality, head-mounted virtual reality), input techniques (mouse, touch, handheld controllers), and visual representations (monoscopic and allocentric 2D\/3D maps, stereoscopic egocentric views). Using the rich qualitative and quantitative data from our study, we evaluated participants' perceptions of transitional collaboration and identified commonalities and differences between dyads. We then derived four lenses including metrics and visualizations to analyze key aspects of transitional collaboration: (1) place and distance, (2) temporal patterns, (3) group use of contexts, (4) individual use of contexts.","LowLevel":"user interfaces","MidLevel":"human-computer interaction","HighLevel":"end users and user experience","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[203,419,248,279,183,237,4,263,360,26,80,479,267,331,437,69,424,195,303,219],"Top20Abs":[203,248,419,183,4,263,80,437,26,237,267,360,199,331,65,316,424,195,166,69],"Top20Tags":[314,203,419,331,72,80,49,397,92,382,367,221,36,180,238,101,364,309,242,3],"Citation":"Schr\u00f6der, J.-H., Schacht, D., Peper, N., Hamurculu, A. M., & Jetter, H.-C. (2023). Collaborating Across Realities: Analytical Lenses for Understanding Dyadic Collaboration in Transitional Interfaces. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580879\n"},{"Title":"Towards More Inclusive and Accessible Virtual Reality: Conducting Large-scale Studies in the Wild","DOI":"10.1145\/3544549.3583888","Publication year":2023,"Abstract":"In this work, we demonstrate a mobile laboratory with virtual and augmented reality (VR\/AR) technology housed in a truck that enables large-scale VR\/AR studies and therapies in real-world environments. This project aims to improve accessibility and inclusiveness in human-computer interaction (HCI) methods, providing a platform for researchers, medical professionals, and patients to utilize laboratory hardware and space. The mobile laboratory is equipped with motion tracking technology and other hardware to allow for a range of user groups to participate in VR studies and therapies that could otherwise never partake or benefit from these services. Our findings, applications, and experiences will be presented at the CHI interactivity track, with the goal of fostering future research opportunities.","LowLevel":"human computer interaction;interactive systems;medical computing;patient treatment","MidLevel":"education;human-computer interaction;medical;input","HighLevel":"industries;technology;end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[200,383,394,64,234,165,152,196,439,320,44,60,175,440,126,216,142,70,202,342],"Top20Abs":[200,383,394,234,165,440,64,320,152,342,196,267,142,70,439,202,60,44,91,103],"Top20Tags":[58,394,133,397,64,175,75,363,122,55,314,436,216,415,205,44,242,140,238,224],"Citation":"Schmelter, T., Kruse, L., Karaosmanoglu, S., Rings, S., Steinicke, F., & Hildebrand, K. (2023). Towards More Inclusive and Accessible Virtual Reality: Conducting Large-scale Studies in the Wild. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583888\n"},{"Title":"A View Direction-Driven Approach for Automatic Room Mapping in Mixed Reality","DOI":"10.1145\/3582177.3582183","Publication year":2023,"Abstract":"Virtual Reality and Augmented Reality technologies have greatly improved recently, and developers are trying to make the experience as realistic as possible and close the gap between the physical world and the virtual world. In this paper, we propose an efficient and intuitive method to create an immersive Mixed Reality environment by automatically mapping your room. Our method is view direction driven, which allows the users to simply \"look at\" any indoor space to create a 3-dimensional model of the area the user is located in. This approach is easier and more intuitive for the users to use and reduces the time and effort compared to other MR environment generating methods. We use the Meta Quest 2's cameras and gyroscope sensor and the Unity engine for the ray casting and the passthrough API. We will present the mathematical details of our method and show that the proposed method achieves better results than previous methods through the user study results.","LowLevel":"gyroscopes","MidLevel":"input","HighLevel":"technology","Venue":"IPMV '23: Proceedings of the 2023 5th International Conference on Image Processing and Machine Vision","Top20AbsAndTags":[440,198,48,15,142,340,439,200,226,163,410,365,419,382,166,452,246,316,348,203],"Top20Abs":[440,198,48,142,15,163,340,382,410,419,166,365,439,246,226,0,395,452,177,304],"Top20Tags":[200,239,29,79,17,226,73,36,56,439,87,72,210,60,422,275,238,313,167,219],"Citation":"Kim, D. J., & Li, W. (2023). A View Direction-Driven Approach for Automatic Room Mapping in Mixed Reality. Proceedings of the 2023 5th International Conference on Image Processing and Machine Vision. https:\/\/doi.org\/10.1145\/3582177.3582183\n"},{"Title":"Virtual and Augmented Experience in Virtual Learning Tours &dagger;","DOI":"10.3390\/info14050294","Publication year":2023,"Abstract":"The aim of this work is to highlight the possibilities of using VR applications in the informal learning process. This is attempted through the development of virtual reality cultural applications for historical monuments. For this purpose, the theoretical framework of virtual and augmented reality techniques is presented, developing as a showcase of the virtual environment of the historical bridge of Arta, in Greece. The bridge model is created through 3D software, which is then imported into virtual world environment by employing the Unity engine. The main objective of the research is the technical and empirical evaluation of the VR application by specialists, in comparison with the real environment of the monument. Accordingly, the use of the application in the learning process is evaluated by high school students. Using the conclusions of the evaluation, the environment will be enriched with multimedia elements and the application will be evaluated by secondary school students as a learning experience and process, using electroencephalography (EEG). The recording and analysis of research results can be generalized and lead to safe conclusions for the use of similar applications in the field of culture and learning. &copy; 2023 by the authors.","LowLevel":"3d modeling;e-learning;electroencephalography;electrophysiology;learning systems","MidLevel":"education;manufacturing;medical","HighLevel":"industries","Venue":"Information","Top20AbsAndTags":[163,320,369,262,306,87,39,134,436,114,442,51,105,453,440,414,244,357,136,392],"Top20Abs":[163,320,306,39,87,134,369,262,51,105,436,414,440,226,357,114,136,342,77,150],"Top20Tags":[138,453,464,432,480,0,442,170,488,43,369,457,24,433,341,446,323,285,459,247],"Citation":"Bosmos, F., Tzallas, A. T., Tsipouras, M. G., Glavas, E., & Giannakeas, N. (2023). Virtual and Augmented Experience in Virtual Learning Tours. Information, 14(5), 294. https:\/\/doi.org\/10.3390\/info14050294\n"},{"Title":"Realistic visualization of debris flow type landslides through virtual reality","DOI":"10.1007\/s10346-022-01948-x","Publication year":2023,"Abstract":"Immersive media technologies, such as virtual and augmented reality, have recently enabled a more holistic way to comprehend natural hazards. In this work, we aim at visualizing the temporal and spatial evolution of a debris flow in a virtual reality environment. We develop a framework to integrate the output results obtained from a debris flow numerical model into virtual reality. To guide the framework, a real debris flow event, which happened in Hunnedalen (Norway) in 2016 and blocked a road network, is considered as a case study. The debris flow is back-calculated using a depth-averaged numerical model and the simulation results are imported into a dedicated game engine to construct a digital model of the debris flow event. The debris flow is visualized using a Head-Mounted Display. We therefore discuss a wide range of potential applications of virtual reality to manage and grasp landslide phenomena: training for rescue operations; improving decision-making; studying early warning systems, and educating communities affected by natural hazards. We finally provide a quantitative evaluation of the hazard perception for a road user. We show that the debris flow movement is perceived at variable delayed times from the triggering of the landslide, depending on the position along the road where the debris flow is observed. Evaluating the realistic perception time of the natural hazard may be fundamental to designing more effective road networks, signs, and mitigation measures.","LowLevel":"decision making;hazards;helmet mounted displays","MidLevel":"inspection, safety and quality;wearables;human factors;display technology","HighLevel":"displays;use cases;end users and user experience","Venue":"Landslides (Germany)","Top20AbsAndTags":[106,337,481,149,54,436,273,444,440,263,35,301,437,212,170,379,50,120,279,142],"Top20Abs":[106,337,481,149,35,273,54,263,436,440,212,444,207,50,120,142,316,437,458,208],"Top20Tags":[21,217,238,279,301,337,364,80,75,54,198,180,237,288,165,371,440,259,69,424],"Citation":"Alene, G. H., Vicari, H., Irshad, S., Perkis, A., Bruland, O., & Thakur, V. (2022). Realistic visualization of debris flow type landslides through virtual reality. Landslides, 20(1), 13\u201323. https:\/\/doi.org\/10.1007\/s10346-022-01948-x\n"},{"Title":"Is Education Ready To Embrace Metaverse?","DOI":"10.1109\/EDUCON54358.2023.10125182","Publication year":2023,"Abstract":"Metaverse, an immersive virtual world that is facilitated by the use of virtual reality (VR) and augmented reality (AR), is expected to be soon part of our lives. Indeed, in some areas, such as in the game industry, this revolutionary technology is already being used. In the context of education, however, metaverse is not yet being extensively utilized and its possible usage is currently being debated. To contribute to this debate, the current study explores students' and educators' perceptions of metaverse and its potential to be used in education. To study these questions a mixed research approach was employed combining both quantitative and qualitative data collection. Specifically, an online questionnaire was distributed to students and semi structured interviews have been conducted with lecturers. Results suggest that students and teachers have a relatively good understanding of metaverse and foresee its utilization in education in the next few years. They remain open about the extent to which this will happen and divided as how fundamental a change it will be.","LowLevel":"computer aided instruction;computer games","MidLevel":"training;liberal arts","HighLevel":"industries;use cases","Venue":"2023 IEEE Global Engineering Education Conference (EDUCON)","Top20AbsAndTags":[363,249,377,193,355,226,354,328,442,251,244,82,215,246,306,414,196,320,56,134],"Top20Abs":[363,377,354,249,193,355,226,442,328,251,244,306,215,82,246,46,88,272,369,39],"Top20Tags":[147,164,236,51,410,33,196,145,414,93,83,405,59,9,254,154,17,79,114,351],"Citation":"Ktoridou, D., Epaminonda, E., & Efthymiou, L. (2023). Is Education Ready To Embrace Metaverse? 2023 IEEE Global Engineering Education Conference (EDUCON). https:\/\/doi.org\/10.1109\/educon54358.2023.10125182\n"},{"Title":"Challenges of 1.0 &mu;m-pitch liquid crystal spatial light modulator for future high-quality electric holographic display","DOI":"10.1117\/12.2650295","Publication year":2023,"Abstract":"Electronic holographic displays precisely reconstruct the wavefront of object light and have attracted considerable attention for virtual reality (VR) and augmented reality (AR) applications. To achieve a high-quality holographic display with a wide field of view, it is necessary to reduce the pixel pitch of a spatial light modulator (SLM) to about 1 &mu;m. We have achieved a precise control of liquid crystal (LC) alignment in 1 &mu;m pitch pixels by exploiting the anisotropy of pixel space due to the lattice-shaped dielectric walls. In this paper, we have investigated the effect of LC-SLM structure on the image quality of electric holographic displays. As a result, we clarified that the image quality of phase-modulation type holographic displays does not degrade even when the number of gray levels is 4 or more and established a simple pixel structure that allows independent control of 1 &mu;m pitch pixels and high image quality. &copy; 2023 SPIE.","LowLevel":"image quality;light modulation;light modulators;liquid crystals;pixels;quality control;three-dimensional displays","MidLevel":"computer vision;inspection, safety and quality;graphics;chemical;input;display technology","HighLevel":"industries;technology;displays;use cases","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[162,344,228,413,225,265,141,74,278,332,379,223,149,431,201,317,364,343,290,220],"Top20Abs":[162,225,344,265,228,278,141,74,413,332,300,220,379,431,364,317,343,20,201,149],"Top20Tags":[223,201,482,413,141,74,228,149,332,162,278,169,158,344,290,0,431,438,441,192],"Citation":"Ishinabe, T., Chida, K., Isomae, Y., Shibata, Y., & Fujikake, H. (2023). Challenges of 1.0 \u03bcm-pitch liquid crystal spatial light modulator for future high-quality electric holographic display. Advances in Display Technologies XIII. https:\/\/doi.org\/10.1117\/12.2650295\n"},{"Title":"HeritageSite AR: An Exploration Game for Quality Education and Sustainable Cultural Heritage","DOI":"10.1145\/3544549.3583837","Publication year":2023,"Abstract":"Cultural heritage (CH) plays an important role in realizing the Sustainable Development Goals (SDGs). In this paper, we focus on emerging technologies such as Augmented Reality (AR) and gamified learning to foster public understanding of cultural values in historical contexts. We design HeritageSite AR, an exploration game for onsite CH learning and visits with publics in Relics of Arhat Monastery and Twin Pagoda (also known as Shuangta). Based on research investigation of technical means, expert semi-structured interviews and online survey, we distill and incorporate four design goals using user journey map. The implemented game design is evaluated with respect to three design components (i.e., reality, meaning, play) and four stages (i.e., trigger, engage, consolidate, relate) in CH visits. We conclude our work with a discussion of contributions to SDGs.","LowLevel":"computer aided instruction;computer games;history;serious games;sustainable development","MidLevel":"training;liberal arts;simulation;policy","HighLevel":"industries;business;use cases","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[114,302,83,145,105,79,164,310,16,204,150,113,6,111,51,218,418,213,229,163],"Top20Abs":[114,105,302,418,310,83,213,111,79,145,164,16,6,150,207,27,336,163,229,113],"Top20Tags":[204,145,302,16,410,147,405,113,164,51,33,400,196,414,150,93,83,77,67,59],"Citation":"Xu, N., Liang, J., Shuai, K., Li, Y., & Yan, J. (2023). HeritageSite AR: An Exploration Game for Quality Education and Sustainable Cultural Heritage\u2731. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583837\n"},{"Title":"Virtual Big Heads in Extended Reality: Estimation of Ideal Head Scales and Perceptual Thresholds for Comfort and Facial Cues","DOI":"10.1145\/3571074","Publication year":2023,"Abstract":"Extended reality (XR) technologies, such as virtual reality (VR) and augmented reality (AR), provide users, their avatars, and embodied agents a shared platform to collaborate in a spatial context. Although traditional face-to-face communication is limited by users' proximity, meaning that another human's non-verbal embodied cues become more difficult to perceive the farther one is away from that person, researchers and practitioners have started to look into ways to accentuate or amplify such embodied cues and signals to counteract the effects of distance with XR technologies. In this article, we describe and evaluate the&lt;i&gt;Big Head&lt;\/i&gt;technique, in which a human's head in VR\/AR is scaled up relative to their distance from the observer as a mechanism for enhancing the visibility of non-verbal facial cues, such as facial expressions or eye gaze. To better understand and explore this technique, we present two complimentary human-subject experiments in this article. In our first experiment, we conducted a VR study with a head-mounted display to understand the impact of increased or decreased head scales on participants' ability to perceive facial expressions as well as their sense of comfort and feeling of \"uncannniness\" over distances of up to 10 m. We explored two different scaling methods and compared perceptual thresholds and user preferences. Our second experiment was performed in an outdoor AR environment with an optical see-through head-mounted display. Participants were asked to estimate facial expressions and eye gaze, and identify a virtual human over large distances of 30, 60, and 90 m. In both experiments, our results show significant differences in minimum, maximum, and ideal head scales for different distances and tasks related to perceiving faces, facial expressions, and eye gaze, and we also found that participants were more comfortable with slightly bigger heads at larger distances. We discuss our findings with respect to the technologies used, and we discuss implications and guidelines for practical applications that aim to leverage XR-enhanced facial cues.","LowLevel":"helmet mounted displays","MidLevel":"wearables;display technology","HighLevel":"displays","Venue":"ACM Trans. Appl. Percept. (USA)","Top20AbsAndTags":[182,32,259,277,165,338,283,279,75,344,306,40,33,444,314,26,211,69,257,327],"Top20Abs":[182,32,259,283,165,277,338,40,316,33,75,263,306,248,69,390,278,37,279,26],"Top20Tags":[206,337,217,238,80,308,440,21,301,142,279,38,240,364,165,313,168,180,371,224],"Citation":"Choudhary, Z., Erickson, A., Norouzi, N., Kim, K., Bruder, G., & Welch, G. (2023). Virtual Big Heads in Extended Reality: Estimation of Ideal Head Scales and Perceptual Thresholds for Comfort and Facial Cues. ACM Transactions on Applied Perception, 20(1), 1\u201331. https:\/\/doi.org\/10.1145\/3571074\n"},{"Title":"Volumetric Mixed Reality Telepresence for Real-time Cross Modality Collaboration","DOI":"10.1145\/3544548.3581277","Publication year":2023,"Abstract":"Mixed-reality telepresence allows local and remote users feel as if they are present together in the same space. In this paper we report on a mixed-reality volumetric telepresence system that is adaptable, multi-user and cross-modal, i.e. combining augmented and virtual reality technologies with face-to-face interactions. The system extends state-of-art by creating full-body and environmental volumetric renderings in real-time over local enterprise networks. We report findings of an evaluation in a training scenario which was adapted for remote delivery and led by an industry professional. Analysis of interviews and observed behaviours identify varying attitudes towards virtually mediated full-body experiences and highlight the impact of volumetric mixed-reality telepresence to facilitate personal experiences of co-presence and to ground communication with interlocutors.","LowLevel":"groupware;real-time systems;rendering","MidLevel":"education;human-computer interaction;collaboration;graphics","HighLevel":"industries;technology;use cases;end users and user experience","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[366,337,307,304,240,411,440,439,301,84,279,251,303,178,125,284,219,203,267,405],"Top20Abs":[366,337,307,304,440,411,178,240,251,84,284,268,405,357,303,419,118,301,64,248],"Top20Tags":[337,400,240,29,219,17,440,279,243,316,303,439,104,331,304,203,120,409,125,329],"Citation":"Irlitti, A., Latifoglu, M., Zhou, Q., Reinoso, M. N., Hoang, T., Velloso, E., & Vetere, F. (2023). Volumetric Mixed Reality Telepresence for Real-time Cross Modality Collaboration. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581277\n"},{"Title":"Research on virtual reality fusion method based on spatial Marker location","DOI":"10.1117\/12.2666620","Publication year":2023,"Abstract":"Virtual reality fusion based on augmented reality has become a research hotspot, which is widely used in cultural relics exhibition, medical care and other fields. Spatial projection mapping matrix is the basis for projection equipment to project the prefabricated image onto the target surface. However, in practical operation, it is necessary to determine the relative position relationship between the projection equipment and the actual scene based on complex spatial target calibration. This paper aims to solve the problem of projection information dislocation and realize real-time tracking projection. A high-precision center positioning method based on the invariable characteristic of concentric circle intersection ratio is designed, and the mapping matrix from projection equipment to target is calculated based on PNP method. Finally, the distortion parameters of the lens are used to generate a projection pattern that can offset the projection distortion, so as to optimize the coincidence between the projection pattern and the real object, and achieve efficient and high-precision virtual reality fusion projection. &copy; 2023 SPIE.","LowLevel":"mapping","MidLevel":"navigation","HighLevel":"use cases","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[444,433,49,324,2,76,393,486,456,431,338,177,340,488,350,285,98,295,436,358],"Top20Abs":[444,433,324,49,76,393,2,456,431,338,340,486,177,350,98,358,488,20,285,24],"Top20Tags":[2,191,132,471,486,347,312,432,290,68,459,187,38,63,149,70,183,440,321,81],"Citation":"Li, S., Li, D., Du, L., Guo, H., & Wang, H. (2023). Research on virtual reality fusion method based on spatial marker location. Ninth Symposium on Novel Photoelectronic Detection Technology and Applications. https:\/\/doi.org\/10.1117\/12.2666620\n"},{"Title":"Functional overview of integration of AIML with 5G and beyond the network","DOI":"10.1109\/ICCCI56745.2023.10128466","Publication year":2023,"Abstract":"4 G\/LTE mobile networks resolved this issue. Strong physical layer and adaptable network design enable high-capacity mobile broadband Internet. Despite this, the prevalence of bandwidth-intensive technologies like virtual reality, augmented reality, and others has grown. In addition, the rising popularity of new services places astrain on mobile infrastructure. Applications requiring high availability and low latency, such Internet-of-Vehicles or communications between vehicles (IoV). With the advent of the new 5G technology with its massive MIMO radio interface, these problems are no longer a concern. Networks protected by software-defined networking (SDN)and NFV have added a new level of flexibility that allows network operators to serve services with very high requirements across several industries. Network operators must increase and diversify their intelligence to fully comprehend the operational environment, user behaviours, and user demands. A further goal is to become(self-) networkable proactively and effectively. This chapter will look at how AI may help us in the modern world. Next-generation mobile networks that are both efficient and adaptable may benefit greatly from machine learning in the 5G era and beyond. The evolution of AI and ML in network applications.","LowLevel":"5g mobile communication;computer network security;internet;long term evolution;mimo communication;software defined networking;virtualization","MidLevel":"simulation;telecommunication;security;input;networks","HighLevel":"industries;technology;use cases","Venue":"2023 International Conference on Computer Communication and Informatics (ICCCI)","Top20AbsAndTags":[447,404,209,448,458,256,426,335,423,427,307,266,111,251,416,273,298,258,385,159],"Top20Abs":[447,404,458,209,256,426,307,335,448,111,427,273,103,179,423,411,416,316,144,258],"Top20Tags":[209,423,427,330,163,428,251,404,266,274,416,377,298,159,244,10,376,264,257,448],"Citation":"Srinivas, K., Aswini., J., Patro, P., & Kumar, D. (2023). Functional overview of integration of AIML with 5G and beyond the network. 2023 International Conference on Computer Communication and Informatics (ICCCI). https:\/\/doi.org\/10.1109\/iccci56745.2023.10128466\n"},{"Title":"Task offloading and parameters optimization of MAR in multi-access edge computing","DOI":"10.1016\/j.eswa.2022.119379","Publication year":2023,"Abstract":"With the development of mobile augmented reality (MAR) technology, the demand for MAR applications is increasing. However, MAR is rarely used in mobile devices due to its high computational and energy consumption. In this paper, we study the task offloading and parameters optimization of MAR applied to mobile devices in mobile edge computing. Considering the influence of the MAR client energy consumption, service delay and detection accuracy in the task offloading and parameters optimization process, we design a function to evaluate MAR client energy efficiency. The problem of task offloading and parameters optimization is formulated to minimize energy efficiency function under the limitation of MAR task completion time and wireless bandwidth resources. To solve this problem, we propose a server selection and parameters optimization (SSPO) algorithm to realize client task offloading and parameters optimization. The SSPO algorithm first generates priority queue of tasks. Based on the order of priority queue, tasks are offloaded to appropriate mobile edge server according to the analytic hierarchy process. After that, the parameters are calculated and the tasks are redistributed according to the completion time until the energy efficiency function converges. Simulation results show that the proposed algorithm is better than the comparison algorithm. All rights reserved Elsevier.","LowLevel":"edge computing;energy conservation;energy consumption;mobile computing;optimization;resource allocation;telecommunication power management;telecommunication scheduling","MidLevel":"telecommunication;power and energy;business planning and management;business performance metrics;developers;input;geospatial;farming and natural science;networks","HighLevel":"industries;technology;business","Venue":"Expert Syst. Appl. (Netherlands)","Top20AbsAndTags":[36,346,369,111,269,164,147,280,258,471,335,360,469,195,271,209,96,268,194,24],"Top20Abs":[36,346,369,269,164,111,147,360,258,335,469,280,471,194,128,195,151,419,172,96],"Top20Tags":[346,209,257,266,111,404,258,271,427,423,335,36,428,139,8,13,142,14,123,268],"Citation":"Li, Y., Zhu, X., Song, S., Ma, S., Yang, F., & Zhai, L. (2023). Task offloading and parameters optimization of MAR in multi-access edge computing. Expert Systems with Applications, 215, 119379. https:\/\/doi.org\/10.1016\/j.eswa.2022.119379\n"},{"Title":"Perceptual Visibility Model for Temporal Contrast Changes in Periphery","DOI":"10.1145\/3564241","Publication year":2023,"Abstract":"Modeling perception is critical for many applications and developments in computer graphics to optimize and evaluate content generation techniques. Most of the work to date has focused on central (foveal) vision. However, this is insufficient for novel wide-field-of-view display devices, such as virtual and augmented reality headsets. Furthermore, the perceptual models proposed for the fovea do not readily extend to the off-center, peripheral visual field, where human perception is drastically different. In this article, we focus on modeling the temporal aspect of visual perception in the periphery. We present new psychophysical experiments that measure the sensitivity of human observers to different spatio-temporal stimuli across a wide field of view. We use the collected data to build a perceptual model for the visibility of temporal changes at different eccentricities in complex video content. Finally, we discuss, demonstrate, and evaluate several problems that can be addressed using our technique. First, we show how our model enables injecting new content into the periphery without distracting the viewer, and we discuss the link between the model and human attention. Second, we demonstrate how foveated rendering methods can be evaluated and optimized to limit the visibility of temporal aliasing.","LowLevel":"display devices;rendering;spatiotemporal phenomena;visual perception","MidLevel":"geospatial;input;graphics;display technology","HighLevel":"displays;technology","Venue":"ACM Trans. Graph. (USA)","Top20AbsAndTags":[458,259,340,126,317,45,274,252,237,365,316,414,107,260,4,50,211,119,397,10],"Top20Abs":[458,340,259,45,237,317,4,365,452,412,274,252,123,397,10,414,211,302,126,119],"Top20Tags":[259,107,422,72,71,260,199,130,100,316,300,219,250,243,50,331,167,95,126,327],"Citation":"Tursun, C., & Didyk, P. (2022). Perceptual Visibility Model for Temporal Contrast Changes in Periphery. ACM Transactions on Graphics, 42(2), 1\u201316. https:\/\/doi.org\/10.1145\/3564241\n"},{"Title":"Practical Saccade Prediction for Head-Mounted Displays: Towards a Comprehensive Model","DOI":"10.1145\/3568311","Publication year":2023,"Abstract":"Eye-tracking technology has started to become an integral component of new display devices such as virtual and augmented reality headsets. Applications of gaze information range from new interaction techniques that exploit eye patterns to gaze-contingent digital content creation. However, system latency is still a significant issue in many of these applications because it breaks the synchronization between the current and measured gaze positions. Consequently, it may lead to unwanted visual artifacts and degradation of the user experience. In this work, we focus on foveated rendering applications where the quality of an image is reduced towards the periphery for computational savings. In foveated rendering, the presence of system latency leads to delayed updates to the rendered frame, making the quality degradation visible to the user. To address this issue and to combat system latency, recent work proposes using saccade landing position prediction to extrapolate gaze information from delayed eye tracking samples. Although the benefits of such a strategy have already been demonstrated, the solutions range from simple and efficient ones, which make several assumptions about the saccadic eye movements, to more complex and costly ones, which use machine learning techniques. However, it is unclear to what extent the prediction can benefit from accounting for additional factors and how more complex predictions can be performed efficiently to respect the latency requirements. This paper presents a series of experiments investigating the importance of different factors for saccades prediction in common virtual and augmented reality applications. In particular, we investigate the effects of saccade orientation in 3D space and &lt;b&gt;smooth pursuit eye-motion (SPEM) &lt;\/b&gt;and how their influence compares to the variability across users. We also present a simple, yet efficient post-hoc correction method that adapts existing saccade prediction methods to handle these factors without performing extensive data collection. Furthermore, our investigation and the correction technique may also help future developments of machine-learning-based techniques by limiting the required amount of training data.","LowLevel":"biomechanics;eye;gaze tracking;helmet mounted displays;human factors;learning algorithms;rendering;user experience;visual perception","MidLevel":"artificial intelligence;human factors;computer vision;medical;graphics;wearables;input;display technology","HighLevel":"displays;technology;industries;end users and user experience","Venue":"ACM Trans. Appl. Percept. (USA)","Top20AbsAndTags":[277,32,250,385,237,116,301,263,453,358,484,243,123,470,72,267,126,33,481,163],"Top20Abs":[277,385,32,237,484,267,453,358,116,250,263,301,470,151,33,123,40,390,87,433],"Top20Tags":[279,32,72,243,277,415,79,263,21,71,250,198,75,365,260,142,206,180,80,199],"Citation":"Arabadzhiyska, E., Tursun, C., Seidel, H.-P., & Didyk, P. (2023). Practical Saccade Prediction for Head-Mounted Displays: Towards a Comprehensive Model. ACM Transactions on Applied Perception, 20(1), 1\u201323. https:\/\/doi.org\/10.1145\/3568311\n"},{"Title":"Edge Computing Unloading Technology Based on Electric Vehicle Charging Pile","DOI":"10.1109\/ACPEE56931.2023.10135587","Publication year":2023,"Abstract":"With the rapid development of the new energy vehicle industry, more and more people choose electric vehicles as their means of travel, and the corresponding computing tasks are also increasing. The functions derived from it are also becoming more and more complex, such as the very popular augmented reality, driverless and other emerging technologies in recent years. For this reason, edge computing unloading technology is introduced into the Internet of Vehicles, which sinks the pressure of vehicle computing to the edge network of charging piles. Vehicle tasks are unloaded to the nearby charging pile server, improving vehicle computing capacity and reducing computing costs. To sum up, in order to solve the huge problem of vehicle task calculation and reduce the cost of charging piles, this paper proposes a edge computing unloading strategy based on electric vehicle charging piles.Set the server in the electric vehicle charging pile to meet the calculation task delay requirements of vehicles near the charging pile. The experimental results show that the edge computing unloading strategy based on the EV charging pile can increase the task delay by more than 31.7% compared with the vehicle itself.","LowLevel":"battery powered vehicles;edge computing;electric vehicle charging;internet of things;power engineering computing;vehicular ad hoc networks","MidLevel":"internet of things;engineering;automotive;power and energy;networks;other","HighLevel":"industries;technology;other","Venue":"2023 8th Asia Conference on Power and Electrical Engineering (ACPEE)","Top20AbsAndTags":[242,264,335,268,111,472,404,346,107,315,271,110,361,123,298,447,282,158,209,289],"Top20Abs":[242,335,264,268,111,346,472,107,158,361,110,271,447,282,404,194,269,151,444,221],"Top20Tags":[123,298,129,286,266,271,38,395,110,220,175,378,121,289,31,362,359,264,472,111],"Citation":"Xue, L., Zhang, T., Wang, K., Han, C., Jia, B., Zhou, L., & Zhou, K. (2023). Edge Computing Unloading Technology Based on Electric Vehicle Charging Pile. 2023 8th Asia Conference on Power and Electrical Engineering (ACPEE). https:\/\/doi.org\/10.1109\/acpee56931.2023.10135587\n"},{"Title":"Efficient resource allocation and scheduling mechanism of XR traffic","DOI":"10.1145\/3580219.3580243","Publication year":2023,"Abstract":"EXtended reality (XR) has become one of the most important 5G and 5G-Advance media applications in the industry. Specifically, XR is a broad term covering augmented reality, mixed reality and virtual reality. Although XR traffic has some similar characteristics with URLLC traffic, there are also some differences, such as multiple periodic data streams, variable packets sizes and so on. Therefore, based on the analysis of XR-specific traffic, the paper aims to investigate efficient resource allocation and scheduling mechanism of XR traffic. For instance, with the configured grant (CG) transmission, the latency of uplink (UL) XR data flows can be reduced. In order to improve the reliability, K-repeated CG configuration can be considered. To help the gNB make efficient scheduling, the UE can also use the assistance information to indicate XR traffics' quality requirements and time-aligned transmission to gNB. After that, the paper analyses and evaluates the efficiency of the UL grants allocation method. It shows that with the assistance information, the gNB can make priority classification and efficient resource allocation, which can reduce resources by up to 81%.","LowLevel":"5g mobile communication;data flow computing;resource allocation;telecommunication computing;telecommunication network reliability;telecommunication scheduling;telecommunication traffic","MidLevel":"telecommunication;data;business planning and management;developers;input;geospatial;other","HighLevel":"industries;technology;business;other","Venue":"CCEAI'23: Proceedings of the 7th International Conference on Control Engineering and Artificial Intelligence","Top20AbsAndTags":[404,466,314,327,308,306,67,426,305,231,296,423,117,427,193,256,396,269,389,237],"Top20Abs":[466,327,314,306,404,308,67,193,426,396,305,231,269,389,117,427,448,403,296,467],"Top20Tags":[404,423,209,271,346,264,428,426,427,459,256,326,67,268,335,266,447,355,416,44],"Citation":"Xin, J., Li, Y., Liu, T., Xing, Z., & Xu, S. (2023). Efficient resource allocation and scheduling mechanism of XR traffic. 2023 7th International Conference on Control Engineering and Artificial Intelligence. https:\/\/doi.org\/10.1145\/3580219.3580243\n"},{"Title":"Pensieve 5G: Implementation of RL-based ABR Algorithm for UHD 4K\/8K Content Delivery on Commercial 5G SA\/NR-DC Network","DOI":"10.1109\/WCNC55385.2023.10118834","Publication year":2023,"Abstract":"While the rollout of the fifth-generation mobile network (5G) is underway across the globe with the intention to deliver 4K\/8K UHD videos, Augmented Reality (AR), and Virtual Reality (VR) content to the mass amounts of users, the coverage and throughput are still one of the most significant issues, especially in the rural areas, where only 5G in the low-frequency band are being deployed. This called for a highperformance adaptive bitrate (ABR) algorithm that can maximize the user quality of experience given 5G network characteristics and data rate of UHD contents.Recently, many of the newly proposed ABR techniques were machine-learning based. Among that, Pensieve is one of the state-of-the-art techniques, which utilized reinforcement-learning to generate an ABR algorithm based on observation of past decision performance. By incorporating the context of the 5G network and UHD content, Pensieve has been optimized into Pensieve 5G. New QoE metrics that more accurately represent the QoE of UHD video streaming on the different types of devices were proposed and used to evaluate Pensieve 5G against other ABR techniques including the original Pensieve. The results from the simulation based on the real 5G Standalone (SA) network throughput shows that Pensieve 5G outperforms both conventional algorithms and Pensieve with the average QoE improvement of 8.8% and 14.2%, respectively. Additionally, Pensieve 5G also performed well on the commercial 5G NR-NR Dual Connectivity (NR-DC) Network, despite the training being done solely using the data from the 5G Standalone (SA) network.","LowLevel":"5g mobile communication;mobile computing;quality of experience;reinforcement learning;video streaming","MidLevel":"artificial intelligence;telecommunication;video;medical;inspection, safety and quality;input","HighLevel":"industries;technology;use cases","Venue":"2023 IEEE Wireless Communications and Networking Conference (WCNC)","Top20AbsAndTags":[404,268,447,426,257,423,335,163,428,458,411,116,427,209,144,162,300,259,453,263],"Top20Abs":[404,268,447,426,423,411,335,458,257,428,427,163,116,300,209,263,480,144,464,402],"Top20Tags":[144,428,209,413,446,485,459,335,443,210,341,326,216,93,64,257,363,147,44,453],"Citation":"Arunruangsirilert, K., Wei, B., Song, H., & Katto, J. (2023). Pensieve 5G: Implementation of RL-based ABR Algorithm for UHD 4K\/8K Content Delivery on Commercial 5G SA\/NR-DC Network. 2023 IEEE Wireless Communications and Networking Conference (WCNC). https:\/\/doi.org\/10.1109\/wcnc55385.2023.10118834\n"},{"Title":"Deep Convolutional Neural Networks applied to Hand Keypoints Estimation","DOI":"10.1109\/ICARSC58346.2023.10129621","Publication year":2023,"Abstract":"Accurate estimation of hand shape and position is an important task in various applications, such as human-computer interaction, human-robot interaction, and virtual and augmented reality. In this paper, it is proposed a method to estimate the hand keypoints from single and colored images utilizing the pre-trained deep convolutional neural networks VGG-16 and VGG-19. The method is evaluated on the FreiHAND dataset, and the performance of the two neural networks is compared. The best results were achieved by the VGG-19, with average estimation errors of 7.40 pixels and 11.36 millimeters for the best cases of two-dimensional and three-dimensional hand keypoints estimation, respectively.","LowLevel":"convolutional neural nets;deep learning (artificial intelligence);feature extraction;human-robot interaction;image classification;image colour analysis;pose estimation","MidLevel":"artificial intelligence;computer vision;medical;robotics;graphics;liberal arts;chemical;networks;other","HighLevel":"industries;technology;other","Venue":"2023 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)","Top20AbsAndTags":[19,387,174,160,205,379,12,415,81,381,425,142,143,70,352,366,159,144,476,358],"Top20Abs":[387,12,160,460,415,142,476,19,453,366,379,174,144,143,381,396,425,480,358,190],"Top20Tags":[19,407,70,205,387,352,159,174,415,381,425,379,81,95,451,143,18,98,99,160],"Citation":"Santos, B. M., Pais, P., Ribeiro, F. M., Lima, J., Gon\u00e7alves, G., & Pinto, V. H. (2023). Deep Convolutional Neural Networks applied to Hand Keypoints Estimation. 2023 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC). https:\/\/doi.org\/10.1109\/icarsc58346.2023.10129621\n"},{"Title":"Developing Cargo Loading Software for Navy and Marine Aircraft","DOI":"10.1109\/AERO55745.2023.10115709","Publication year":2023,"Abstract":"Managing cargo loading for U.S. Navy and Marine Corps aircraft is a challenging task, requiring an understanding of elements such as aircraft limitations, aircraft center of gravity, cargo space dimensions, and tie-down procedures to name a few. These loading requirements are specified in each aircraft's lengthy Cargo Loading Guide (CLG). To address the problem of efficiently and effectively stowing cargo, the U.S. Navy has proposed the development of an Android app that assists aircrew in completing their loadmaster duties. This paper describes the Aircraft Cargo Evaluator app, which uses three specific capabilities to perform calculations and provide feedback to help achieve efficient and effective cargo loading. The first creates 3D models for novel cargo using Augmented Reality. The second allows the user to develop scenarios that include 3D models of aircraft, cargo, and tie-down patterns, and then analyzes the tie-downs according to CLG-defined rules. The third uses genetic algorithms to automatically search for and efficient and effective tie-down patterns for a scenario. The primary contribution of this work is summarizing how existing tools from augmented reality, computer games, and artificial intelligence were brought together to rapidly prototype an end-to-end solution in this challenging domain - and then following what happens as this research prototype takes the first steps towards the reality of operational use.","LowLevel":"aerospace computing;aircraft;android;artificial intelligence;computer games;freight handling;genetic algorithms;human factors;military aircraft;mobile computing","MidLevel":"artificial intelligence;human factors;telecommunication;government;aviation and aerospace;liberal arts;developers;human-computer interaction;other","HighLevel":"industries;technology;other;end users and user experience","Venue":"2023 IEEE Aerospace Conference","Top20AbsAndTags":[245,51,216,77,27,79,286,374,26,414,410,382,403,408,273,189,446,307,65,56],"Top20Abs":[245,27,307,403,446,286,189,374,408,161,51,45,26,414,454,202,82,397,467,106],"Top20Tags":[308,339,79,204,147,410,216,18,51,58,164,59,97,251,466,359,77,286,115,111],"Citation":"Ludwig, J., Presnell, B., & Tuohy, D. (2023). Developing Cargo Loading Software for Navy and Marine Aircraft. 2023 IEEE Aerospace Conference. https:\/\/doi.org\/10.1109\/aero55745.2023.10115709\n"},{"Title":"Using Physiological Signals and Machine Learning Algorithms to Measure Attentiveness During Robot-Assisted Social Skills Intervention: A Case Study of Two Children with Autism Spectrum Disorder","DOI":"10.1109\/MIM.2023.10121412","Publication year":2023,"Abstract":"Individuals with autism spectrum disorder (ASD) often face barriers in accessing opportunities across a range of educational, employment, and social contexts. One of these barriers is the development of effective communication skills sufficient for navigating the social demands of everyday environments. Fortunately, researchers have established evidence-based practices (EBP) for teaching critical communication skills to individuals with ASD [1]. One EBP that has received a great deal of attention over the last few decades is technology-aided instruction and intervention (TAII) [1], [2]. TAII is an instructional practice in which technology is an essential component and is used to facilitate behavior change. Further, it encompasses a wide range of applications including computer-assisted instruction, virtual and augmented reality, augmentative and alternative communication, and robot-assisted intervention [2].","LowLevel":"computer aided instruction;learning algorithms;medical disorders;medical robotics;medical signal processing;teaching","MidLevel":"education;artificial intelligence;training;medical;data;robotics;sensors","HighLevel":"industries;technology;use cases","Venue":"IEEE Instrum. Meas. Mag. (USA)","Top20AbsAndTags":[222,443,19,394,414,34,312,219,184,103,368,128,240,367,371,56,372,188,174,17],"Top20Abs":[222,443,394,219,368,209,184,240,34,367,103,128,188,414,351,137,203,312,442,61],"Top20Tags":[230,222,394,417,108,9,455,33,165,34,372,17,425,326,428,204,391,363,115,367],"Citation":"Welch, K. C., Pennington, R., Vanaparthy, S., Do, H. M., Narayanan, R., Popa, D., Barnes, G., & Kuravackel, G. (2023). Using Physiological Signals and Machine Learning Algorithms to Measure Attentiveness During Robot-Assisted Social Skills Intervention: A Case Study of Two Children with Autism Spectrum Disorder. IEEE Instrumentation &amp; Measurement Magazine, 26(3), 39\u201345. https:\/\/doi.org\/10.1109\/mim.2023.10121412\n"},{"Title":"AI for Immersive Metaverse Experience","DOI":"10.1145\/3570991.3571045","Publication year":2023,"Abstract":"Metaverse has received a huge attention in recent times with several Big Techs having invested in this concept. Accenture defines the metaverse as \"an evolution of the Internet that enables a user to move beyond 'browsing' to 'inhabiting' in a persistent, shared experience that spans the spectrum of our real world to the fully virtual and in between\". The evolution that Metaverse brings can be seen along three dimensions: 1) shift towards spatial experiences: which includes 2D, 3D, augmented, virtual, and mixed reality immersive experiences, 2) shared co-presence: where users experience a persistent shared space with a sense of co-presence with others, and 3) trusted identities and transactions to address challenges of fake identities, products, and transactions as present in today's internet. For example, a retail marketplace, on Metaverse could be seen as an immersive spatial experience where users can shop along with their families and friends who join virtually in the same environment. The sense of shared co-presence gives them the ability to discuss about products in real time and persistency gives them ability to come back to the same space. This evolution opens an enormous opportunity to rethink the digital experiences future applications would offer to the people. AI would be the core engine behind making these experiences richer, immersive, and engaging. The role of AI, in the Metaverse, is broad; however, in this tutorial, we will focus on two areas where AI will play a major role in shaping up the form and function of the Metaverse by: 1) bringing more realism in Metaverse with high fidelity immersive content generated through AI techniques and 2) enhancing user interactions by bringing more intelligence in the interaction modes.","LowLevel":"artificial intelligence;internet;user experience","MidLevel":"artificial intelligence;human factors;networks;liberal arts","HighLevel":"end users and user experience;technology;industries","Venue":"CODS-COMAD '23: Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)","Top20AbsAndTags":[377,249,363,193,355,442,226,328,204,448,354,244,440,215,338,79,268,125,353,219],"Top20Abs":[377,249,355,354,442,193,363,226,204,328,244,440,448,215,338,267,125,411,219,79],"Top20Tags":[308,330,58,79,377,213,163,339,391,159,415,18,254,242,244,327,365,142,209,368],"Citation":"Dubey, A., Bhardwaj, N., Upadhyay, A., & Ramnani, R. (2023). AI for Immersive Metaverse Experience. Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD). https:\/\/doi.org\/10.1145\/3570991.3571045\n"},{"Title":"Gaze &amp; Tongue: A Subtle, Hands-Free Interaction for Head-Worn Devices","DOI":"10.1145\/3544549.3583930","Publication year":2023,"Abstract":"Gaze tracking allows hands-free and voice-free interaction with computers, and has gained more use recently in virtual and augmented reality headsets. However, it traditionally uses dwell time for selection tasks, which suffers from the Midas Touch problem. Tongue gestures are subtle, accessible and can be sensed non-intrusively using an IMU at the back of the ear, PPG and EEG. We demonstrate a novel interaction method combining gaze tracking with tongue gestures for gaze-based selection faster than dwell time and multiple selection options. We showcase its usage as a point-and-click interface in three hands-free games and a musical instrument.","LowLevel":"electroencephalography;gaze tracking;gesture recognition;human computer interaction;medical signal processing;musical instruments","MidLevel":"human factors;computer vision;audio;data;medical;sensors;input;human-computer interaction","HighLevel":"end users and user experience;technology;industries","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[32,75,277,259,177,127,242,80,172,348,419,384,43,237,366,455,104,435,424,415],"Top20Abs":[32,75,277,177,127,172,259,43,419,435,202,242,415,348,66,288,366,375,221,118],"Top20Tags":[165,32,455,417,384,259,198,108,72,180,230,80,104,217,284,53,253,242,415,123],"Citation":"Gemicioglu, T., Winters, R. M., Wang, Y.-T., Gable, T. M., Paradiso, A., & Tashev, I. J. (2023). Gaze &amp; Tongue: A Subtle, Hands-Free Interaction for Head-Worn\u00a0Devices. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583930\n"},{"Title":"Metaverse -An overview of daily usage and risks","DOI":"10.1109\/OTCON56053.2023.10113922","Publication year":2023,"Abstract":"Metaverse is a new technology that is in rapid development. The new technology is a digital revolution that integrates into every facet of our physical existence. The term \"metaverse\" was coined to fit this revolutionary vision. Social networks, video conferencing, 3D virtual worlds (such as virtual reality chat), augmented reality applications (such as Poke&#769;mon Go), and artificial avatars are just a few examples of computer-mediated virtual environments (such as Upland). Metaverse is composed of two words: Meta: is a Greek prefix that means \"after\" or \"beyond\" and Universe. In other words, the Metaverse is a post-reality world, a multiuser environment that integrates actual reality with digital virtuality and describes a possible synthetic ecosystem with physical linkages. This research study provides an overview of the Metaverse's daily usage, risks, and practical uses. Finally, you will see some provided case studies as suggested improvements that can enhance the Metaverse experience, to make it even more practical.","LowLevel":"avatars;computer games;human computer interaction;social networking;teleconferencing","MidLevel":"liberal arts;human-computer interaction;collaboration;presence","HighLevel":"end users and user experience;industries;use cases","Venue":"2022 OPJU International Technology Conference on Emerging Technologies for Sustainable Development (OTCON)","Top20AbsAndTags":[377,193,363,355,226,204,251,328,442,354,215,244,411,436,448,440,439,44,313,279],"Top20Abs":[377,226,355,354,193,363,204,442,251,328,215,244,411,440,436,448,313,374,208,439],"Top20Tags":[193,377,164,213,363,411,59,219,204,439,368,402,72,302,279,251,254,351,414,66],"Citation":"Yaqob, M., & Hafez, M. M. (2023). Metaverse -An overview of daily usage and risks. 2022 OPJU International Technology Conference on Emerging Technologies for Sustainable Development (OTCON). https:\/\/doi.org\/10.1109\/otcon56053.2023.10113922\n"},{"Title":"ARound the Smartphone: Investigating the Effects of Virtually-Extended Display Size on Spatial Memory","DOI":"10.1145\/3544548.3581438","Publication year":2023,"Abstract":"Smartphones conveniently place large information spaces in the palms of our hands. While research has shown that larger screens positively affect spatial memory, workload, and user experience, smartphones remain fairly compact for the sake of device ergonomics and portability. Thus, we investigate the use of hybrid user interfaces to virtually increase the available display size by complementing the smartphone with an augmented reality head-worn display. We thereby combine the benefits of familiar touch interaction with the near-infinite visual display space afforded by augmented reality. To better understand the potential of virtually-extended displays and the possible issues of splitting the user's visual attention between two screens (real and virtual), we conducted a within-subjects experiment with 24 participants completing navigation tasks using different virtually-augmented display sizes. Our findings reveal that a desktop monitor size represents a \"sweet spot\" for extending smartphones with augmented reality, informing the design of hybrid user interfaces.","LowLevel":"computer displays;data visualization;ergonomics;human computer interaction;mobile computing;smartphones;user experience;user interfaces","MidLevel":"human factors;telecommunication;data;liberal arts;human-computer interaction;display technology","HighLevel":"displays;technology;industries;end users and user experience","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[419,424,348,141,279,27,364,129,288,211,304,221,365,432,422,75,70,82,66,63],"Top20Abs":[419,141,364,211,129,279,393,432,288,153,304,225,278,276,237,224,75,70,422,424],"Top20Tags":[419,339,142,365,382,424,238,29,415,348,414,221,79,309,70,279,37,422,288,410],"Citation":"Hubenschmid, S., Zagermann, J., Leicht, D., Reiterer, H., & Feuchtner, T. (2023). ARound the Smartphone: Investigating the Effects of Virtually-Extended Display Size on Spatial Memory. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581438\n"},{"Title":"Embossing micro-fabrication-based epidermal electrode with a CNT-composed metal film as human&ndash;machine interface","DOI":"10.1007\/s10854-023-10599-0","Publication year":2023,"Abstract":"Recent years witness the rapid development of the internet of things and artificial intelligence, which ignites the worldwide research needs for flexible electronics to fulfill intelligent human&ndash;machine interfaces. As a promising future technology, the flexible epidermal electrode attracts great research enthusiasm in the field of intelligent robots, human&ndash;machine interfaces, virtual and augmented reality, medical treatment, health monitoring, and other emerging applications. However, it is still the main challenge to enhance the vertical charge conduction bridging the interface of metal electrode and human skin, which constructs an insurmountable obstacle for wide usage. In this article, a low-cost embossing fabrication is introduced to pattern a stretchable epidermal electrode device that consists of a carbon-nanotube (CNT)-composed metal thin film. Benefited from the excellent flexibility and stretchability, the fabricated electrode can conformably attach to the skin and collect high-quality bioelectrical signals. Moreover, this epidermal electrode features good conductivity along the vertical path for acquiring the epidermal charge, due to conductive CNTs bridging the interface of skin and this epidermal electrode. With this device structure configuration, the impedance between skin and epidermal electrode could effectively reduce by more than 50%. The conformal contact mechanism relies upon an ultra-thin metal film on skin texture, which makes the device achieves high sensitivity and durability in a human&ndash;machine interface. The electrocardiogram (ECG) and electromyography (EMG) demonstration and gesture recognition further conclude that this technology has a chance to bring up the potential of clinical applications as a human&ndash;machine interface. &copy; 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","LowLevel":"carbon nanotubes;electrocardiography;fabrication;flexible electronics;intelligent robots;metallic films;nanocrystals;textures;thin films;wearable technology","MidLevel":"medical;robotics;inspection, safety and quality;wearables;input;manufacturing;human-computer interaction;other;display technology","HighLevel":"displays;end users and user experience;use cases;industries;technology;other","Venue":"J Mater Sci Mater Electron","Top20AbsAndTags":[138,441,407,288,295,352,373,174,205,397,281,331,419,215,463,453,19,361,343,53],"Top20Abs":[138,441,352,281,288,205,174,419,397,19,373,463,12,159,286,116,448,439,53,112],"Top20Tags":[441,435,74,207,323,189,312,361,215,324,291,463,0,138,307,285,445,397,170,432],"Citation":"Liu, G., Xu, C., Ren, J., Liu, Y., Yao, L., Xie, Y., Zhang, T., & Liu, Y. (2023). Embossing micro-fabrication-based epidermal electrode with a CNT-composed metal film as human\u2013machine interface. Journal of Materials Science: Materials in Electronics, 34(16). https:\/\/doi.org\/10.1007\/s10854-023-10599-0\n"},{"Title":"Experimental investigation of the tropospheric temperature gradient with Flightradar24","DOI":"10.1119\/5.0141246","Publication year":2023,"Abstract":"Sitting on the terrace on a warm summer evening and looking up at the sky, we often wonder where an airplane is going to fly. Based on the observed flight direction, we can make assumptions, but rarely check if they are really correct. The app Flightradar24 offers the possibility to confirm our assumptions. For this purpose, you just need to target the aircraft with your smartphone or tablet and then you will get augmented reality (AR) information for the flight displayed in the live image (Fig. 1): takeoff and destination, distance to the observer, airline, and type and photo of the aircraft. By tapping on the AR window, more detailed information on the flight times, the distance traveled, and the exact flight route is displayed on a map (Fig. 2). In this way, data of particular physical interest can also be retrieved, namely the aircraft's current altitude, its speed, its position, the prevailing wind speed, and the outside temperature. This corresponds to a retrieval of measurement data, so that the app can also be used for physical quasi-experiments. At this point, the investigation of the temperature profile of the troposphere will be described. Many further quantitative analyses can be found in Ref. 2. This approach to modeling also provides an example of handling real data in the sense of data literacy. [The copyright for the referenced work is owned by Author(s). Copies of full-text articles should only be made or obtained from the publisher or authorized sources.]","LowLevel":"aircraft;aircraft displays;atmospheric temperature;travel industry;troposphere;wind","MidLevel":"engineering;transportation;aviation and aerospace;farming and natural science;other;display technology","HighLevel":"industries;technology;displays;other","Venue":"Phys. Teach. (USA)","Top20AbsAndTags":[254,362,52,374,77,460,178,9,166,486,148,76,105,42,81,5,237,119,4,69],"Top20Abs":[52,254,77,178,486,460,374,9,4,166,412,362,42,479,105,7,183,81,316,76],"Top20Tags":[64,254,370,90,69,96,419,339,407,166,422,392,233,109,148,214,282,71,345,198],"Citation":"Vogt, P., & Kasper, L. (2023). Experimental investigation of the tropospheric temperature gradient with Flightradar24. The Physics Teacher, 61(2), 148\u2013149. https:\/\/doi.org\/10.1119\/5.0141246\n"},{"Title":"A New Trend in Car Personalization Based on Augmented Reality: A Study","DOI":"10.1007\/978-3-031-28225-6_11","Publication year":2023,"Abstract":"The article points to a new trend occurring in marketing. In the car industry, there is a lot of money and that is why car companies come to the market with new ideas. Augmented reality has become one of the tools to transfer the 3D dimension to the customer. Publicly available applications are being created that allow us to personalize car models according to our own requirements. The customer can choose the type, colour, design and equipment of the car. He can then project the configured car into space using a mobile phone. In the article, we want to present and compare three freely available augmented reality applications from three automotive companies. Each of the evaluated applications has its advantages and disadvantages, but this article points to selected parameters and options for working with the application for the best possible experience. In the end, individual applications of augmented reality are compared. In the article, we wanted to show that applications can be used in marketing to create the first contact with the client. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"commerce;marketing;model automobiles","MidLevel":"automotive;sales and marketing;other","HighLevel":"industries;business;other","Venue":"EAI\/Springer Inno. Comm. Comp.","Top20AbsAndTags":[144,368,25,96,454,102,333,315,44,307,471,30,436,414,194,40,119,440,448,193],"Top20Abs":[144,368,454,25,315,96,44,307,414,119,193,102,448,436,440,194,30,207,35,171],"Top20Tags":[25,333,161,471,192,187,30,155,442,194,156,355,479,361,315,13,157,258,2,160],"Citation":"Hus\u00e1r, J., Hrehova, S., Knap\u010d\u00edkov\u00e1, L., & Trojanowska, J. (2023). A New Trend in Car Personalization Based on Augmented Reality: A Study. EAI\/Springer Innovations in Communication and Computing, 165\u2013178. https:\/\/doi.org\/10.1007\/978-3-031-28225-6_11\n"},{"Title":"Eye-Perspective View Management for Optical See-Through Head-Mounted Displays","DOI":"10.1145\/3544548.3581059","Publication year":2023,"Abstract":"Optical see-through (OST) head-mounted displays (HMDs) enable users to experience Augmented Reality (AR) support in the form of helpful real-world annotations. Unfortunately, the blend of the environment with virtual augmentations due to semitransparent OST displays often deteriorates the contrast and legibility of annotations. View management algorithms adapt the annotations' layout to improve legibility based on real-world information, typically captured by built-in HMD cameras. However, the camera views are different from the user's view through the OST display which decreases the final layout quality. We present eye-perspective view management that synthesizes high-fidelity renderings of the user's view to optimize annotation placement. Our method significantly improves over traditional camera-based view management in terms of annotation placement and legibility. Eye-perspective optimizations open up opportunities for further research on use cases relying on the user's true view through OST HMDs.","LowLevel":"cameras;helmet mounted displays;rendering","MidLevel":"graphics;wearables;input;display technology","HighLevel":"displays;technology","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[10,80,279,277,48,225,259,32,15,444,440,217,364,50,208,301,142,73,481,69],"Top20Abs":[10,225,48,15,208,444,50,279,142,440,80,73,0,364,69,32,217,278,44,481],"Top20Tags":[259,277,72,239,440,301,69,316,217,21,288,331,206,238,71,120,279,182,32,424],"Citation":"Emsenhuber, G., Langlotz, T., Kalkofen, D., Sutton, J., & Tatzgern, M. (2023). Eye-Perspective View Management for Optical See-Through Head-Mounted Displays. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581059\n"},{"Title":"The GW Community Medi-Corps Program: A Mobile Mixed-Reality Immersive Learning Center","DOI":"10.1007\/978-3-031-21569-8_32","Publication year":2023,"Abstract":"The Community Medi-Corps Program-designed and implemented by the George Washington University (GW) School of Medicine and Health Sciences (SMHS) faculty with Growth and Opportunity Virginia funding (GO Virginia)-is aimed at leveraging the power of community, educational institutions, mentors, industry, and business partners to close the opportunity gap, transform student learning, and enrich the regional workforce. This program transforms educational experience through innovative virtual reality, augmented reality, and a mix between the two that is the enhanced reality (e-REAL). Students will be better prepared in the pathways they choose for high demand health and life sciences industry jobs that will help grow the economy.","LowLevel":"computer aided instruction;educational courses;educational institutions;mobile computing","MidLevel":"education;telecommunication;training","HighLevel":"industries;use cases","Venue":"Innovative Approaches to Technology-Enhanced Learning for the Workplace and Higher Education: Proceedings of 'The Learning Ideas Conference' 2022. Lecture Notes in Networks and Systems (581)","Top20AbsAndTags":[56,372,171,42,134,47,184,246,113,46,312,380,88,163,51,320,140,136,296,82],"Top20Abs":[372,56,42,312,184,46,163,188,47,88,369,140,113,388,354,380,171,134,342,82],"Top20Tags":[56,51,262,171,93,246,139,154,134,320,405,128,82,145,196,103,185,88,136,164],"Citation":"Salvetti, F., Capshaw, T. L., Zanin, L., O\u2019Connor, K. C., Zeng, Q., & Bertagni, B. (2022). The GW Community Medi-Corps Program: A Mobile Mixed-Reality Immersive Learning Center. Lecture Notes in Networks and Systems, 335\u2013350. https:\/\/doi.org\/10.1007\/978-3-031-21569-8_32\n"},{"Title":"Omnidirectional Haptic Stimulation System via Pneumatic Actuators for Presence Presentation","DOI":"10.3390\/s23020584","Publication year":2023,"Abstract":"Recently, remote meetings and work-from-home have become more common, reducing the opportunities for face-to-face communication. To facilitate communication among remote workers, researchers have focused on virtual space technology and spatial augmented reality technology. Although these technologies can enhance immersiveness in collaborative work, they face the challenge of fostering a sense of physical contact. In this work, we aimed to foster a sense of presence through haptic stimulation using pneumatic actuators. Specifically, we developed a choker-type wearable device that presents various pressure patterns around the neck; the pattern presented depends on the message the device must convey. Various combinations of haptic presentation are achieved by pumping air to the multiple pneumatic actuators attached to the choker. In addition, we conducted experiments involving actuators of different shapes to optimize the haptic presentation. When linked with a smartphone, the proposed device can present pressure patterns to indicate incoming calls and notifications, to give warning about an obstacle that one who is texting might miss while walking, and to provide direction to a pedestrian. Furthermore, the device can be used in a wide range of applications, from those necessary in daily living to those that enhance one's experience in the realm of entertainment. For example, haptic feedback that synchronizes with the presence of a singer or with the rhythm of a song one listens to or with a performer's movements during a stage performance will immerse users in an enjoyable experience.","LowLevel":"actuators;groupware;haptic interfaces;pneumatic actuators;smartphones","MidLevel":"input;telecommunication;collaboration;industrial equipment;liberal arts;semiconductors","HighLevel":"industries;technology;use cases","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[167,239,129,203,219,106,112,283,68,303,126,304,338,366,337,251,464,253,27,160],"Top20Abs":[167,106,129,253,239,219,464,338,366,251,283,440,304,303,203,261,126,420,259,234],"Top20Tags":[167,239,348,199,337,424,279,419,29,82,455,339,112,129,126,203,93,221,303,27],"Citation":"Yoshida, S., Xie, H., & Miyata, K. (2023). Omnidirectional Haptic Stimulation System via Pneumatic Actuators for Presence Presentation. Sensors, 23(2), 584. https:\/\/doi.org\/10.3390\/s23020584\n"},{"Title":"Text Me if You Can: Investigating Text Input Methods for Cyclists","DOI":"10.1145\/3544549.3585734","Publication year":2023,"Abstract":"Cycling is emerging as a relevant alternative to cars. However, the more people commute by bicycle, the higher the number of cyclists who use their smartphones on the go and endanger road safety. To better understand input while cycling, in this paper, we present the design and evaluation of three text input methods for cyclists: (1) touch input using smartphones, (2) midair input using a Microsoft Hololens 2, and (3) a set of ten physical buttons placed on both sides of the handlebar. We conducted a controlled indoor experiment (N = 12) on a bicycle simulator to evaluate these input methods. We found that text input via touch input was faster and less mentally demanding than input with midair gestures and physical buttons. However, the midair gestures were the least error-prone, and the physical buttons facilitated keeping both hands on the handlebars and were more intuitive and less distracting.","LowLevel":"bicycles;gesture recognition;road safety;smartphones","MidLevel":"human factors;telecommunication;transportation;inspection, safety and quality;liberal arts;input","HighLevel":"end users and user experience;technology;industries;use cases","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[424,248,419,177,313,353,27,250,208,75,66,172,216,80,276,351,103,79,118,105],"Top20Abs":[424,80,313,200,331,177,419,75,224,248,250,353,351,172,208,364,66,79,211,1],"Top20Tags":[288,339,419,129,27,424,93,221,242,240,248,82,182,126,216,309,220,387,147,422],"Citation":"Matviienko, A., Durand-Pierre, J.-B., Cvancar, J., & M\u00fchlh\u00e4user, M. (2023). Text Me if You Can: Investigating Text Input Methods for Cyclists. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585734\n"},{"Title":"Rate-Distortion Modeling for Bit Rate Constrained Point Cloud Compression","DOI":"10.1109\/TCSVT.2022.3223898","Publication year":2023,"Abstract":"As being one of the main representation formats of 3D real world and well-suited for virtual reality and augmented reality applications, point clouds have gained a lot of popularity. In order to reduce the huge amount of data, a considerable amount of research on point cloud compression has been done. However, given a target bit rate, how to properly choose the color and geometry quantization parameters for compressing point clouds is still an open issue. In this paper, we propose a rate-distortion model based quantization parameter selection scheme for bit rate constrained point cloud compression. Firstly, to overcome the measurement uncertainty in evaluating the distortion of the point clouds, we propose a unified model to combine the geometry distortion and color distortion. In this model, we take into account the correlation between geometry and color variables of point clouds and derive a dimensionless quantity to represent the overall quality degradation. Then, we derive the relationships of overall distortion and bit rate with the quantization parameters. Finally, we formulate the bit rate constrained point cloud compression as a constrained minimization problem using the derived polynomial models and deduce the solution via an iterative numerical method. Experimental results show that the proposed algorithm can achieve optimal decoded point cloud quality at various target bit rates, and substantially outperform the video-rate-distortion model based point cloud compression scheme.","LowLevel":"data compression;iterative methods;minimisation;polynomials;quantisation (signal);rate distortion theory;video coding","MidLevel":"artificial intelligence;device energy management;video;data;other","HighLevel":"displays;technology;other","Venue":"IEEE Trans. Circuits Syst. Video Technol. (USA)","Top20AbsAndTags":[451,92,98,321,318,340,438,5,298,99,433,120,471,95,488,76,358,409,270,132],"Top20Abs":[451,92,98,321,318,340,438,120,99,5,471,270,433,488,95,300,76,298,132,234],"Top20Tags":[15,321,99,144,119,213,32,172,256,309,432,276,265,308,433,312,274,428,300,358],"Citation":"Gao, P., Luo, S., & Paul, M. (2023). Rate-Distortion Modeling for Bit Rate Constrained Point Cloud Compression. IEEE Transactions on Circuits and Systems for Video Technology, 33(5), 2424\u20132438. https:\/\/doi.org\/10.1109\/tcsvt.2022.3223898\n"},{"Title":"Interactive AR Applications for Nonspeaking Autistic People? - A Usability Study","DOI":"10.1145\/3544548.3580721","Publication year":2023,"Abstract":"About one-third of autistic people are nonspeaking, and most are never provided access to an effective alternative to speech. Thoughtfully designed AR applications could provide members of this population with structured learning opportunities, including training on skills that underlie alternative forms of communication. A fundamental step toward creating such opportunities, however, is to investigate nonspeaking autistic people's ability to tolerate a head-mounted AR device and to interact with virtual objects. We present the first study to examine the usability of an interactive AR-based application by this population. We recruited 17 nonspeaking autistic subjects to play a HoloLens 2 game we developed that involved holographic animations and buttons. Almost all subjects tolerated the device long enough to begin the game, and most completed increasingly challenging tasks that involved pressing holographic buttons. Based on the results, we discuss best practice design and process recommendations. Our findings contradict prevailing assumptions about nonspeaking autistic people and thus open up exciting possibilities for AR-based solutions for this understudied and underserved population.","LowLevel":"computer animation;computer games;handicapped aids;holographic displays","MidLevel":"graphics;liberal arts;medical;display technology","HighLevel":"displays;technology;industries","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[367,93,344,219,34,180,265,63,414,149,27,51,111,26,103,163,348,222,79,400],"Top20Abs":[367,93,34,222,180,265,344,27,219,103,447,408,111,163,253,414,63,400,118,149],"Top20Tags":[93,204,72,410,214,180,63,394,59,275,106,414,344,92,164,376,14,26,147,254],"Citation":"Nazari, A., Shahidi, A., Kaufman, K. M., Bondi, J. E., Alabood, L., Jaswal, V. K., Krishnamurthy, D., & Wang, M. (2023). Interactive AR Applications for Nonspeaking Autistic People? - A Usability Study. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580721\n"},{"Title":"Generating Diffraction Efficiency Profiles in Bayfol<sup>&reg;<\/sup> HX vHOEs","DOI":"10.1117\/12.2650108","Publication year":2023,"Abstract":"Bayfol&reg; HX photopolymer films prove themselves as easy-to-process recording materials for volume holographic optical elements (vHOEs) and are available in customized grade at industrial scale. Their full-color (RGB) recording and replay capabilities are two of their major advantages. Moreover, the adjustable diffraction efficiency, tunable angular and spectral selectivity of vHOEs recorded into Bayfol&reg; HX as well as their unmatched optical clarity enables superior invisible \"off Bragg\" optical functionality. As a film product, the replication of vHOEs in Bayfol&reg; HX can be carried out in a highly cost-efficient and purely photonic roll-to-roll (R2R) process. Utilizing thermoplastic substrates, Bayfol&reg; HX was demonstrated to be compatible to state-of-the-art plastic processing techniques like thermoforming, film insert molding and casting, which opened up using a variety of industry-proven integration technologies for vHOEs. Therefore, Bayfol&reg; HX makes its way in applications in the field of augmented reality such as Head-up-Displays (HUD) and Head-mounted-Displays (HMD), in free-space combiners, in plastic optical waveguides, and in transparent screens. Also, vHOEs made from Bayfol&reg; HX are utilized in highly sophisticated spectrometers in astronomy as well as in narrow band notch filters for eyeglasses against laser strikes. See through applications such as, HMD and HUD, have demanding performance requirements on combiner and imaging technologies such as efficiency, optical function, and clarity. The properties of Bayfol&reg; HX make it well suited to solve these challenges in primary display, and near-infrared imaging applications such as eye-tracking, while maintaining the requirements on optical performance. We demonstrate practical examples of Bayfol&reg; HX vHOE's using novel holography techniques for spatially varying diffraction efficiency. &copy; 2023 SPIE.","LowLevel":"diffraction efficiency;eye tracking;holographic optical elements;holography;infrared devices;notch filters;thermography","MidLevel":"computer vision;sensors;input;optics;human-computer interaction;networks;other;display technology","HighLevel":"displays;technology;other;end users and user experience","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[344,444,343,278,364,332,74,228,317,169,180,225,217,149,158,80,237,431,141,441],"Top20Abs":[343,444,344,278,317,74,332,364,228,158,169,225,180,217,431,141,445,288,441,481],"Top20Tags":[344,470,149,332,444,191,225,307,265,180,75,431,433,80,21,279,364,201,278,206],"Citation":"Bruder, F.-K., Frank, J., Hansen, S., Lorenz, A., Manecke, C., Mills, J., Pitzer, L., Pochorovski, I., & R\u00f6lle, T. (2023). Generating diffraction efficiency profiles in Bayfol HX vHOEs. Practical Holography XXXVII: Displays, Materials, and Applications. https:\/\/doi.org\/10.1117\/12.2650108\n"},{"Title":"Research on Measuring Method of Ball Indentation Based on MATLAB Image Edge Detection","DOI":"10.1007\/978-981-99-1230-8_4","Publication year":2023,"Abstract":"Ball pressure test is an important test to evaluate the heat resistance of non-metallic materials. At present, most of the tests are judged by manual measurement, with low accuracy. In this paper, the digital image measurement technology of MATLAB is used to transform and enhance the collected indentation image. The Gaussian Laplace operator is used to detect the edge of the image, find the maximum value of the coordinate difference between each element point on the image edge and other element points, traverse all the element points on the image edge, and put the obtained maximum value into the maximum value array; continue to find the maximum value in the maximum value array to obtain the diameter of the ball pressure indentation image, thus improving the measurement accuracy and realizing the intelligent measurement of the diameter of the ball pressure detection image. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"edge detection;image enhancement;indentation","MidLevel":"computer vision;other","HighLevel":"technology;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[76,20,169,297,438,453,98,290,462,74,345,456,473,471,99,344,433,374,318,214],"Top20Abs":[76,20,169,297,453,438,98,290,74,345,462,473,433,344,99,471,318,374,214,456],"Top20Tags":[20,434,76,458,187,295,7,438,462,132,200,456,115,487,470,190,2,323,453,311],"Citation":"Zhou, Y., Wang, X., Liu, Y., Wang, X., Li, Z., & Wang, Y. (2023). Research on Measuring Method of Ball Indentation Based on MATLAB Image Edge Detection. Smart Innovation, Systems and Technologies, 41\u201349. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_4\n"},{"Title":"SOC Estimation of Lithium Titanate Battery Based on Variable Temperature Equivalent Model","DOI":"10.1007\/978-981-99-1230-8_25","Publication year":2023,"Abstract":"To ensure the normal service life and battery safety, accurate estimation of state of charge (SOC) of lithium titanate ion battery is of great significance. For the purpose of improving the accuracy of SOC estimation further, an equivalent circuit model considering temperature factors is established according to the external properties of lithium titanate ion battery at various ambient temperatures. Then, based on the proposed variable temperature equivalent model, the battery SOC is estimated using the Cubature Kalman Filter (CKF) algorithm. Finally, the SOC estimation outcomes are compared with the real value, and it is found that the maximum estimation error is within &plusmn;2% at various temperature conditions. According to the test results, the suggested SOC estimation algorithm on the basis of the variable temperature equivalent model has good temperature adaptability and high estimation accuracy. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"battery management systems;equivalent circuits;kalman filters;lithium compounds;lithium-ion batteries","MidLevel":"education;artificial intelligence;other","HighLevel":"industries;technology;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[160,194,283,255,466,245,461,12,387,475,471,438,435,467,486,187,106,20,307,464],"Top20Abs":[160,255,194,245,461,12,466,387,283,438,467,435,471,307,486,475,20,106,464,173],"Top20Tags":[95,122,260,483,308,320,316,18,58,423,160,115,475,377,289,339,63,254,415,458],"Citation":"Song, C., Luo, J., Chen, X., & Peng, Z. (2023). SOC Estimation of Lithium Titanate Battery Based on Variable Temperature Equivalent Model. Smart Innovation, Systems and Technologies, 285\u2013298. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_25\n"},{"Title":"Course Homework Reform in Universities Based on Extended Reality","DOI":"10.1007\/978-981-99-2449-3_25","Publication year":2023,"Abstract":"In view of the serious issues commonly existing with coursework in Chinese universities at present, such as its original function weakening or being suppressed, its form being too abstract and lack of elaborate design. In order to resolve these problems observed&nbsp;in usual homework, this paper proposes a reform scheme for cloud coursework based on Extended Reality and Bloom model. It mainly includes five key parts, such as interaction, scene, comprehensive evaluation, digital twin and data analysis. In addition, the reform&rsquo;s effectiveness is illustrated through&nbsp;amassing&nbsp;and&nbsp;comparing the feedback of&nbsp;college&nbsp;students&nbsp;for&nbsp;teaching. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"e-learning;students","MidLevel":"education;users;medical","HighLevel":"industries;end users and user experience","Venue":"Commun. Comput. Info. Sci.","Top20AbsAndTags":[475,188,447,482,457,474,161,463,467,414,471,312,157,486,454,369,443,246,134,466],"Top20Abs":[475,188,447,457,482,474,467,486,157,463,414,479,312,412,20,471,458,155,466,453],"Top20Tags":[161,188,454,413,341,440,369,207,31,446,443,485,314,308,257,442,154,300,470,134],"Citation":"Fu, J., Liu, Y., Mi, C., Peng, X., & Xiao, J. (2023). Course Homework Reform in Universities Based on Extended Reality. Communications in Computer and Information Science, 281\u2013288. https:\/\/doi.org\/10.1007\/978-981-99-2449-3_25\n"},{"Title":"A Review of Temporal Network Analysis and Applications","DOI":"10.1007\/978-981-99-1230-8_1","Publication year":2023,"Abstract":"With the continuous development of network science, a single and static network structure has become more and more difficult to portray various complex systems, while the temporal network is becoming an effective tool to solve the above problem. At present, the research on the temporal network is still at the stage of primary, and there are still many worthy areas to be further explored. Inspired by this, in our paper, we review the modeling and representation of temporal networks, the structural characteristics and statistical properties of networks, and the application analysis; analyze the weaknesses of the current research; and look forward to the future development aspects. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"other","MidLevel":"other","HighLevel":"other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[268,447,263,453,457,307,404,426,341,273,358,7,476,256,468,157,160,144,402,20],"Top20Abs":[268,453,447,263,307,457,404,426,341,273,157,402,358,476,256,144,454,468,395,483],"Top20Tags":[187,7,462,200,461,231,2,425,280,258,345,474,265,233,423,226,378,28,416,212],"Citation":"Yu, J., Xiao, B., & Cui, Y. (2023). A Review of Temporal Network Analysis and Applications. Smart Innovation, Systems and Technologies, 1\u201310. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_1\n"},{"Title":"Research on the Application of 3D Modeling Technology in Mechanical Structure Teaching","DOI":"10.1007\/978-981-99-1230-8_11","Publication year":2023,"Abstract":"Due to the constant acceleration of the upgrading of equipment, the mechanical structure teaching ability of colleges and universities is required to be higher and higher. Therefore, it is urgent to explore some new and efficient teaching methods to enrich the teaching means. In this paper, the current situation and characteristics of 3D modeling technology are introduced first. Then, the problems and reasons for mechanical structure teaching in college are analyzed in detail. Finally, combined with the characteristics of \"3D modeling technology\", the paper puts forward concrete ideas to improve the quality of mechanical structure teaching. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"3d modeling;three dimensional computer graphics","MidLevel":"graphics;manufacturing","HighLevel":"industries;technology","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[157,414,467,458,307,342,136,312,446,3,459,466,443,391,372,471,52,87,363,246],"Top20Abs":[458,414,312,342,459,3,412,466,157,391,131,471,446,467,340,443,371,52,307,453],"Top20Tags":[467,157,307,464,24,207,475,488,446,183,438,433,278,480,283,413,468,369,443,290],"Citation":"Qin, L., Yang, J., Tang, L., Gan, Q., & Wang, H. (2023). Research on the Application of 3D Modeling Technology in Mechanical Structure Teaching. Smart Innovation, Systems and Technologies, 121\u2013129. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_11\n"},{"Title":"Ship Target Detection in Remote Sensing Image Based on Improved RetinaNet","DOI":"10.1007\/978-981-99-1230-8_10","Publication year":2023,"Abstract":"Ship image target detection has important applications for ship management. In recent years, target detection based on deep learning has been widely studied in visual ship target detection. However, due to the difference and overlap of ship targets, the target loss rate is high. In order to solve the problem, this paper proposes a target detection algorithm based on improved RetinaNet for ship image target detection. The cyclical focal loss function and CIOU loss function are used to increase the training times of negative samples in the middle of training, which effectively increases ship detection precision by 2.5%. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"deep learning;image enhancement;image recognition;remote sensing","MidLevel":"artificial intelligence;human factors;computer vision;medical;sensors","HighLevel":"end users and user experience;technology;industries","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[76,116,453,202,431,20,270,438,485,345,79,476,273,484,461,274,49,177,341,326],"Top20Abs":[116,76,202,431,270,20,485,345,476,438,273,177,79,324,49,484,326,32,453,274],"Top20Tags":[453,323,487,431,341,20,480,483,470,484,76,300,433,0,160,115,95,321,488,49],"Citation":"Sun, Y., & Fan, T. (2023). Ship Target Detection in Remote Sensing Image Based on Improved RetinaNet. Smart Innovation, Systems and Technologies, 109\u2013119. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_10\n"},{"Title":"Visual and haptic feedback in detecting motor imagery within a wearable brain-computer interface","DOI":"10.1016\/j.measurement.2022.112304","Publication year":2023,"Abstract":"This paper presents a wearable brain-computer interface relying on neurofeedback in extended reality for the enhancement of motor imagery training. Visual and vibrotactile feedback modalities were evaluated when presented either singularly or simultaneously. Only three acquisition channels and state-of-the-art vibrotactile chest-based feedback were employed. Experimental validation was carried out with eight subjects participating in two or three sessions on different days, with 360 trials per subject per session. Neurofeedback led to statistically significant improvement in performance over the two\/three sessions, thus demonstrating for the first time functionality of a motor imagery-based instrument even by using an utmost wearable electroencephalograph and a commercial gaming vibrotactile suit. In the best cases, classification accuracy exceeded 80 % with more than 20 % improvement with respect to the initial performance. No feedback modality was generally preferable across the cohort study, but it is concluded that the best feedback modality may be subject-dependent. All rights reserved Elsevier.","LowLevel":"brain-computer interfaces;electroencephalography;feedback;haptic interfaces;medical signal processing;neurophysiology","MidLevel":"human factors;farming and natural science;data;medical;sensors;input;human-computer interaction","HighLevel":"industries;technology;end users and user experience","Venue":"Measurement (Netherlands)","Top20AbsAndTags":[384,167,165,91,108,463,250,367,383,183,54,112,294,126,60,255,87,416,330,358],"Top20Abs":[384,91,167,367,383,165,108,183,294,255,463,416,60,54,288,87,175,79,128,200],"Top20Tags":[165,384,167,250,417,108,112,230,253,126,240,313,419,424,314,421,422,386,394,330],"Citation":"Arpaia, P., Coyle, D., Donnarumma, F., Esposito, A., Natalizio, A., & Parvis, M. (2023). Visual and haptic feedback in detecting motor imagery within a wearable brain\u2013computer interface. Measurement, 206, 112304. https:\/\/doi.org\/10.1016\/j.measurement.2022.112304\n"},{"Title":"Augmented and&nbsp;Virtual Reality in&nbsp;Computer Science Education","DOI":"10.1007\/978-3-031-29800-4_45","Publication year":2023,"Abstract":"This paper shares and analyzes some experiences in the use of augmented and virtual reality, presenting technologies and methodologies leveraged for implementing three different projects. Such projects, carried out at the high school level, are used for discussing on the relevance of augmented and virtual reality for providing fundamental concepts on computer science education. To this end, an assessment toolkit is provided for an effective evaluation of this kind of activities. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"e-learning;education computing;engineering education","MidLevel":"education;medical","HighLevel":"industries","Venue":"Commun. Comput. Info. Sci.","Top20AbsAndTags":[184,124,188,161,442,307,440,44,187,207,171,312,1,163,194,485,363,185,306,25],"Top20Abs":[124,188,44,184,440,307,1,315,187,194,312,35,25,197,161,246,405,207,189,361],"Top20Tags":[369,485,184,161,434,442,459,135,272,30,341,171,207,446,443,413,232,154,134,256],"Citation":"Anastasi, G. F., & Munna, E. G. (2023). Augmented and\u00a0Virtual Reality in\u00a0Computer Science Education. Communications in Computer and Information Science, 601\u2013612. https:\/\/doi.org\/10.1007\/978-3-031-29800-4_45\n"},{"Title":"Prediction of Breast Cancer Via Deep Learning","DOI":"10.1007\/978-981-99-1230-8_8","Publication year":2023,"Abstract":"With the continuous progress of machine learning in image processing, artificial neural networks have more and more applications in medical image processing. Aiming at the method of selecting a BP neural network to realize the diagnosis of breast cancer pathological images, the method of deep learning is selected for prediction, which improves the prediction accuracy and obtains a more effective diagnosis effect. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"deep learning;diseases;forecasting;image enhancement;medical imaging;neural networks","MidLevel":"artificial intelligence;computer vision;medical;business planning and management","HighLevel":"industries;technology;business","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[484,323,138,116,341,458,160,480,468,163,456,20,358,144,207,485,259,81,464,76],"Top20Abs":[458,468,484,358,20,76,163,116,81,160,144,476,461,259,480,385,447,462,485,381],"Top20Tags":[323,341,432,480,43,170,207,0,456,138,488,484,470,483,431,464,433,463,285,442],"Citation":"Huang, Y. (2023). Prediction of Breast Cancer Via Deep Learning. Smart Innovation, Systems and Technologies, 87\u201397. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_8\n"},{"Title":"Detecting an Offset-Adjusted Similarity Score based on Duchenne Smiles","DOI":"10.1145\/3544549.3585709","Publication year":2023,"Abstract":"Detecting interpersonal synchrony in the wild through ubiquitous wearable sensing invites promising new social insights as well as the possibility of new interactions between humans-humans and humans-agents. We present the Offset-Adjusted SImilarity Score (OASIS), a real-time method of detecting similarity which we show working on visual detection of Duchenne smile between a pair of users. We conduct a user study survey (N = 27) to measure a user-based interoperability score on smile similarity and compare the user score with OASIS as well as the rolling window Pearson correlation and the Dynamic Time Warping (DTW) method. Ultimately, our results indicate that our algorithm has intrinsic qualities comparable to the user score and measures well to the statistical correlation methods. It takes the temporal offset between the input signals into account with the added benefit of being an algorithm which can be adapted to run in real-time will less computational intensity than traditional time series correlation methods.","LowLevel":"correlation methods;emotion recognition;open systems;real-time systems;social sciences computing;time series;ubiquitous computing","MidLevel":"education;artificial intelligence;human factors;policy;data;developers;input;human-computer interaction","HighLevel":"industries;technology;business;end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[208,182,84,471,366,311,142,166,439,212,110,205,316,263,419,79,70,409,99,15],"Top20Abs":[208,263,205,84,311,471,182,142,366,316,463,212,419,48,166,286,136,79,439,15],"Top20Tags":[377,110,182,328,239,121,339,352,410,70,409,160,170,243,104,36,242,254,387,433],"Citation":"Henneberg, M., Eghtebas, C., De Candido, O., Kunze, K., & Ward, J. A. (2023). Detecting an Offset-Adjusted Similarity Score based on Duchenne Smiles. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585709\n"},{"Title":"Are Industry 4.0 technologies enablers of lean? Evidence from manufacturing industries","DOI":"10.1108\/IJLSS-04-2021-0085","Publication year":2023,"Abstract":"&lt;b&gt;Purpose &lt;\/b&gt;This study aims to propose a conceptual model indicating the impact of Industry 4.0 (I4.0) technologies on lean tools. Additionally, it prioritizes I4.0 technologies for the digital transformation of lean plants. &lt;b&gt;Design\/methodology\/approach &lt;\/b&gt;The authors conducted a questionnaire-based survey to capture the perception of 115 experts of manufacturing industries from Germany, India, Taiwan and China. The impact of I4.0 on lean tools, using analysis of variance (ANOVA). Further, the authors drew a prioritization map of I4.0 on the employment of lean tools in manufacturing, using the Best-Worst Method (BWM). &lt;b&gt;Findings &lt;\/b&gt;The findings indicate that cloud manufacturing, simulation, industrial internet of things, horizontal and vertical integration impact 100% of the lean tools, while both cyber-security, big data analytics impact 93% of the lean tools and advanced robotics impact 74% of the lean tools. On the other hand, it is observed that augmented reality and additive manufacturing will impact 21% and 14% of the lean tools, respectively. &lt;b&gt;Practical implications &lt;\/b&gt;The results of this study would help practitioners draw up a strategic plan and roadmap for implementing lean 4.0. The amalgamation of lean with I4.0 technologies in the right combination would enhance speed productivity and facilitate autonomous operations. &lt;b&gt;Originality\/value &lt;\/b&gt;Studies exploring the influence of I4.0 on lean manufacturing lack comprehensiveness, testing and validation. Importantly, no studies in the recent past have explored mapping and prioritizing I4.0 technologies in the \"lean\" context. This study thereby attempts to establish a conceptual model, indicating the influence of I4.0 technologies on lean tools and presents the hierarchy of all digital technologies.","LowLevel":"lean production;manufacturing industries;production engineering computing;statistical analysis","MidLevel":"manufacturing;data;other;engineering","HighLevel":"industries;technology;other","Venue":"Int. J. Lean Six Sig. (UK)","Top20AbsAndTags":[40,33,390,378,13,146,159,37,151,321,91,315,429,237,179,259,2,434,281,135],"Top20Abs":[40,33,390,378,146,13,159,37,315,151,2,91,429,321,237,259,281,179,135,54],"Top20Tags":[388,159,370,2,227,174,185,407,411,45,12,360,28,77,273,289,429,205,179,122],"Citation":"Narula, S., Puppala, H., Kumar, A., Luthra, S., Dwivedy, M., Prakash, S., & Talwar, V. (2022). Are Industry 4.0 technologies enablers of lean? Evidence from manufacturing industries. International Journal of Lean Six Sigma, 14(1), 115\u2013138. https:\/\/doi.org\/10.1108\/ijlss-04-2021-0085\n"},{"Title":"Beyond Text-to-Image: Multimodal Prompts to Explore Generative AI","DOI":"10.1145\/3544549.3577043","Publication year":2023,"Abstract":"Text-to-image AI systems have proven to have extraordinary generative capacities that have facilitated widespread adoption. However, these systems are primarily text-based, which is a fundamental inversion of what many artists are traditionally used to: having full control over the composition of their work. Prior work has shown that there is great utility in using text prompts and that AI augmented workflows can increase momentum on creative tasks for end users. However, multimodal interactions beyond text need to be further defined, so end users can have rich points of interaction that allow them to truly co-pilot AI-generated content creation. To this end, the goal of my research is to equip creators with workflows that 1) translate abstract design goals into prompts of visual language, 2) structure exploration of design outcomes, and 3) integrate creator contributions into generations.","LowLevel":"artificial intelligence;image processing;text analysis;visual languages","MidLevel":"artificial intelligence;computer vision;data;liberal arts;developers;input","HighLevel":"industries;technology","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[75,313,58,308,348,251,79,23,183,419,382,219,224,405,92,236,401,307,175,465],"Top20Abs":[75,58,313,23,308,419,251,268,224,175,92,183,405,382,3,219,236,348,27,278],"Top20Tags":[308,79,381,313,18,76,58,326,415,387,119,379,99,425,160,254,251,71,95,72],"Citation":"Liu, V. (2023). Beyond Text-to-Image: Multimodal Prompts to Explore Generative AI. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3577043\n"},{"Title":"Future Internet Architectures on an Emerging Scale&mdash;A Systematic Review","DOI":"10.3390\/fi15050166","Publication year":2023,"Abstract":"Future Internet is a general term that is used to refer to the study of new Internet architectures that emphasize the advancements that are paving the way for the next generation of internet. Today&rsquo;s internet has become more complicated and arduous to manage due to its increased traffic. This traffic is a result of the transfer of 247 billion emails, the management of more than a billion websites and 735 active top-level domains, the viewing of at least one billion YouTube videos per day (which is the source of main traffic), and the uploading of more than 2.5 billion photos to Facebook every year. The internet was never anticipated to provide quality of service (QoS) support, but one can have a best effort service that provides support for video streams and downloaded media applications. Therefore, the future architecture of the internet becomes crucial. Furthermore, the internet as a service has witnessed many evolving conflicts among its stakeholders, leading to extensive research. This article presents a systematic review of the internet&rsquo;s evolution and discusses the ongoing research efforts towards new internet architectures, as well as the challenges that are faced in increasing the network&rsquo;s performance and quality. Moreover, as part of these anticipated future developments, this article draws attention to the Metaverse, which combines the emerging areas of augmented reality, virtual reality, mixed reality, and extended reality, and is considered to be the next frontier for the future internet. This article examines the key role of the blockchain in organizing and advancing the applications and services within the Metaverse. It also discusses the potential benefits and challenges of future internet research. Finally, the article outlines certain directions for future internet research, particularly in the context of utilizing blockchains in the Metaverse. &copy; 2023 by the authors.","LowLevel":"blockchain;network architecture;quality of service;software defined networking","MidLevel":"security;construction;business performance metrics;networks","HighLevel":"industries;technology;business","Venue":"Future Internet","Top20AbsAndTags":[377,159,268,251,447,355,193,330,354,298,249,328,209,215,252,281,289,10,286,226],"Top20Abs":[354,447,377,355,159,193,251,268,249,286,298,215,328,252,266,204,359,281,38,363],"Top20Tags":[281,289,335,426,268,484,443,446,68,323,442,410,467,475,427,191,141,457,480,170],"Citation":"Mohammed, S. A., & Ralescu, A. L. (2023). Future Internet Architectures on an Emerging Scale\u2014A Systematic Review. Future Internet, 15(5), 166. https:\/\/doi.org\/10.3390\/fi15050166\n"},{"Title":"New Technological Waves Emerging in Digital Transformation: Internet of Things IoT\/IoE, 5G\/6G Mobile Networks and Industries 4.0\/5.0","DOI":"10.1007\/978-3-031-31007-2_30","Publication year":2023,"Abstract":"The Internet has established itself as a \"Network of Networks\" and a globalized communication tool, initially enabling the connection between people themselves and between people and computers (machines). The cheapness and reduction in the size of Internet access components made it possible for simpler equipment and devices of our daily lives to also access the global network, thus creating the Internet of Things (IoT), which aims to interconnect devices\/objects\/things of everyday use. Intelligence and automation result from the additions of processing, memory, and communication in the objects involved and the use of access by Mobile Networks, AdHoc, and RFID. The 5G Mobile Network&nbsp;evolved to serve as the basis of IoT communication, due to its characteristics of Massive Use, Broadband, Low Latency (Delay), and high demand for better quality service. Aspects of architectures, multiple access techniques, and emerging technologies (massive MIMO, software-defined networking, mm-Wave) are presented historically, including 1G\/2G\/3G\/4G\/5G Mobile Networks up to the future 6G Network. Industry 4.0&nbsp;is known as the fourth industrial revolution, in which we will have billions of people connected by mobile devices, with processing power, storage resources, connectivity, and access to knowledge without limits. This new wave of technology, which will include several areas: Artificial Intelligence (AI), Advanced Robotics, Internet of Things (IoT), Autonomous Vehicles, Big Data, Cloud Storage, Virtual Reality (VR)\/Augmented Reality (AR), Cyber-Physical Systems, Digital Security, and 3D Printing, among others, has been increasingly incorporated by large companies as a new trend of digital transformation, enabling more security and productivity. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"5g mobile communication systems;digital storage;embedded systems;industry 4.0;metadata;millimeter waves;network security;wireless networks","MidLevel":"education;internet of things;telecommunication;data;security;developers;input;networks;semiconductors","HighLevel":"industries;technology","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[268,315,404,129,448,266,361,209,256,458,159,423,307,459,281,359,352,395,426,462],"Top20Abs":[268,315,404,209,129,448,266,458,307,159,256,361,423,459,395,426,281,352,359,453],"Top20Tags":[361,315,31,188,462,137,469,472,478,483,289,129,281,485,175,257,378,332,201,235],"Citation":"Leite, J. R. E., Ursini, E. L., Chmielewski, A. M. M., & da Silva, A. J. D. (2023). New Technological Waves Emerging in Digital Transformation: Internet of Things IoT\/IoE, 5G\/6G Mobile Networks and Industries 4.0\/5.0. Smart Innovation, Systems and Technologies, 329\u2013339. https:\/\/doi.org\/10.1007\/978-3-031-31007-2_30\n"},{"Title":"A virtual-reality framework for graph-based damage evaluation of reinforced concrete structures","DOI":"10.1117\/12.2657736","Publication year":2023,"Abstract":"Artificial intelligence (AI) and virtual\/augmented reality (VR\/AR) are facilitating objective and fast assessment of infrastructures. Computer vision advancements are also transforming traditional methods into automated information modeling and decision support systems. These advancements offer new opportunities to combat growing challenges that threaten infrastructure systems. In particular, climate change, aging structures, and population growth have intensified threats to infrastructure, requiring methods for evaluating infrastructure quickly after a disaster. VR uses cameras and sensors to provide images of the current state of a structural system, including the pattern of concrete cracks. A novel framework for analyzing the degree of damage in cracked reinforced concrete shear walls (RCSWs) is presented in this paper by leveraging virtual reality (VR) technology. An automated and unbiased approach is enabled by converting images of crack patterns on the surface of concrete structures into graphs. It is possible to extract relevant information from graphs, including graph features, to quantify the extent of damage using graph theory. A machine learning algorithm is then trained using these features to predict the extent of the damage. To validate the approach, the framework was applied to data collected from three RCSWs that were subjected to quasi-static cyclic loading. ML was used to predict the secant stiffness and its descending trend during each load cycle in the experimental test. The VR-based approach achieves high R2 scores of 0.90, 0.91, and 0.99 in a machine learning regression, indicating the framework's success. &copy; 2023 SPIE.","LowLevel":"climate change;computer vision;concrete buildings;concrete construction;decision support systems;e-learning;graph theory;graphic methods;learning algorithms;learning systems;machine learning;population statistics;reinforced concrete","MidLevel":"education;artificial intelligence;policy;computer vision;medical;graphics;construction;business performance metrics;other","HighLevel":"industries;technology;business;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[480,413,416,484,278,341,151,116,207,457,163,485,369,290,45,200,412,244,76,453],"Top20Abs":[480,416,412,484,45,200,76,151,179,485,116,316,457,290,278,98,207,425,341,451],"Top20Tags":[413,341,369,373,478,482,468,433,480,207,443,488,457,459,467,307,278,483,157,485],"Citation":"Bazrafshan, P., & Ebrahimkhanlou, A. (2023). A virtual-reality framework for graph-based damage evaluation of reinforced concrete structures. Nondestructive Characterization and Monitoring of Advanced Materials, Aerospace, Civil Infrastructure, and Transportation XVII. https:\/\/doi.org\/10.1117\/12.2657736\n"},{"Title":"Polymer Optical Sensor Glove Prototype Based on Eccentric FBGs","DOI":"10.1117\/12.2647682","Publication year":2023,"Abstract":"We report on the development, test and comparison of a prototype sensor glove for 3D shape detection of the human hand. The prototype is based on polymer optical fibers with eccentrically inscribed Bragg gratings, which are mantled with a simple jacket woven into a textile fabric glove. All of these elements are lightweight and flexible, taking away the drawback of motion handicaps, that sensor gloves usually come with due to material stiffness. The sensor glove is tested with a set of approximately 15 different and simply defined hand gestures, which incorporate iconic and everyday gestures like grasping a cylindrical shape or showing numbers with fingers, assisted with 3D printed models. Hence a set of gestures is defined, subsequently we compared two commercial systems based on optical sensors from 5DT (Data Glove 5\/ 14 Ultra) with the prototype. The prototype is not capable to measure motion accurately yet, due to its long integration times as of now, it is, however, advanced in the measurement accuracy, especially regarding the direction of the shape deformation, which is rendered possible by the structure of the FBG sensor. In the next steps, the integration time of the sensor, as well as its illumination and the evaluation will be improved. For that step, the light source, the optical spectrum analyzer and the computer will be replaced by integrated devices like LEDs, photodiodes and single-board microcontrollers. In the future, the gloves, as well as the used technology of the sensor, offer the potential for application in logistics, virtual and augmented reality as well as medical diagnostics and general observation. &copy; 2023 SPIE.","LowLevel":"diagnosis;fiber optics;light sources;optical sensors;petroleum reservoir evaluation;plastic optical fibers;spectrum analyzers;wearable sensors;weaving","MidLevel":"telecommunication;medical;inspection, safety and quality;power and energy;wearables;sensors;input;optics;other","HighLevel":"displays;use cases;industries;technology;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[317,220,182,287,177,158,18,172,301,343,169,112,444,277,422,53,332,141,413,441],"Top20Abs":[220,287,182,177,317,172,18,301,112,277,343,158,412,110,157,376,278,153,7,208],"Top20Tags":[194,169,317,285,138,431,74,0,223,437,482,201,158,441,444,170,141,290,430,324],"Citation":"Leffers, L., Roth, B., & Overmeyer, L. (2023). Polymer-optical sensor glove prototype based on eccentric FBGs. Optical Components and Materials XX. https:\/\/doi.org\/10.1117\/12.2647682\n"},{"Title":"Optimized Design and Manufacturing Process of Diffuse Micro Corner Cubes for Head Up Projection Display Applications","DOI":"10.1117\/12.2649947","Publication year":2023,"Abstract":"The superposition of digital information in the Field of View (FOV) of a user is the basis of the current developments in Mixed and Augmented Reality. Before being studied for Near Eye Device and Head Mounted Display, this application was implemented in Head Up Display (HUD) to help pilots and drivers to manage both the driving stress and the information flow related to the vehicle. Classical optical design of HUD based on the use of a combiner are strongly limited in FOV due to the issues related to pupil management. To overcome this issue Head Up Projection Displays have been developed based on the projection of digital image directly on the windshield. To support this approach an efficient projection surface that meets bright reflection and clear transparency has to be developed. We have introduced few years ago an optical approach based on retro-reflective transparent projection surface and a manufacturing process to provide microscopic corner cubes that incorporate an optical diffuser function. We present in this contribution an optimized design that increases the efficiency of the retroreflective structure towards 100%. We also discuss a possible technological process that allows the manufacturing of the master used to replicate the microstructure. This process based on grayscale lithography and on Deep Reactive Ion etching (DRIE) may guaranty a high retro-reflection efficiency, a high transparency and a realistic draft to allow a molding manufacturing process for the microstructure fabrication. &copy; 2023 SPIE.","LowLevel":"efficiency;helmet mounted displays;microstructure;optical design;optical instruments;transparency","MidLevel":"graphics;wearables;optics;human-computer interaction;other;display technology","HighLevel":"displays;technology;other;end users and user experience","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[270,149,344,317,393,228,158,450,278,288,481,276,80,343,0,74,332,205,243,237],"Top20Abs":[270,149,393,2,276,228,288,481,278,158,317,450,344,324,343,407,295,205,332,243],"Top20Tags":[169,344,438,317,223,0,158,482,290,450,441,149,445,74,235,228,470,21,201,141],"Citation":"Martinez, C., Quemper, J.-M., Lee, Y., & Sixt, P. (2023). Optimized design and manufacturing process of diffuse micro corner cubes for head-up projection display applications. Advances in Display Technologies XIII. https:\/\/doi.org\/10.1117\/12.2649947\n"},{"Title":"Software Hope Design for Children with ASD.","DOI":"10.1007\/978-3-031-32213-6_5","Publication year":2023,"Abstract":"This article describes the analysis, design, implementation, and evaluation of a software called Hope to help children with autism spectrum disorder (ASD) to express themselves through dance. The proposed software is based on augmented reality and allows strengthening teaching learning processes that include aspects related to imitation, perception, gross and fine motor skills, and visual coordination. The design process conducted in an interactive way, centered on the human being, with the participation and guidance of a multidisciplinary team. The characteristics considered to design and implement the software are explained, which was tested in a Ludic Place Therapeutic Center with children with ASD, in addition, a pedagogical intervention proposal was built where parameters for evaluation are defined; the start-up of Hope required constant interaction, participatory design methods, likewise the software was improved with the recommendations of the participants, some students from the therapeutic center tried the application, they did so initially accompanied by their caregivers and progressively they achieved individual use of the system, Hope was assessed by 5 children with ASD, in addition to 5 parents, 5 IT specialists, followed by 5 experts (teachers, psychologists, therapists, doctors). In the end, encouraging results achieved that included recognition of the body, nonverbal dialogue, less directive, structured expressions, and the ability to create and make the participants&rsquo; thinking more flexible. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"application programs;diseases;human computer interaction;software testing;user centered design","MidLevel":"human-computer interaction;medical;developers;inspection, safety and quality","HighLevel":"end users and user experience;technology;industries;use cases","Venue":"Commun. Comput. Info. Sci.","Top20AbsAndTags":[222,394,115,34,253,188,114,319,157,161,205,405,180,207,136,307,457,414,83,150],"Top20Abs":[222,394,34,253,188,115,83,157,114,180,405,205,207,25,136,418,454,192,307,140],"Top20Tags":[157,369,191,307,446,405,341,161,459,413,205,68,192,189,84,256,222,439,126,170],"Citation":"Romero, M. R., Macas, E. M., Armijos, N., & Harari, I. (2023). Software Hope Design for Children with ASD. Communications in Computer and Information Science, 64\u201376. https:\/\/doi.org\/10.1007\/978-3-031-32213-6_5\n"},{"Title":"Adopting Metaverse as a Pedagogy in Problem-Based Learning","DOI":"10.1007\/978-3-031-31153-6_24","Publication year":2023,"Abstract":"Pedagogical practices vary from time to time based on the requirement of various academic disciplines. Course instructors are constantly searching for inclusive and innovative pedagogies to enhance learning experiences. The introduction of Metaverse can be observed as an opportunity to enable the course instructors to combine virtual reality with augmented reality to enable immersive learning. The scope of immersive learning experience with Metaverse attracted many major universities in the world to try Metaverse as a pedagogy in fields such as management studies, medical education, and architecture. Adopting Metaverse as a pedagogy for problem-based learning enables the course instructors to create an active learning space that tackles the physical barriers of traditional pedagogical practices of case-based learning facilitating collaborative learning. Metaverse, as an established virtual learning platform, is provided by Meta Inc., providing the company a monopoly over the VR-based pedagogy. Entry of other tech firms into similar or collaborative ventures would open up a wide array of virtual reality-based platforms, eliminating the monopoly and subsequent dependency on a singular platform. The findings of the study indicate that, currently, the engagements on Metaverse are limited to tier 1 educational institutions worldwide due to the initial investment requirements. The wide adoption of the Metaverse platform in future depends on the ability of the platform providers to bridge the digital gap and facilitate curricula development. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"competition;curricula;e-learning;medical education","MidLevel":"education;business performance metrics;medical","HighLevel":"industries;business","Venue":"Lect. Notes Networks Syst.","Top20AbsAndTags":[363,193,249,377,355,244,226,251,328,204,163,215,354,262,207,454,369,140,383,185],"Top20Abs":[193,363,377,249,355,226,244,354,251,204,328,163,215,262,140,383,440,185,369,103],"Top20Tags":[207,454,369,485,315,135,161,459,184,453,432,138,479,285,170,0,341,43,434,446],"Citation":"Baby, R., Siby, A., Joseph, J. J., & Zacharias, P. (2023). Adopting Metaverse as a Pedagogy in Problem-Based Learning. Lecture Notes in Networks and Systems, 287\u2013295. https:\/\/doi.org\/10.1007\/978-3-031-31153-6_24\n"},{"Title":"Ultrafast black color tunability of electrochromic dimming films using polyoxometalate-anchored metal oxide nanoparticles","DOI":"10.1117\/12.2648690","Publication year":2023,"Abstract":"The fast switchable electrochromic (EC) materials have strong interest for controlling unnecessary lights from environment or achieving color tunability in transmissive-type and reflective-type display. In this study, a black color tunability of electrochromic dimming device was explored using polyoxometalate (PW)-anchored metal oxide (MOx) nanoparticles, poly(3,3-bis(bromomethyl)-3,4-dihydro-2H-thieno[3,4-b][1,4]dioxepine)s (PRBr), and an acid-free electrolyte layer. The PW-anchored MOx (PWMOx) layer was formed by electrostatic anchoring between the protonated MOx film and PW anion on a transparent electrode. The PWMOx was not only gave positive feedback to the electrochromic performance of its film, but also lowered the operating voltage by increasing the potential applied to the polymer layer in the electrochromic device, achieved black EC switching with high transparency modulation, a fast response time, and long endurance at a low operating voltage between 1.5 and -1.5 V. Furthermore, the boosted EC polymer properties arising from the charge balancing effect had the blocking capability for high-intensity light, such as 240 cd\/m2 and ~ 2379 cd\/m2 of light. The ECD blocked light transmission up to 95 % and dimming was adaptable to step voltage. This strategy may be coupled with various devices, including smart windows, transparent displays, image sensors, and augmented reality systems. &copy; 2023 SPIE.","LowLevel":"color;display devices;electrochromic devices;electrolytes;feedback;high intensity light;light transmission;metal nanoparticles;oxide films;oxides;transparent electrodes","MidLevel":"farming and natural science;graphics;liberal arts;input;human-computer interaction;other;display technology","HighLevel":"displays;end users and user experience;industries;technology;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[413,158,247,99,169,438,95,149,235,223,393,481,444,290,228,201,277,129,0,317],"Top20Abs":[413,247,129,481,99,235,158,228,393,277,444,169,123,445,80,111,41,472,141,268],"Top20Tags":[169,74,223,393,317,247,438,201,149,158,431,324,290,0,463,228,437,235,444,445],"Citation":"Jang, H., Kim, J., & Kim, E. (2023). Ultrafast black color tunability of electrochromic dimming films using polyoxometalate-anchored metal oxide nanoparticles. Organic Photonic Materials and Devices XXV. https:\/\/doi.org\/10.1117\/12.2648690\n"},{"Title":"Design of&nbsp;a&nbsp;Mixed Reality-Based Immersive Virtual Environment System for&nbsp;Social Interaction and&nbsp;Behavioral Studies","DOI":"10.1007\/978-3-031-27199-1_21","Publication year":2023,"Abstract":"The advancements in immersive technologies allow us to create more sophisticated environments designed to help engage users by merging the physical world with a digital or simulated reality. These can range from completely immersive virtual environments to mixed reality immersive environments, where the virtual world and the real world collide. Virtual reality is a completely immersive environment where the users&rsquo; reality is replaced with a simulated environment, and the hardware works to convince the user that they are in a different world. In contrast, augmented reality is a mixed type of reality, combining both the virtual and the natural world by augmenting the real world with digital assets and components. While both types of experiences contribute to creating rich collaborative environments, a limitation, and sometimes inconvenience, is present with the requirement of wearing a head-mounted device (HMD), creating restriction that prevents users from having physical interactions with others. Rather than interacting in the virtual space, we propose a concept that provides the structure for a physical space where users can interact with the shared mixed reality environment, an environment projected to help create the collaborative aspect in this project without any wearable devices. This paper will present the developed system and implemented four-dimensional interactions and demonstrate the feasibility of the structured experience we have created. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"helmet mounted displays","MidLevel":"wearables;display technology","HighLevel":"displays","Venue":"Lect. Notes Comput. Sci.","Top20AbsAndTags":[226,208,439,374,365,7,251,279,304,142,194,80,301,303,267,419,338,17,15,239],"Top20Abs":[226,208,439,374,251,365,304,142,419,7,303,279,210,454,249,207,267,80,338,17],"Top20Tags":[2,459,407,439,194,239,243,21,470,191,400,206,238,344,80,279,7,288,38,301],"Citation":"Matar, S., Shaker, A., Mahmud, S., Kim, J.-H., & van\u2019t Klooster, J.-W. (2023). Design of\u00a0a\u00a0Mixed Reality-Based Immersive Virtual Environment System for\u00a0Social Interaction and\u00a0Behavioral Studies. Lecture Notes in Computer Science, 201\u2013212. https:\/\/doi.org\/10.1007\/978-3-031-27199-1_21\n"},{"Title":"Effective Extended Reality: A Mixed-Reality Simulation Demonstration with Digitized and Holographic Tools and Intelligent Avatars","DOI":"10.1007\/978-3-031-21569-8_35","Publication year":2023,"Abstract":"How can we design engaging and effective medical education both online and onsite? Extended reality (XR) is a term referring to all real-and-virtual combined environments and human-machine interactions generated by computer technology and wearables. It includes representative forms such as virtual reality (VR), augmented reality (AR), or mixed reality (MR), and the areas interpolated among them. MR is a domain of particular interest today. It takes place not only in the physical world or in the virtual world, but is a mix of the real and the virtual. Metaverses can be enabled by MR wearable augments. Glasses-free MR is another very interesting dimension: e-REAL&#174;, as a MR environment for hybrid simulation and medical education in general, can be a stand-alone solution or even networked between multiple places through a link to a special videoconferencing system. Digital humans and human-sized holograms are part of the e-REAL scenarios, making this solution unique, rich, and diversified.","LowLevel":"avatars;biomedical education;computer aided instruction;human computer interaction;interpolation;wearable computers","MidLevel":"education;training;data;medical;presence;inspection, safety and quality;wearables;developers;human-computer interaction","HighLevel":"displays;end users and user experience;use cases;industries;technology","Venue":"Innovative Approaches to Technology-Enhanced Learning for the Workplace and Higher Education: Proceedings of 'The Learning Ideas Conference' 2022. Lecture Notes in Networks and Systems (581)","Top20AbsAndTags":[365,440,227,64,418,397,395,424,436,246,17,319,419,383,412,345,226,366,411,210],"Top20Abs":[227,365,395,397,64,440,418,424,419,246,319,226,412,383,17,403,345,112,360,142],"Top20Tags":[383,411,440,47,172,400,277,17,405,33,239,54,377,436,249,288,443,9,112,45],"Citation":"Salvetti, F., Gardner, R., Minehart, R., & Bertagni, B. (2022). Effective Extended Reality: A Mixed-Reality Simulation Demonstration with Digitized and Holographic Tools and Intelligent Avatars. Lecture Notes in Networks and Systems, 370\u2013376. https:\/\/doi.org\/10.1007\/978-3-031-21569-8_35\n"},{"Title":"Research on Mass Image Data Storage Method for Data Center","DOI":"10.1007\/978-981-99-1230-8_6","Publication year":2023,"Abstract":"With the advancement of technology and the development of the electric power business, power enterprises have generated a large amount of image data, which contains rich potential information, and it is urgent to store massive-scale image data for further mining and analysis. Starting from the storage requirements of power grid image data, this paper expounds on the drawbacks of direct storage of small files according to the characteristics of power grid image files. Then, the architecture design and function design of the grid image small file storage are carried out, and the function implementation is carried out. Finally, the grid image small file storage method based on the SequenceFile file format is briefly summarized. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"other","MidLevel":"other","HighLevel":"other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[478,486,479,322,345,447,76,461,20,99,453,467,215,336,4,74,300,278,456,422],"Top20Abs":[478,479,486,322,345,76,453,20,447,461,99,467,4,215,157,412,178,425,74,422],"Top20Tags":[447,188,478,469,31,458,315,187,432,7,479,361,322,472,200,467,461,486,2,483],"Citation":"Pan, S., Jiang, J., Qiu, H., Qiao, J., & Xu, M. (2023). Research on Mass Image Data Storage Method for Data Center. Smart Innovation, Systems and Technologies, 69\u201375. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_6\n"},{"Title":"Soft Tissue Cutting Based on Position Dynamics","DOI":"10.1007\/978-981-99-1230-8_17","Publication year":2023,"Abstract":"In order to fit the development wave of virtual medical surgery, a soft tissue cutting model based on position dynamics is proposed. The particle-constrained tetrahedron is used to reconstruct the soft tissue part of the human body, and the facility and real-time operation is met through a visual interactive platform and force feedback calculation. The user can control the surgical instrument to reach the organ lesion area through the manipulation force feedback stroke area, and can also adjust the optimal position and posture of the instrument. Through experimental simulation, it is fully demonstrated that the model has good stability under increasing time steps, and at the same time, the higher computational efficiency is highlighted by expressing the applied force in the form of constraints, which can meet the high standards of human&ndash;computer interaction. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"feedback;geometry;human computer interaction;surgery;surgical equipment;tissue","MidLevel":"graphics;human-computer interaction;medical","HighLevel":"end users and user experience;technology;industries","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[464,157,480,167,112,152,455,54,453,0,212,340,341,326,356,291,486,127,170,177],"Top20Abs":[157,464,152,112,340,212,127,467,326,341,486,439,84,356,480,20,54,291,158,167],"Top20Tags":[464,138,488,0,480,285,207,441,453,432,43,170,167,291,99,323,475,247,295,435],"Citation":"Wang, Z., & Yu, H. (2023). Soft Tissue Cutting Based on Position Dynamics. Smart Innovation, Systems and Technologies, 193\u2013201. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_17\n"},{"Title":"Algorithms Applied in Soft Tissue Deformation Simulation: A Survey","DOI":"10.1007\/978-981-99-1230-8_16","Publication year":2023,"Abstract":"In modern medical science, Minimally Invasive Surgery (MIS) is one of the most direct and effective ways to treat the malignant lesions. The training of MIS is now not only limited to the actual surgical anatomy but also start to have a growing tendency to the VR technology which is depended on computer graphics and haptic rendering. As well as the surgery equipment simulation and graphics rendering, the Soft Tissue Deformation (STD) simulation is another critical technique in MIS simulation of VR. Therefore, we have collected lots of STD algorithms from 1986&ndash;2022, especially, the haptic feedback algorithms for STD, which can be divided into two categories. The first one is algorithms based on image distortion and the another one is based on the physical properties simulating. After classification of the two categories, we make a comparison among the algorithms proposed above, such as efficiency, accuracy, complexity and rendering speed. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"rendering;surgery;surgical equipment","MidLevel":"graphics;medical","HighLevel":"industries;technology","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[463,167,175,152,326,403,301,453,435,471,438,294,157,480,234,126,466,457,240,356],"Top20Abs":[167,175,463,152,326,435,301,471,438,294,403,240,414,466,453,128,234,356,157,20],"Top20Tags":[463,138,488,480,207,457,453,285,0,157,307,433,74,467,432,468,43,170,446,341],"Citation":"Cai, X., & Yu, H. (2023). Algorithms Applied in Soft Tissue Deformation Simulation: A Survey. Smart Innovation, Systems and Technologies, 181\u2013192. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_16\n"},{"Title":"Identification of Expressway Traffic States Based on the Enhanced FCM Algorithm","DOI":"10.1007\/978-981-99-1230-8_15","Publication year":2023,"Abstract":"In order to enhance the accuracy and effectiveness of traffic state identification, a fuzzy C-means algorithm based on simulated annealing genetic algorithm (SAGA-FCM) was proposed. First, according to the characteristics of expressway traffic, traffic states were divided into five states based on the Van Aerde model. Second, there are characteristics of fuzziness in expressway traffic. Flow, speed and density were taken as characteristic attributes of sample data. This paper proposed an enhanced fuzzy C-means clustering method SAGA-FCM for the identification of traffic states. It overcomes the problem that the traditional FCM algorithm is sensitive to the initial clustering center and easy to fall into the local optimum. Finally, the M25 motorway was used as an example to evaluate traffic conditions. The results were consistent with measured traffic conditions, which verified the effectiveness of the method. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"copying;fuzzy clustering;genetic algorithms;highway engineering;simulated annealing","MidLevel":"artificial intelligence;other","HighLevel":"technology;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[257,482,426,231,404,471,28,448,435,438,457,464,157,20,460,427,373,458,288,25],"Top20Abs":[257,426,404,482,28,435,448,457,438,471,231,157,464,20,427,288,460,307,373,194],"Top20Tags":[295,254,482,290,192,312,468,187,308,158,361,472,485,2,373,484,459,18,1,285],"Citation":"Yang, Z., Hao, L., Liu, Y., & Cai, L. (2023). Identification of Expressway Traffic States Based on the Enhanced FCM Algorithm. Smart Innovation, Systems and Technologies, 167\u2013179. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_15\n"},{"Title":"Text-driven object affordance for guiding grasp-type recognition in multimodal robot teaching","DOI":"10.1007\/s00138-023-01408-z","Publication year":2023,"Abstract":"In robot teaching, the grasping strategies taught to robots by users are critical information, because these strategies contain the implicit knowledge necessary to successfully perform a series of manipulations; however, limited practical knowledge exists on how to utilize linguistic information for supporting grasp-type recognition in multimodal teaching. This study focused on the effects of text-driven object affordance&mdash;a prior distribution of grasp types for each object&mdash;on image-based grasp-type recognition. To this end, we created the datasets of first-person grasping-hand images labeled with grasp types and object names and tested if the object affordance enhanced the performance of image-based recognition. We evaluated two scenarios with real and illusory objects to be grasped, considering a teaching condition in mixed reality, where the lack of visual object information can make image-based recognition challenging. The results show that object affordance guided the image-based recognition in two scenarios, that is, increasing the recognition accuracy by (1) excluding the unlikely grasp types from the candidates and (2) enhancing the likely grasp types. Additionally, the \"enhancing effect\" was more pronounced with greater grasp-type bias for each object in a test dataset. These results indicate the effectiveness of object affordance for guiding grasp-type recognition in multimodal robot teaching applications. The contributions of this study are (1) demonstrating the effectiveness of object affordance in guiding grasp-type recognition both with and without the real objects in images, (2) demonstrating the conditions under which the merits of object affordance are pronounced, and (3) providing a dataset of first-person grasping images labeled with possible grasp types for each object. &copy; 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","LowLevel":"acceptance tests;character recognition;image enhancement;image recognition;robotics;statistical tests","MidLevel":"human factors;computer vision;data;robotics;developers;input","HighLevel":"end users and user experience;technology","Venue":"Mach Vision Appl","Top20AbsAndTags":[409,400,19,435,415,20,49,352,290,76,201,425,218,167,142,194,144,379,438,312],"Top20Abs":[409,415,400,19,49,435,352,20,201,167,194,218,290,142,76,144,425,379,312,193],"Top20Tags":[473,438,361,456,435,291,312,20,470,433,187,307,478,300,30,431,459,479,205,469],"Citation":"Wake, N., Saito, D., Sasabuchi, K., Koike, H., & Ikeuchi, K. (2023). Text-driven object affordance for guiding grasp-type recognition in multimodal robot teaching. Machine Vision and Applications, 34(4). https:\/\/doi.org\/10.1007\/s00138-023-01408-z\n"},{"Title":"Research on Virtual and Real Spatial Data Interconnection Mapping Technology for Digital Twin","DOI":"10.1007\/978-981-99-1230-8_7","Publication year":2023,"Abstract":"In recent years, distributed photovoltaics, decentralized wind power, new loads, electric vehicles, etc. have been connected to the power grid in a wide range, and the distribution network has become more active, with strong fluctuations, and large peak-to-valley differences. The management and coordination of distributed resources interaction put forward higher requirements. This paper conducts research on the data interconnection and mapping technology of virtual and real space for digital twins. The first step is to study the modeling and driving technology of data interconnection, fusion mechanism model, and information model based on the real twin of the power grid. Coordinate data and monitoring data of main and auxiliary equipment and the data interconnection mechanism of the power grid real scene twin, build a dual-mode driving method that integrates the mechanism model and data model; the second step is to study the power grid \"virtual-real\" space multi-service continuous mapping mechanism and real-time data. The panoramic mapping method realizes the continuous and real-time mapping of point data and surface data to the digital twin and realizes multi-block, multi-level, and multi-type mapping according to the steps of feature extraction, feature matching, model parameter estimation, transformation, and interpolation. The digital twin data is accurately matched; the third step is to study the efficient transmission and update integration technology of twin data in the power grid environment, research methods to improve the real-time and concurrency of data transmission, and research multi-source heterogeneous data dynamic update and integration technology. The fourth step is to study the two-way intelligent cooperation and integration technology of station equipment, realize real-time data interaction and state interaction between the virtual digital model and the real physical model, and realize the interconnection and mapping between the station panoramic model and the virtual and real data. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"auxiliary equipment;data integration;electric loads;electric power distribution;electric power system interconnection;electric power transmission;electric power transmission networks;integration;metadata;wind power","MidLevel":"education;data;power and energy;developers;farming and natural science;other;integration","HighLevel":"industries;technology;business;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[467,478,479,462,412,215,212,281,322,4,24,362,2,340,166,270,120,370,76,194],"Top20Abs":[479,478,467,462,412,4,215,166,362,212,24,370,281,322,157,340,76,226,321,381],"Top20Tags":[478,215,2,362,141,191,120,479,194,467,239,270,462,24,212,157,183,182,134,322],"Citation":"He, Z., Peng, L., Yu, H., & Wang, H. (2023). Research on Virtual and Real Spatial Data Interconnection Mapping Technology for Digital Twin. Smart Innovation, Systems and Technologies, 77\u201385. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_7\n"},{"Title":"Research on Spam Detection with a Hybrid Machine Learning Model","DOI":"10.1007\/978-981-99-1230-8_20","Publication year":2023,"Abstract":"Since the beginning of the twentieth century, with the rapid development and popularization of computer technology, e-mail has become an indispensable communication tool in people's social life, and greatly simplified people's daily life such as learning and working methods. However, following the rapid development of computer information technology, e-mail brings convenience to people, also generates some spam messages, which seriously threaten and discount the safety of e-mail users. Although the spam detection technology has been deeply studied and widely used, the traditional spam detection methods mostly rely on the static features extracted from the mails while these methods have great limitations and cannot effectively deal with new malicious mail attacks that are complex, aggressive, destructive, and targeted. Thus, with the rapid development of machine learning and artificial intelligence technology, this paper proposed a spam detection model with a hybrid machine learning method: first presented text pre-processing process including word tokenization, removing stop word, and extracting feature vector, then a spam detection classifier based on linear-regression hybrid model with three commonly-used machine learning methods, namely SVM, ANN, and RF, was discussed. The results show that the three models are able to produce a good result, but the hybrid model would present a better performance and demonstrate the effectiveness of the hybrid method. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"electronic mail;engineering education;support vector machines","MidLevel":"education;artificial intelligence;other","HighLevel":"industries;technology;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[116,274,453,163,244,456,446,383,454,313,114,79,476,77,345,341,208,447,340,369],"Top20Abs":[116,274,453,163,456,313,244,383,77,476,345,79,76,208,446,402,114,294,447,340],"Top20Tags":[454,369,184,434,161,442,459,446,135,413,256,341,171,187,453,447,76,134,207,308],"Citation":"Gao, Y., Song, J., Gao, J., Suo, N., Ren, A., Wang, J., & Zhang, K. (2023). Research on Spam Detection with a Hybrid Machine Learning Model. Smart Innovation, Systems and Technologies, 227\u2013235. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_20\n"},{"Title":"Graph Convolutional Neural Networks for Drug Target Affinity Prediction in U-Shaped and Skip-Connection Architectures","DOI":"10.1007\/978-981-99-1230-8_24","Publication year":2023,"Abstract":"It is common knowledge that traditional new medication development is a costly, drawn-out procedure with greater safety uncertainty. Among them, drug target affinity prediction (DTA) is an important step in drug discovery and drug research. If we can significantly improve the accuracy of DTA prediction, it can help us potentially reduce the cost of new drug design and development significantly. Therefore, drug target affinity prediction is a very important topic. The precise and thorough characterization of medicines and proteins is the key to this subject. With the advancement of deep learning, it has become popular for academics to integrate deep learning into DTA prediction in an effort to increase accuracy. For example, DeepDTA, WideDTA, GraphDTA, etc., which are basically trained using information of drug molecules, information of protein molecules respectively, and does not make good use of their graph relationships as well as graph deep information, and with the increase in depth of the model over the years, what should have been excellent results are hardly excellent training results because of the difficulty of training. Inspired by Unet, this paper proposes a new method, UGraphDTA, which uses a new U-shaped architecture and Skip connection architecture to enable the model to understand deeper graph information. The novelty of this method is the use of skip connections in the convolutional network, which allows the model to utilize both the original molecular graph structure information and the information after convolution of the graph, enhancing the model's prediction ability for DTA tasks. The prediction performance of UGraphDTA is empirically proven to be better than other baseline models. This indicates that our proposed U-shaped convolutional architecture for drug target affinity prediction strategy that mines the deep information of drugs and proteins is effective. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"binding energy;convolution;convolutional neural networks;deep learning;graph neural networks;information use;molecules;network architecture;proteins","MidLevel":"artificial intelligence;medical;construction;business planning and management;networks;other","HighLevel":"industries;technology;business;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[358,453,480,385,446,259,456,379,81,416,244,381,95,289,340,483,177,458,160,116],"Top20Abs":[358,453,385,259,480,446,379,81,456,416,244,381,289,95,45,340,116,485,458,177],"Top20Tags":[480,323,453,476,341,468,483,176,472,456,431,160,95,448,373,446,158,190,115,433],"Citation":"Chen, J., Dong, X., & Yang, Z. (2023). Graph Convolutional Neural Networks for Drug Target Affinity Prediction in U-Shaped and Skip-Connection Architectures. Smart Innovation, Systems and Technologies, 271\u2013283. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_24\n"},{"Title":"f<sup>2<\/sup>-ToF: A feature-alignment and frequency-division time-of-flight data denoise network","DOI":"10.1016\/j.comcom.2023.04.033","Publication year":2023,"Abstract":"Infrared wave time-of-flight (ToF) imaging is a important method to sense the 3D information of scene for Internet of Things (IoT) and artificial intelligent (AI). Driven by heavy demands from industry and users, ToF imaging has received significant research attention in recent year, but the artifacts of depth image still remain and need to be improved. Removing multiple artifacts of ToF data is usually treated as a multi-stage stitching problem for deep learning methods. However, the multi-stage cascade and cross-domain refinement architecture could increase the difficulty of model fitting and hurt the effect of noise reduction. In this paper, we classify the artifacts of ToF data as temporal-related or modulation frequency-related noise and propose a ToF denoising convolutional neural network (f2-ToF) to reduce multiple artifacts simultaneously. Specifically, a frequency-division structure is designed to reduce the influence of frequency-related noise in different modulation frequencies. For efficient correcting misalignment data and ensuring a one-stage end-to-end training, the feature-wise alignment module is proposed. In experiments, every proposed module effectively performed its designed task, and the whole framework achieved strong performance in ToF image refinement. &copy; 2023 Elsevier B.V.","LowLevel":"computer vision;convolutional neural networks;deep learning;frequency modulation;image enhancement;internet of things;learning systems","MidLevel":"education;artificial intelligence;internet of things;computer vision;medical;power and energy;networks","HighLevel":"industries;technology","Venue":"Comput Commun","Top20AbsAndTags":[341,453,323,480,81,232,479,381,447,438,467,178,20,280,476,486,469,361,401,458],"Top20Abs":[81,232,479,178,381,341,458,480,438,387,401,486,379,447,323,20,476,402,453,98],"Top20Tags":[341,31,480,323,453,331,238,469,95,431,484,446,456,315,447,478,267,129,472,220],"Citation":"Tong, Y., Chen, J., Leng, Z., Liu, B., & Wang, Y. (2023). <mml:math xmlns:mml=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" display=\"inline\" id=\"d1e930\" altimg=\"si53.svg\"><mml:msup><mml:mrow><mml:mi>f<\/mml:mi><\/mml:mrow><mml:mrow><mml:mn>2<\/mml:mn><\/mml:mrow><\/mml:msup><\/mml:math>-ToF: A feature-alignment and frequency-division time-of-flight data denoise network. Computer Communications, 207, 66\u201376. https:\/\/doi.org\/10.1016\/j.comcom.2023.04.033\n"},{"Title":"Rapid Identification of Herbaceous Biomass Based on Raman Spectrum Analysis","DOI":"10.1007\/978-981-99-1230-8_19","Publication year":2023,"Abstract":"To realize the rapid online identification of tobacco quality style in a few seconds, this paper proposed a rapid identification method of tobacco quality style based on the Raman spectrum analysis. This method could quickly obtain the Raman spectrum of tobacco samples, then establish the mapping relationship database between the tobacco information and the Raman signal. The KNN (K-Nearest Neighbors) algorithm as the identification algorithm of tobacco information was used. The rapid and accurate identification of tobacco origin through the Raman signal in the range of 2700&ndash;3500&nbsp;cm&minus;1 was realized. The accuracy of origin identification can reach 95.7%. To identify the tobacco grade accurately, the competitive adaptive weighted sampling (CARS) was used to select the key Raman characteristic spectral coverage that determines the acteristics of tobacco grade. Combined with the KNN algorithm, the rapid and accurate identification of tobacco grades by analyzing the signals of key Raman characteristic spectral coverage was realized. By using the key Raman characteristic spectral coverage in the range of 800&ndash;800&nbsp;cm&minus;1 and 2700&ndash;3500&nbsp;cm&minus;1, the accuracy of tobacco grade identification can reach 87.0%. The results showed that by analyzing the key Raman characteristic band signals of unknown tobacco samples, combined with the identification algorithm proposed in this paper, the efficient identification of the quality style of unknown tobacco can be realized. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"discriminant analysis;learning algorithms;nearest neighbor search;quality control;raman scattering;raman spectroscopy;spectrum analysis","MidLevel":"artificial intelligence;medical;inspection, safety and quality;input;other","HighLevel":"industries;technology;use cases;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[466,459,157,20,457,438,423,464,119,435,471,290,373,469,295,247,485,475,468,467],"Top20Abs":[466,459,157,20,457,423,438,119,435,464,469,295,485,220,247,471,218,300,285,463],"Top20Tags":[235,290,446,471,273,192,169,438,468,138,445,444,466,201,317,312,74,430,413,223],"Citation":"Li, Q., Ye, Z., Liang, H., Yu, Z., Fang, Z., Cai, G., Zheng, Q., Yan, L., Zhong, H., Xiong, Z., Xu, J., & Liu, Z. (2023). Rapid Identification of Herbaceous Biomass Based on Raman Spectrum Analysis. Smart Innovation, Systems and Technologies, 213\u2013226. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_19\n"},{"Title":"RGB LED matrix display for augmented driving for higher traffic safety","DOI":"10.1117\/12.2648294","Publication year":2023,"Abstract":"Augmentation in manually driven vehicles can raise traffic safety significantly. The most ergonomic (eyes -on-the-road, no refocusing) solution is AR-HUD but the FOV is limited today to 10&deg; by 5&deg;. Transparent displays in the windshield (eyes-on-the-road, refocusing required) are costly (replacement) and hardly meet legal requirements for transparency. The cheapest solution is video-AR on dashboard displays (eyes-off-the-road, refocusing required). We report on a new approach for augmentation as compromise between ergonomics and cost: An 8-line RGB matrix display to be mounted on top of the dashboard at the bottom of the windshield. It spreads from pillar -to-pillar (150 cm, 150 x 8 pixel, RGB LED) and therefore enables augmented information along the whole windshield. In consequence, it needs less eyes-off-the road and refocusing and is a very ergonomic add-on for video-AR. We started with a single line pixelated light guide in a seating buck to measure and to evaluate the required luminance (&gt; 3,300 cd\/m2) RGB luminance ratio (35:50:15) and perception of information from night to blinding sunlight. We optimized the RGB LED display by testing and measuring various diffusers at different distances to the LEDs for an optimum combination of sharpness and pixelation. Image quality and content such as the visualization of actual speed (including color-coding), warnings (e.g. slippery), navigation, and comfort functions (e.g. incoming call, beat mode) were evaluated by subjects via online survey and in our seating buck. The display was rated as being very helpful with significantly reduction in time for grasping the information. &copy; 2023 SPIE.","LowLevel":"accident prevention;display devices;ergonomics;light emitting diodes;luminance;roads and streets;windshields","MidLevel":"human factors;automotive;transportation;inspection, safety and quality;graphics;input;farming and natural science;display technology","HighLevel":"displays;end users and user experience;use cases;industries;technology","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[169,149,444,332,344,170,259,345,277,288,470,243,441,235,432,206,129,162,66,433],"Top20Abs":[149,444,169,344,332,129,259,345,206,277,432,433,170,288,162,278,235,441,21,237],"Top20Tags":[332,149,169,470,344,393,235,441,435,223,288,295,90,307,192,228,141,177,206,0],"Citation":"Blankenbach, K., Eisenhardt, M., Brezing, K., & Reichel, S. (2023). RGB LED matrix display for augmented driving for higher traffic safety. Advances in Display Technologies XIII. https:\/\/doi.org\/10.1117\/12.2648294\n"},{"Title":"An Ensemble Approach for Histopathological Classification of Vulvar Cancer","DOI":"10.1117\/12.2653858","Publication year":2023,"Abstract":"Light microscopy of tissue slides is an important tool for analyzing human diseases including cancer. In this work, we focus on classifying patches from a immunohistochemically stained tissue microarray (TMA) of vulvar cancer. We propose a novel ensemble-based deep learning technique to classify patches of tissue as cancerous, stroma, both, or none. Our ensemble model consists of a pre-trained data-efficient image transformer (DeiT) module to extract features of patches followed by a transformer block and graph convolutional networks (GCN). Transformer blocks aid the sequential learning of the extracted features from DeiT while the graph convolutional network (GCN) extracts neighborhood information. Our approach combines both methods for classification. In the evaluation, we show that our approach outperforms state-of-the-art architectures for the addressed application. We also show that is applicable when only small amounts of labelled data are available. &copy; 2023 SPIE.","LowLevel":"convolution;convolutional neural networks;deep learning;diseases;image classification;learning systems;tissue","MidLevel":"education;artificial intelligence;computer vision;medical;networks","HighLevel":"industries;technology","Venue":"Progr. Biomed. Opt. Imaging Proc. SPIE","Top20AbsAndTags":[341,446,323,484,453,463,358,476,451,99,483,464,318,381,290,207,413,401,244,255],"Top20Abs":[341,446,323,99,453,484,318,381,451,358,401,48,244,290,144,463,4,45,76,387],"Top20Tags":[484,476,453,464,207,323,468,483,341,488,138,433,446,431,463,456,373,457,432,317],"Citation":"Mushunuri, R. V., Choschzick, M., Braumann, U.-D., & Hess, J. (2023). An ensemble approach for histopathological classification of vulvar cancer. Medical Imaging 2023: Digital and Computational Pathology. https:\/\/doi.org\/10.1117\/12.2653858\n"},{"Title":"Research on Cross Domain Data Analysis and Data Mining Technology of Power Grid Digital Benefits","DOI":"10.1007\/978-981-99-1230-8_23","Publication year":2023,"Abstract":"The digital power grid system integrates flexible resources such as traditional power sources and distributed power sources and has the characteristics of high time variability and high complexity. It urgently needs cross domain data support. However, the traditional evaluation methods can&rsquo;t evaluate cross domain data, which is not targeted and effective, and its practicality needs to be improved. This paper analyzes the cross domain intelligent construction and business service requirements, the characteristics of power grid digital cross domain data, studies the internal coupling relationship between digital system application functions and cross domain data, and proposes a coupling evaluation model to measure the digital system and cross domain data. Research the process of cross domain data mining, cluster similarity measurement, historical data feature mining, and other technologies to achieve historical cross domain data feature analysis. Study the fitting analysis technology, and use correlation analysis and regression analysis to achieve the fitting analysis of spanning data, so as to support the technical and economic analysis of spanning data of power grid digital system. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"data handling;data mining;regression analysis","MidLevel":"artificial intelligence;data;metals and mining","HighLevel":"industries;technology","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[486,478,462,467,178,322,345,373,215,483,157,4,7,403,211,148,166,252,412,419],"Top20Abs":[486,462,478,467,178,4,322,345,7,215,373,412,483,157,166,150,341,252,45,370],"Top20Tags":[473,438,433,315,467,442,30,183,487,457,431,462,157,307,486,480,277,464,308,24],"Citation":"Wang, G., Dong, A., Lv, C., Zhao, B., & Pan, J. (2023). Research on Cross Domain Data Analysis and Data Mining Technology of Power Grid Digital Benefits. Smart Innovation, Systems and Technologies, 261\u2013270. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_23\n"},{"Title":"Research and Implementation of Hybrid Storage Method for Multi-source Heterogeneous Data of Electric Distribution Network","DOI":"10.1007\/978-981-99-1230-8_3","Publication year":2023,"Abstract":"The traditional relational database is difficult to efficiently store the power grid topology data with complex network relationships, which seriously restricts the application expansion of power grid topology analysis and calculation. Graph database is a new data management and analysis and calculation technology based on graph theory. The research mainly focuses on the following four aspects: first, the basic requirements of the power grid diagram database, including: the data requirements of the power grid diagram database, the functional requirements of the power grid diagram database, and the performance requirements of the power grid diagram database; the second is the research on the core technology of the grid diagram database, including: the research on the data description method of the grid diagram database, the research on the data storage technology of the grid diagram database, the research on the data modeling technology of the grid diagram database, and the research on the fast retrieval technology for the grid diagram data; the third is the development and realization of the main functions of the power grid diagram database, including: the design of the power grid diagram database system framework, the research on the development and selection of the power grid diagram database technology, and the development and realization of the basic functions of the power grid diagram database; the fourth is grid diagram database testing and application verification, including: grid diagram database testing and grid diagram database application verification. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"complex networks;electric power distribution;graph theory;information management;search engines;stereochemistry","MidLevel":"artificial intelligence;data;medical;power and energy;business planning and management;chemical;other","HighLevel":"industries;technology;business;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[462,486,479,322,467,215,267,345,233,134,106,45,283,373,43,218,458,447,266,278],"Top20Abs":[462,486,479,322,467,267,345,215,45,106,283,134,233,373,218,43,421,222,4,458],"Top20Tags":[467,469,486,446,462,473,447,483,184,188,487,431,476,141,31,269,315,373,432,24],"Citation":"Qiao, J., Zhou, A., Peng, L., Zhu, L., Pan, S., & Yang, P. (2023). Research and Implementation of Hybrid Storage Method for Multi-source Heterogeneous Data of Electric Distribution Network. Smart Innovation, Systems and Technologies, 31\u201340. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_3\n"},{"Title":"2D Numerical Model Used to Investigate the Influence of Vegetation on Geomorphological Evolution of Mudflats","DOI":"10.1007\/978-981-99-1230-8_12","Publication year":2023,"Abstract":"In this study, a 2D numerical model (Delft3D) was used to investigate the influence of vegetation on geomorphological evolution of mudflats. The model results indicated that the mudflat was in a pattern of accretion when the vegetation was excluded from the model. The formation of tidal channels in the intertidal zone was well reproduced. Moreover, the middle and upper sections of the intertidal zone were characterized by a convex profile. It is suggested that the presence of vegetation could promote the formation and development of tidal channels compared with that without vegetation. Furthermore, the promoting effect of vegetation on tidal channel formation was more significant when the vegetation biomass was ecologically distributed. Different vegetation biomass distribution patterns showed different effects on the evolution of the mudflat. When the vegetation biomass was ecologically distributed, the intensity of reduction in current flow was stronger than parabolic distribution during the early stage of evolution. However, the result was opposite during the later stage of evolution. Further analysis also indicated that the presence of vegetation decreased the width of the upper intertidal zone (vegetated mudflat), while increased its elevation. In addition, the steep cliff between vegetated mudflat and bare mudflat increased significantly. The findings of this study may provide some implications for the management and restoration of mudflat&ndash;salt marsh ecosystem. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"biomass;ecology;numerical models","MidLevel":"farming and natural science;data;other","HighLevel":"industries;technology;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[473,469,251,416,206,486,458,467,476,106,463,240,265,74,81,134,187,428,25,157],"Top20Abs":[473,469,251,416,206,467,476,240,458,106,486,453,463,337,81,74,265,341,157,134],"Top20Tags":[184,22,373,295,233,475,18,463,69,469,458,187,194,113,380,90,330,165,7,428],"Citation":"Ji, J. (2023). 2D Numerical Model Used to Investigate the Influence of Vegetation on Geomorphological Evolution of Mudflats. Smart Innovation, Systems and Technologies, 131\u2013142. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_12\n"},{"Title":"Research on Weld Defect Object Detection Based on Multi-channel Fusion Convolutional Neural Network","DOI":"10.1007\/978-981-99-1230-8_21","Publication year":2023,"Abstract":"Aiming at the problems of low efficiency and strong subjectivity in the current detection of weld defects by radiographic imaging technology, an object detection method of weld defects based on multi-channel fusion convolutional neural network is proposed. In this method, the images of weld defects are encoded and input into multiple feature extraction channels formed by parallel fusion of CNN. After that, the extracted features are fused with full connection layer and the feature vectors are output. Finally, the final output is obtained by Softmax for classification. The proposed method is verified by weld defect images in actual production. The experimental results indicate that the mAP of the multi-channel fusion convolutional neural network reaches 76.37%, and the detection accuracy of weld defects is higher than that of other network such as ResNet-50 and VGG-16. The proposed method can be applied to X-ray intelligent detection of weld defects and other scenarios. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"convolution;convolutional neural networks;defects;image fusion;object recognition;welds","MidLevel":"artificial intelligence;human factors;computer vision;inspection, safety and quality;manufacturing;networks","HighLevel":"end users and user experience;technology;industries;use cases","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[469,467,358,99,402,345,453,480,341,98,456,116,76,318,255,485,81,438,458,416],"Top20Abs":[469,453,402,358,467,99,98,255,341,76,116,456,345,318,485,458,81,438,416,451],"Top20Tags":[480,484,467,469,433,201,138,468,190,307,478,457,483,49,475,144,68,183,345,283],"Citation":"Geng, H., Li, Z., & Zhou, Y. (2023). Research on Weld Defect Object Detection Based on Multi-channel Fusion Convolutional Neural Network. Smart Innovation, Systems and Technologies, 237\u2013249. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_21\n"},{"Title":"Sustainable Solutions by the Use of Immersive Technologies for Repurposing Buildings","DOI":"10.1007\/978-3-031-28839-5_62","Publication year":2023,"Abstract":"In the context of urban production and sustainable reuse of existing buildings, a detailed planning of the later usage is indispensable. One approach is to enable large-scale AR simulation on site with a sufficient Level of Detail (LoD) and stability. To determine performance metrics, a technology-stack is created and presented that enables a realistic field experiment in an industrial environment (area of 1,314&nbsp;m2) using Microsoft HoloLens 2. For the experiment, a 3D model was instantiated as often as possible up to the limit of system stability and in different LoDs (100% down to 10%). The result shows that it is feasible to represent 2.63 million polygons (equivalent to about 1,909&nbsp;m3 of augmented space) on LOD-35%; LoD-100% is equivalent to 327.38&nbsp;m3 and 1,284 million polygons. Polygonal density [polygons\/m3] is introduced as new indicator for better comparability when using 3D models. Thus, it is possible to immersively visualize urban production planning processes in large-scale scenarios. This expands the functional planning space of Urban Production and overcomes previous technical limitations. &copy; 2023, The Author(s).","LowLevel":"3d modeling;geometry;production control;sustainable development;system stability","MidLevel":"education;policy;inspection, safety and quality;graphics;manufacturing","HighLevel":"industries;technology;business;use cases","Venue":"Lect. Notes Mech. Eng.","Top20AbsAndTags":[459,60,35,467,189,295,388,418,356,361,447,123,457,474,12,341,411,157,471,207],"Top20Abs":[459,60,356,418,388,474,361,401,189,447,123,349,261,152,471,411,82,12,208,197],"Top20Tags":[467,24,189,457,283,307,207,423,295,315,316,317,463,183,464,482,373,157,67,438],"Citation":"Rosilius, M., Wilhelm, M., von Eitzen, I., Decker, S., Damek, S., & Braeutigam, V. (2023). Sustainable Solutions by the Use of Immersive Technologies for Repurposing Buildings. Manufacturing Driving Circular Economy, 551\u2013558. https:\/\/doi.org\/10.1007\/978-3-031-28839-5_62\n"},{"Title":"Design and Experimental Verification of High Functional Density Cubesat System","DOI":"10.1007\/978-981-99-1230-8_13","Publication year":2023,"Abstract":"A Cubesat is regarded as a concentrated instrument intuitively and it includes all necessary elements of a satellite. High-cost performance is the key point and advantage, which is different from traditional satellites. In this paper, the development trends and task dimensions of Cubesat are analyzed first. Then it details the standardization and integration design requirements of high functional density Cubesat as the technologies of electronics and advanced devices are promoted rapidly. Technical approaches are proposed for system design of high functional density Cubesat in two dimensions. Furthermore, a specific design example for 6U Cubesat is shown in detail, which includes standardized stack combination, power system integration, autonomous operation management, and diversified task modes. All these methods are applied to achieve more functions in a Cubesat. The 6U Cubesat is well-verified on orbit for 10&nbsp;months and large amounts of data are transmitted from space to earth. The key developing technologies of Cubesat are prospected at last. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"earth;orbits;small satellites","MidLevel":"geospatial;liberal arts;medical;other","HighLevel":"industries;technology;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[459,475,118,471,486,462,157,479,467,286,2,466,457,340,374,20,341,416,141,418],"Top20Abs":[459,475,157,479,486,467,462,118,471,457,286,463,2,281,41,20,141,466,341,205],"Top20Tags":[176,158,271,379,14,118,197,18,416,30,24,451,29,458,160,423,257,187,93,95],"Citation":"Yao, Y., Fu, W., Guo, X., Shi, S., & Yan, J. (2023). Design and Experimental Verification of High Functional Density Cubesat System. Smart Innovation, Systems and Technologies, 143\u2013156. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_13\n"},{"Title":"Multi-feature Extraction of Mineral Zone of Tabling Through Deep Semantic Segmentation","DOI":"10.1007\/978-981-99-1230-8_5","Publication year":2023,"Abstract":"Various segmentation algorithms for mineral zone images can only extract the boundary of the concentrate zone or the separation point of the mineral zone. To obtain fuller and more productive feature information from the mineral zone of Tabling&rsquo;s separation, deep semantic segmentation models with DeepLab, U-net, and Xception are constructed. The image datasets of the industrial Tabling separation are collected and marked, and the corresponding mineral zone image dataset is constructed, the training and test sets are distributed in a certain proportion and imported into the deep semantic models for training. The training results of these models are compared, and the segmentation of the mineral zone images is evaluated. DeepLab-xception and DeepLab v3+ have the highest accuracy 0.9943 and mean intersection over the union value of 0.989. Finally, the DeepLab v3+ is adopted as the model for the image feature segmentation of Tabling&rsquo;s mineral zone. Through the corresponding image processing and feature extraction operators, the effective multi-scale features of Tabling&rsquo;s mineral zone can be well extracted. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"extraction;feature extraction;minerals;semantics;statistical tests","MidLevel":"artificial intelligence;computer vision;data;chemical;metals and mining","HighLevel":"industries;technology","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[318,20,76,99,341,477,461,438,178,98,453,469,425,471,435,381,467,451,188,476],"Top20Abs":[318,20,76,99,453,461,477,178,438,341,98,425,435,471,469,462,225,121,451,188],"Top20Tags":[341,479,487,438,433,223,467,478,468,283,381,446,30,480,431,387,76,71,278,476],"Citation":"Liu, H., & You, K. (2023). Multi-feature Extraction of Mineral Zone of Tabling Through Deep Semantic Segmentation. Smart Innovation, Systems and Technologies, 51\u201368. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_5\n"},{"Title":"Design and Research of Highway Tunnel Construction Positioning System Based on UWB","DOI":"10.1007\/978-981-99-1230-8_22","Publication year":2023,"Abstract":"Maintenance personnel, according to the theory of highway tunnel construction vehicle real-time positioning problem, combined with the tunnel's construction environment, make full use of information technology, the basic principles of UWB indoor positioning technology is analysed, and based on this, advances the UWB positioning system of the highway tunnel construction solution, finally, set up the test environment to analyse the location accuracy of the positioning system. The results show that the system can fully meet the positioning requirements of tunnel construction personnel and vehicles. The system is mainly composed of four parts: the perception layer, the transmission layer, the solution layer, and the application layer. Through the Internet of things technology, the positioning detection, track playback, one-button alarm, and other functions of the construction personnel and vehicles can be realized, which can effectively improve the level of intelligent management and security control in the construction stage of highway tunnel. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"highway administration;highway planning;human resource management;information use;internet of things;vehicles","MidLevel":"internet of things;human resources;automotive;transportation;business planning and management;networks","HighLevel":"industries;technology;business","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[153,315,146,258,38,35,129,181,268,447,361,25,374,86,347,41,261,2,312,389],"Top20Abs":[146,35,181,258,86,41,129,38,268,374,261,157,389,347,153,414,441,2,345,315],"Top20Tags":[31,315,188,361,447,129,258,286,484,289,469,25,483,156,175,123,135,298,378,373],"Citation":"Zhang, H., Wang, X., Liu, Y., & Cai, L. (2023). Design and Research of Highway Tunnel Construction Positioning System Based on UWB. Smart Innovation, Systems and Technologies, 251\u2013259. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_22\n"},{"Title":"Space Design of Exhibition Hall Based on Virtual Reality","DOI":"10.1007\/978-981-99-1230-8_14","Publication year":2023,"Abstract":"With the emergence of new art forms and technical means, more and more exhibition activities are presented in front of people with a novel artistic expression, relying on brand-new technical means, which has greatly changed people&rsquo;s understanding of exhibition space. The author puts forward the idea of applying VR (Virtual Reality) display design method to the space design of exhibition halls and establishes a panoramic view generation model of exhibition halls. Because there is a high similarity between the optimal simple polygon formed by plane discrete corners and TSP (Traveling Salesman Problem), this paper will directly use the basic steps of GA (Genetic Algorithm) to solve TSP problem to generate the building ground contour. The research shows that by comparison, it can be seen that the time consumed by this algorithm is obviously less than that of the algorithm based on feature points. When the number of points is 16, the two algorithms get the same criterion function value. As the number of points increases, the optimization effect of GA becomes more and more obvious, while the error of the algorithm based on feature points becomes larger. It shows that the optimized GA has a good optimization effect. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"design;traveling salesman problem","MidLevel":"human-computer interaction;sales and marketing;other","HighLevel":"end users and user experience;business;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[98,405,318,438,435,229,132,464,466,264,20,321,340,451,414,486,341,128,350,161],"Top20Abs":[98,318,438,405,435,229,464,264,20,451,414,341,340,466,467,321,457,350,92,128],"Top20Tags":[135,161,482,373,2,156,132,25,1,355,446,472,187,270,24,430,188,418,432,469],"Citation":"Wang, Y. (2023). Space Design of Exhibition Hall Based on Virtual Reality. Smart Innovation, Systems and Technologies, 157\u2013165. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_14\n"},{"Title":"Comparison of video capture cards for streaming real-time procedural imaging onto mixed reality headset","DOI":"10.1117\/12.2655751","Publication year":2023,"Abstract":"Ergonomics for image-guided procedures can be improved by using mixed reality headsets. Such headsets offer the ability to position holographic monitors that display information, such as an ultrasound stream, within the operator&rsquo;s field of view during procedures. However, one of the barriers of clinical adoption of mixed reality headsets is high latency of information projected on to the headset. Upwards of 40% of the overall latency of the entire system can be due to video cards that capture procedural imaging for wireless streaming. The costs of the video cards can vary widely, from as low as $20 to upwards of several hundreds of dollars. The latencies of four separate video cards with a range of costs were evaluated. Based on these results, we propose an ideal tradeoff between latency and costs for clinical use of wirelessly mirroring procedural imaging into mixed reality headsets. &copy; 2023 SPIE.","LowLevel":"display devices;ergonomics;image enhancement;medical applications;medical imaging","MidLevel":"human factors;computer vision;medical;developers;display technology","HighLevel":"displays;technology;industries;end users and user experience","Venue":"Progr. Biomed. Opt. Imaging Proc. SPIE","Top20AbsAndTags":[432,228,259,323,488,344,481,301,0,271,408,169,453,225,340,317,440,263,149,100],"Top20Abs":[432,259,100,228,408,271,301,371,274,340,225,263,176,265,488,357,323,50,344,358],"Top20Tags":[43,450,0,432,149,323,453,170,488,487,433,228,481,440,169,456,138,459,191,207],"Citation":"Shah, S. R., & Park, B. J. (2023). Comparison of video capture cards for streaming real-time procedural imaging onto mixed reality headset. Medical Imaging 2023: Imaging Informatics for Healthcare, Research, and Applications. https:\/\/doi.org\/10.1117\/12.2655751\n"},{"Title":"Scheme Design of Network Neighbor Discovery Algorithm Based on the Combination of Directional Antenna and Multi-channel Parallelism","DOI":"10.1007\/978-981-99-1230-8_9","Publication year":2023,"Abstract":"The existing deign has the dynamic blind zone with high energy efficiency, which is lower the quality of connection in the place like airport. The asynchronous, heterogeneous, and multi-channel fusion mobile Ad-Hoc network with multi-channel and multi-interface is an inevitable choice to further improve the communication bandwidth and communication quality. The key problem is to study and design a fast neighbor discovery strategy that integrates \"directional antenna and multi-channel parallel\" in the case of limited energy. This paper proposed a neighbor discovery algorithm combining directional antenna and multi-channel parallelism. In more details, our framework uses multi-channel and multi-interface asynchronous, heterogeneous, and multi-channel fusion mobile Ad Hoc network, and this optimized algorithm enables fast neighbor discovery in energy-constrained situations. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"constrained optimization;directive antennas;energy efficiency","MidLevel":"human-computer interaction;telecommunication;computer vision;power and energy;business performance metrics;farming and natural science;other","HighLevel":"end users and user experience;business;industries;technology;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[467,476,416,486,264,99,265,447,477,268,483,458,473,269,346,471,316,402,340,419],"Top20Abs":[467,476,416,486,264,99,265,268,477,316,458,402,447,346,473,419,404,340,282,471],"Top20Tags":[478,447,31,315,483,462,188,476,177,472,361,135,233,467,184,194,487,187,264,329],"Citation":"Zhao, N., Gao, F., & Pu, K. (2023). Scheme Design of Network Neighbor Discovery Algorithm Based on the Combination of Directional Antenna and Multi-channel Parallelism. Smart Innovation, Systems and Technologies, 99\u2013107. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_9\n"},{"Title":"A Knowledge Representation Method for Hierarchical Diagnosis Decision-Making Under Uncertain Conditions","DOI":"10.1007\/978-981-99-1230-8_2","Publication year":2023,"Abstract":"In order to effectively solve the problems existing in hierarchical diagnosis decision-making knowledge representation under uncertain conditions, the paper lists several key characteristics of complex systems diagnosis decision-making knowledge, combines with hierarchical diagnosis decision-making knowledge classification, makes ontology modeling of related diagnosis decision-making knowledge based on ontology theory, and constructs the semantic hierarchical analysis and its relationship between global ontology and each core domain ontology. In this way, the paper realizes semantic hierarchical analysis and knowledge association. Through the case study of a certain modular aircraft support system, the knowledge representation of diagnosis decision-making under uncertain conditions is realized. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"information classification;computer aided diagnosis;decision theory;hierarchical systems;knowledge representation;ontology;semantics","MidLevel":"education;artificial intelligence;medical;data;web services;other","HighLevel":"industries;technology;other","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[45,453,402,179,86,361,458,156,30,315,479,121,457,166,202,436,466,25,446,157],"Top20Abs":[453,402,86,45,179,361,458,121,30,156,315,479,166,457,164,330,388,140,466,359],"Top20Tags":[480,373,446,484,45,464,482,433,473,207,138,488,457,156,315,476,278,290,307,413],"Citation":"Ge, Y., Hou, X., Meng, Z., & Lu, Y. (2023). A Knowledge Representation Method for Hierarchical Diagnosis Decision-Making Under Uncertain Conditions. Smart Innovation, Systems and Technologies, 11\u201329. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_2\n"},{"Title":"Research on Application Technology of Multi-source Fusion of 3D Digital Resources in Power Grid","DOI":"10.1007\/978-981-99-1230-8_18","Publication year":2023,"Abstract":"In view of the technical requirements of 3D model resource fusion in power grid infrastructure and operation and inspection, this paper studies the spatial transformation of multi-media heterogeneous models, pose registration, and comprehensive reconstruction algorithms and analyzes the multi-source 3D digital resource fusion analysis of variable time and space technology. A fusion registration analysis method of 3D digital resources oriented to multi-medium and changeable space&ndash;time is proposed, forming a management scheme for updating 3D digital models of infrastructure and operation inspection. Relevant achievements support the integration analysis and interaction of 3D digital resources and provide a technical basis for the fusion application of multi-source and polymorphic 3D models. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"3d modeling;image reconstruction","MidLevel":"manufacturing;construction;computer vision","HighLevel":"industries;technology","Venue":"Smart Innov. Syst. Technol.","Top20AbsAndTags":[486,469,340,479,321,476,402,457,24,189,157,307,99,488,275,475,478,322,412,81],"Top20Abs":[486,469,321,479,340,402,157,476,345,322,189,412,81,307,99,275,488,77,463,471],"Top20Tags":[457,478,475,24,189,476,184,446,464,370,207,479,157,307,473,488,183,278,483,469],"Citation":"Yu, H., He, Z., Peng, L., Shen, J., & Qian, K. (2023). Research on Application Technology of Multi-source Fusion of 3D Digital Resources in Power Grid. Smart Innovation, Systems and Technologies, 203\u2013211. https:\/\/doi.org\/10.1007\/978-981-99-1230-8_18\n"},{"Title":"Image Stitching Based on Color Difference and KAZE with a Fast Guided Filter","DOI":"10.3390\/s23104583","Publication year":2023,"Abstract":"Image stitching is of great importance for multiple fields such as moving object detection and tracking, ground reconnaissance and augmented reality. To ameliorate the stitching effect and alleviate the mismatch rate, an effective image stitching algorithm based on color difference and an improved KAZE with a fast guided filter is proposed. Firstly, the fast guided filter is introduced to reduce the mismatch rate before feature matching. Secondly, the KAZE algorithm based on improved random sample consensus is used for feature matching. Then, the color difference and brightness difference of the overlapping area are calculated to make an overall adjustment to the original images so as to improve the nonuniformity of the splicing result. Finally, the warped images with color difference compensation are fused to obtain the stitched image. The proposed method is evaluated by both visual effect mapping and quantitative values. In addition, the proposed algorithm is compared with other current popular stitching algorithms. The results show that the proposed algorithm is superior to other algorithms in terms of the quantity of feature point pairs, the matching accuracy, the root mean square error and the mean absolute error. &copy; 2023 by the authors.","LowLevel":"color;colorimetry;image enhancement;mean square error;object detection","MidLevel":"graphics;computer vision;data;input","HighLevel":"technology","Venue":"Sensors","Top20AbsAndTags":[99,318,98,76,132,435,471,95,20,350,290,461,431,456,116,464,201,466,323,441],"Top20Abs":[318,99,76,98,435,132,471,20,95,350,461,116,464,456,323,341,290,466,425,431],"Top20Tags":[431,290,201,433,138,479,487,74,169,413,183,444,441,223,473,307,30,283,317,149],"Citation":"Zhang, C., Wang, D., & Sun, H. (2023). Image Stitching Based on Color Difference and KAZE with a Fast Guided Filter. Sensors, 23(10), 4583. https:\/\/doi.org\/10.3390\/s23104583\n"},{"Title":"Compact imaging system based on multi-variable focal lens system in AR display","DOI":"10.1117\/12.2648207","Publication year":2023,"Abstract":"Augmented reality (AR) has been attracted considerable attention according to the demand for non-face-to-face services. The principle of AR is overlapping a virtual image in the real world. To display a virtual image at a proper position, depth of field is a significant factor. In this paper, we propose a multi-variable focal lens system that can dynamically tune a depth of field. By using a multifocal lens that has several different focal lengths, an image has depth information corresponding to each focal length. A focus tunable lens controls a focused area and magnification to display the appropriate position and size. The proposed system has a huge advantage in form factor and fever issues owing to its simple architecture. In order to verify the feasibility of the system for AR, numerical simulations are performed. The system divides a 2D image into focused and defocused areas. Focused and defocused areas show feasibility that can be tuned by the multifocal lens and focus tunable lens. The results show the depth range from 0.3 m to 2 m (3.3D to 0.5D), which is determined by the design of the system. &copy; 2023 SPIE.","LowLevel":"microlenses","MidLevel":"optics;display technology","HighLevel":"displays","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[430,393,285,379,300,2,170,290,74,422,158,107,413,228,235,278,81,425,20,169],"Top20Abs":[430,393,285,379,300,170,2,74,290,422,107,278,81,425,20,413,106,228,175,235],"Top20Tags":[430,285,169,223,317,201,158,228,393,441,445,290,74,431,438,162,138,235,0,444],"Citation":"Lee, J.-S., Cho, S.-H., Lee, S.-J., Choi, W. J., & Choi, Y.-W. (2023). Compact imaging system based on multi-variable focal lens system in AR display. Advances in Display Technologies XIII. https:\/\/doi.org\/10.1117\/12.2648207\n"},{"Title":"A Planar Object Tracking Algorithm with Balanced Speed and Accuracy","DOI":"10.1007\/978-981-99-0416-7_52","Publication year":2023,"Abstract":"The planar object tracking algorithm are widely used in robot navigation, intelligent video surveillance, human-computer interaction and augmented reality. For the given object in the first image of the video, the task of the planar object tracking algorithm is to mark the location of the object in subsequent images. In recent years, the research on planar object tracking technology has made great progress, but most tracking algorithms with strong robustness and high accuracy have a slow speed. In this paper, a planar object tracking algorithm combining GIFT feature descriptor and ORB feature descriptor is proposed to solve this problem. The algorithm first uses the faster tracker based on the ORB feature descriptor to track, and if the difference between the obtained tracking result and the previous frame tracking result is larger than a certain threshold, the tracker based on the GIFT feature descriptor will be used. In particular, the threshold can be adjusted to balance the speed and accuracy of the algorithm. Experimental results on POT dataset show when the threshold is 15, the algorithm has a good performance. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"human computer interaction;human-robot interaction;intelligent robots;security systems;tracking","MidLevel":"education;computer vision;robotics;security;human-computer interaction","HighLevel":"industries;technology;end users and user experience","Venue":"Lect. Notes Electr. Eng.","Top20AbsAndTags":[438,341,471,48,326,116,189,173,76,425,487,318,198,464,20,98,128,8,466,400],"Top20Abs":[438,341,48,326,471,116,76,173,425,198,464,318,20,98,487,189,128,123,41,466],"Top20Tags":[312,291,361,487,19,247,255,205,307,70,407,352,189,481,174,463,159,122,339,210],"Citation":"Li, S., Li, P., & Wen, S. (2023). A Planar Object Tracking Algorithm with Balanced Speed and Accuracy. Proceedings of the International Conference on Internet of Things, Communication and Intelligent Technology, 521\u2013528. https:\/\/doi.org\/10.1007\/978-981-99-0416-7_52\n"},{"Title":"Health Research and Education during and after the COVID-19 Pandemic: An Australian Clinician and Researcher Perspective","DOI":"10.3390\/diagnostics13020289","Publication year":2023,"Abstract":"&lt;b&gt;Introduction: &lt;\/b&gt;The COVID-19 pandemic had an unprecedented global effect on teaching and education. This review discusses research, education and diagnostics from the perspectives of four academic clinicians and researchers across different facilities in Australia. &lt;b&gt;Materials and methods: &lt;\/b&gt;The study adopted a literature review and an Australian researcher's perspective on the impact of the COVID-19 pandemic on health education, research and diagnostics. &lt;b&gt;Results: &lt;\/b&gt;At the start of the pandemic, medical facilities had to adhere urgently to major work restrictions, including social distancing, mask-wearing rules and\/or the closure of facilities to protect staff, students and patients from the risk of COVID-19 infection. Telemedicine and telehealth services were rapidly implemented and adapted to meet the needs of medical education, the teaching of students, trainee doctors, nursing and allied health staff and became a widely accepted norm. The impact on clinical research and education saw the closure of clinical trials and the implementation of new methods in the conducting of trials, including electronic consents, remote patient assessments and the ability to commence fully virtual clinical trials. Academic teaching adapted augmented reality and competency-based teaching to become important new modes of education delivery. Diagnostic services also required new policies and procedures to ensure the safety of personnel. &lt;b&gt;Conclusions: &lt;\/b&gt;As a by-product of the COVID-19 pandemic, traditional, face-to-face learning and clinical research were converted into online formats. An hybrid environment of traditional methods and novel technological tools has emerged in readiness for future pandemics that allows for virtual learning with concurrent recognition of the need to provide for interpersonal interactions.","LowLevel":"biomedical education;computer aided instruction;computer based training;diseases;epidemics;health care;medical computing;medical information systems;patient care;teaching;telemedicine","MidLevel":"education;training;medical;developers;farming and natural science","HighLevel":"industries;technology;use cases","Venue":"Diagnostics (Switzerland)","Top20AbsAndTags":[33,40,391,378,449,13,146,171,336,363,37,414,25,91,136,112,272,151,312,61],"Top20Abs":[33,40,391,449,378,13,146,171,363,37,91,151,312,414,136,321,259,237,272,336],"Top20Tags":[363,391,336,140,124,376,171,145,33,112,117,262,164,3,82,56,408,47,87,329],"Citation":"Cordato, D. J., Fatima Shad, K., Soubra, W., & Beran, R. G. (2023). Health Research and Education during and after the COVID-19 Pandemic: An Australian Clinician and Researcher Perspective. Diagnostics, 13(2), 289. https:\/\/doi.org\/10.3390\/diagnostics13020289\n"},{"Title":"Effects of Wearable Hybrid AR\/VR Learning Material on High School Students' Situational Interest, Engagement, and Learning Performance: the Case of a Physics Laboratory Learning Environment","DOI":"10.1007\/s10956-022-10001-4","Publication year":2023,"Abstract":"This study investigates the effect of incorporating different learning materials (paper textbooks, wearable AR material, and wearable hybrid AR\/VR material) in a physics laboratory education on the situational interest, engagement, and learning performance of high school students. The study utilized a quasi-experimental research design. The participants were 105 students, who were assigned to three groups: the traditional learning group, wearable AR group, and wearable hybrid AR\/VR group. The instruments included a situational interest scale, an engagement scale, a learning performance test, and an open-ended questionnaire. The results showed that the situational interest and learning performance of the wearable hybrid AR\/VR group were significantly higher, compared with that of the traditional learning group. The engagement of the wearable hybrid AR\/VR group was significantly higher, compared with that of the other two groups. The wearable hybrid AR\/VR material increased situational interest, engagement, and learning performance in the physical laboratory course. This study suggests that instructors can use wearable hybrid AR\/VR to enhance situational interest, engagement, and learning performance among learners in science laboratory learning environments.","LowLevel":"computer aided instruction;educational courses;further education;human factors;wearable computers","MidLevel":"education;training;human factors;medical;inspection, safety and quality;wearables;developers","HighLevel":"displays;end users and user experience;use cases;industries;technology","Venue":"J. Sci. Educ. Technol. (Germany)","Top20AbsAndTags":[163,185,342,320,210,171,369,306,60,272,439,57,114,39,87,154,136,200,88,442],"Top20Abs":[163,185,342,210,171,369,60,320,306,272,39,439,114,87,165,88,200,136,442,57],"Top20Tags":[439,405,154,88,124,196,195,145,204,114,33,320,288,147,410,9,87,171,57,21],"Citation":"Sun, J. C.-Y., Ye, S.-L., Yu, S.-J., & Chiu, T. K. F. (2022). Effects of Wearable Hybrid AR\/VR Learning Material on High School Students\u2019 Situational Interest, Engagement, and Learning Performance: the Case of a Physics Laboratory Learning Environment. Journal of Science Education and Technology, 32(1), 1\u201312. https:\/\/doi.org\/10.1007\/s10956-022-10001-4\n"},{"Title":"Pareto Optimal Layouts for Adaptive Mixed Reality","DOI":"10.1145\/3544549.3585732","Publication year":2023,"Abstract":"Adaptive mixed reality applications adjust their user interfaces based on the context in which they are used to provide a smooth experience for different users and environments. This involves carefully positioning UI elements, which can be challenging due to the many possible placements and need to balance competing goals. Current approaches employ global criterion optimization methods like weighted sums, which can be difficult to use, inflexible, and might not find preferred solutions. This can prevent the adaptations from meeting end-user expectations. We suggest using online multi-objective optimization methods which generate a set of Pareto optimal adaptation proposals, giving users more control and adding flexibility to the computational decision-making. We explore the feasibility of our approach by generating adaptations for a basic synthetic example and discuss relevant dimensions for a formal evaluation with end-users, including the choice of algorithm, decomposition technique, and objectives.","LowLevel":"decision making;human computer interaction;pareto optimisation;user experience;user interfaces","MidLevel":"business performance metrics;human-computer interaction;human factors;other","HighLevel":"end users and user experience;business;other","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[79,15,179,419,208,166,288,365,353,53,280,205,248,409,70,214,471,86,465,410],"Top20Abs":[208,15,419,280,214,166,79,353,410,304,104,340,471,432,440,288,53,70,179,409],"Top20Tags":[365,142,415,37,248,29,79,179,63,419,242,364,70,36,288,339,436,205,53,259],"Citation":"Johns, C. A., Evangelista Belo, J. M., Klokmose, C. N., & Pfeuffer, K. (2023). Pareto Optimal Layouts for Adaptive Mixed Reality. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585732\n"},{"Title":"Deep Learning for Visual SLAM: The State-of-the-Art and Future Trends","DOI":"10.3390\/electronics12092006","Publication year":2023,"Abstract":"Visual Simultaneous Localization and Mapping (VSLAM) has been a hot topic of research since the 1990s, first based on traditional computer vision and recognition techniques and later on deep learning models. Although the implementation of VSLAM methods is far from perfect and complete, recent research in deep learning has yielded promising results for applications such as autonomous driving and navigation, service robots, virtual and augmented reality, and pose estimation. The pipeline of traditional VSLAM methods based on classical image processing algorithms consists of six main steps, including initialization (data acquisition), feature extraction, feature matching, pose estimation, map construction, and loop closure. Since 2017, deep learning has changed this approach from individual steps to implementation as a whole. Currently, three ways are developing with varying degrees of integration of deep learning into traditional VSLAM systems: (1) adding auxiliary modules based on deep learning, (2) replacing the original modules of traditional VSLAM with deep learning modules, and (3) replacing the traditional VSLAM system with end-to-end deep neural networks. The first way is the most elaborate and includes multiple algorithms. The other two are in the early stages of development due to complex requirements and criteria. The available datasets with multi-modal data are also of interest. The discussed challenges, advantages, and disadvantages underlie future VSLAM trends, guiding subsequent directions of research.","LowLevel":"computer vision;data acquisition;deep learning (artificial intelligence);feature extraction;image processing;mobile robots;pose estimation;robot vision;service robots;slam robotics","MidLevel":"artificial intelligence;computer vision;data;medical;robotics;graphics;liberal arts;chemical;input;other","HighLevel":"industries;technology;other","Venue":"Electronics (Switzerland)","Top20AbsAndTags":[401,387,160,425,81,341,116,12,255,163,486,379,453,318,307,76,178,480,283,483],"Top20Abs":[401,387,160,163,425,486,116,341,453,480,12,81,150,255,318,178,483,244,307,39],"Top20Tags":[387,379,396,255,425,81,76,98,415,71,99,160,95,353,143,220,18,347,115,132],"Citation":"Favorskaya, M. N. (2023). Deep Learning for Visual SLAM: The State-of-the-Art and Future Trends. Electronics, 12(9), 2006. https:\/\/doi.org\/10.3390\/electronics12092006\n"},{"Title":"Analysis of enablers for the digitalization of supply chain using an interpretive structural modelling approach","DOI":"10.1108\/IJPPM-09-2020-0481","Publication year":2023,"Abstract":"&lt;b&gt;Purpose &lt;\/b&gt;Over the years, technology development has rationalized supply chain processes. The demand economy is disrupting every sector causing the supply chain to be more innovative than ever before. The digitalization of the supply chain fulfils this demand. Several technologies such as blockchain, big data analytics, 3D printing, Internet of things (IoT), artificial intelligence (AI), augmented reality (AR), etc. have been innovated in recent years, which expedite the digitalization of the supply chain. The paper aims to analyse the applicability of these technological enablers in the digital transformation of the supply chain and to present an interpretive structural modelling (ISM) model, which presents a sequence in which enablers can be implemented in a sequential manner. &lt;b&gt;Design\/methodology\/approach &lt;\/b&gt;This paper employed the ISM approach to propose a various levelled model for the enablers of the digital supply chain. The enablers are also classified graphically based on their driving and dependence powers using matrix multiplication cross-impact applied to classification (MICMAC) analysis. &lt;b&gt;Findings &lt;\/b&gt;The study indicates that the enablers \"big data analytics\", \"IoT\", \"blockchain\" and \"AI\" are the most powerful enablers for the digitalization of the supply chain and actualizing these enablers should be a topmost concern for organizations, which want to exploit new opportunities created by these technologies. &lt;b&gt;Practical implications &lt;\/b&gt;This study presents a systematic approach to adopt new technologies for performing various supply chain activities and assists the policymakers better organize their assets and execution endeavours towards digitalization of the supply chain. &lt;b&gt;Originality\/value &lt;\/b&gt;This is one of the initial research studies, which has analysed the enablers for the digitalization supply chain using the ISM approach.","LowLevel":"artificial intelligence;big data;blockchains;data analysis;internet of things;matrix multiplication;organisational aspects;supply chain management;supply chains","MidLevel":"internet of things;artificial intelligence;data;security;liberal arts;business planning and management;networks;other;logistics","HighLevel":"industries;technology;business;other","Venue":"Int. J. Product. Perform. Manag. (UK)","Top20AbsAndTags":[33,40,390,146,273,449,13,37,281,361,334,151,359,321,91,259,289,237,448,4],"Top20Abs":[33,40,390,146,273,449,13,37,361,151,91,321,259,334,237,281,159,447,359,479],"Top20Tags":[289,359,129,4,377,273,271,258,123,226,429,298,220,38,308,355,121,334,110,280],"Citation":"Agrawal, P., & Narain, R. (2021). Analysis of enablers for the digitalization of supply chain using an interpretive structural modelling approach. International Journal of Productivity and Performance Management, 72(2), 410\u2013439. https:\/\/doi.org\/10.1108\/ijppm-09-2020-0481\n"},{"Title":"Identity Threats in the Metaverse and Future Research Opportunities","DOI":"10.1109\/ICBATS57792.2023.10111122","Publication year":2023,"Abstract":"The metaverse is the next-generation internet, which can be described as three-dimensional virtual environments that reflect different aspects of the physical world. Users can engage in various experiences while representing themselves as customizable digital avatars. Virtual Reality (VR), Augmented Reality (AR), Digital Twin, Artificial Intelligence (AI), and Blockchain are key technologies that power immersion, interoperability, automation, and 3D elements of the metaverse. Recently, the metaverse has gained significant attention from the tech industry, especially with Facebook rebranding to Meta and announcing its vision for the future metaverse. The metaverse has many applications, including gaming, entertainment, shopping, education, healthcare, and work. Despite the revolutionary potential of the metaverse, several cybersecurity and privacy issues are inevitable and need to be addressed. Specifically, this paper focuses on highlighting identity-related risks for metaverse users. First, an overview of the metaverse elements, technologies, and characteristics is presented, along with a proposed definition of what a user's digital identity represents in the metaverse. Accordingly, potential identity risks and cybersecurity threats are discussed. Finally, AI and biometric authentication techniques are presented as possible solutions and future research opportunities.","LowLevel":"artificial intelligence;avatars;biometrics;blockchains;data privacy;digital twins;internet;open systems;security of data;social networking","MidLevel":"education;artificial intelligence;smart cities;policy;collaboration;presence;security;liberal arts;developers;human-computer interaction;networks;other","HighLevel":"end users and user experience;use cases;business;industries;technology;other","Venue":"2023 International Conference on Business Analytics for Technology and Security (ICBATS)","Top20AbsAndTags":[249,193,251,363,355,226,328,354,204,442,215,448,244,411,44,125,414,268,447,359],"Top20Abs":[249,354,193,251,355,363,226,204,328,442,215,244,448,411,125,414,44,281,440,447],"Top20Tags":[411,104,378,251,226,100,289,249,359,193,368,308,164,219,330,452,439,385,213,339],"Citation":"Awadallah, A. M., Damiani, E., Zemerly, J., & Yeun, C. Y. (2023). Identity Threats in the Metaverse and Future Research Opportunities. 2023 International Conference on Business Analytics for Technology and Security (ICBATS). https:\/\/doi.org\/10.1109\/icbats57792.2023.10111122\n"},{"Title":"Following the Master's Hands: Capturing Piano Performances for Mixed Reality Piano Learning Applications","DOI":"10.1145\/3544549.3585838","Publication year":2023,"Abstract":"Piano learning applications in Mixed Reality (MR) are a promising substitute for physical instruction when a piano teacher is absent. Existing piano learning applications that use visual indicators to highlight the notes to be played on the keyboard or employ video projections of a pianist provide minimal guidance on how the learner should execute hand movements to develop their technique in performance and prevent injuries. To address this gap, we developed an immersive first-person piano learning experience that uses a library of targeted visualizations of the teacher's hands and 3D traces of hand movements in MR. Seeing the piano teacher's hands while hearing the music is central to developing the novice's musical intuition. We introduced an end-to-end workflow to accurately capture the pianist's technical gestures and align them with the musical score. We recorded pianists playing technical exercises and music pieces. We developed a multimodal performance dataset (MPD) comprising virtual hand models, keyboard (MIDI) recordings and the corresponding music scores, and different visualizations of hand traces capturing movement. Finally, we developed Pianoverse, an MR application to assist piano learning, and performed exploratory user testing with novice piano players to understand the impact of multimodal representations of movement on skill learning. Our initial observations suggest that apprehending the movement traces of a recorded performance over a physical keyboard increases the learner's ability to position their body and hands correctly and to replicate hand gestures while playing from written music. Further research will focus on automating performance data collection and a comprehensive evaluation of the use of leading movement traces in piano learning.","LowLevel":"computer aided instruction;gesture recognition;injuries;keyboards;music","MidLevel":"training;human factors;audio;medical;input","HighLevel":"end users and user experience;technology;industries;use cases","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[284,366,337,357,400,163,39,415,118,396,87,127,424,172,7,57,262,433,303,296],"Top20Abs":[366,163,357,284,396,400,39,87,424,415,7,172,118,337,303,250,43,177,207,433],"Top20Tags":[337,127,284,180,94,366,405,436,9,82,3,12,112,204,128,93,320,145,275,147],"Citation":"Labrou, K., Zaman, C. H., Turkyasar, A., & Davis, R. (2023). Following the Master\u2019s Hands: Capturing Piano Performances for Mixed Reality Piano Learning Applications. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585838\n"},{"Title":"A Digital Twin Mixed-reality System for Testing Future Advanced Air Mobility Concepts: A Prototype","DOI":"10.1109\/ICNS58246.2023.10124310","Publication year":2023,"Abstract":"The UK Future Flight Vision and Roadmap defines how aviation in the UK is envisioned to develop by 2030. As part of the Future Flight demonstration segment, project HADO (High-intensity Autonomous Drone Operations) will develop, test, and deploy fully automated Unmanned Aircraft System (UAS) operations at London Heathrow airport. The resource-demanding nature of real-world tests, however, suggests that developing and improving the reliability and efficiency of virtual environment-based testing methods is indispensable for the evolution of such operations. Nonetheless, developing a high-fidelity and real-time virtual environment that enables the safe, scalable, and sustainable development, verification, and validation of UAS operations remains a daunting task. Notably, the need to integrate physical and virtual elements with a high degree of correlation presents a significant challenge. Consequently, as part of the synthetic test environment work package within the HADO project, this paper proposes a Digital Twin (DT) system to enable mixed-reality tests in the context of autonomous UAS operations. This connects a physical world to its digital counterpart made up of five distinct layers and several digital elements to support enhanced mixed-reality functionality. The paper highlights how the static layers of the synthetic test environment are built, and presents a DT prototype that supports mixed-reality test capabilities. In particular, the ability to inject virtual obstacles into physical test environments is demonstrated, highlighting how the sharp boundaries between virtual environments and reality can be blurred for safe, flexible, efficient, and effective testing of UAS operations.","LowLevel":"aerospace computing;aerospace testing;airports;autonomous aerial vehicles;digital twins","MidLevel":"smart cities;automotive;aviation and aerospace;inspection, safety and quality;other","HighLevel":"industries;use cases;other","Venue":"2023 Integrated Communication, Navigation and Surveillance Conference (ICNS)","Top20AbsAndTags":[440,389,118,226,120,286,122,412,439,9,408,436,207,245,136,395,365,134,7,399],"Top20Abs":[440,389,118,120,226,122,412,207,408,136,286,395,439,9,365,436,245,429,86,41],"Top20Tags":[282,398,106,65,286,64,407,101,68,310,254,345,411,322,266,67,311,412,60,120],"Citation":"Zhao, J., Conrad, C., Delezenne, Q., Xu, Y., & Tsourdos, A. (2023). A Digital Twin Mixed-reality System for Testing Future Advanced Air Mobility Concepts: A Prototype. 2023 Integrated Communication, Navigation and Surveillance Conference (ICNS). https:\/\/doi.org\/10.1109\/icns58246.2023.10124310\n"},{"Title":"Digital technology implementation and impact of artificial intelligence based on bipolar complex fuzzy Schweizer&ndash;Sklar power aggregation operators","DOI":"10.1016\/j.asoc.2023.110375","Publication year":2023,"Abstract":"Digital technology refers to any technology that uses digital signals or electronic data to process, store, and transmit information. Some examples of digital technologies include social media platforms, cloud computing, artificial intelligence, virtual and augmented reality, and blockchain technology. Digital technology has the potential to play a significant role in achieving sustainable development goals by providing solutions for a wide range of environmental, social, and economic challenges. In this manuscript, we investigate digital technology implementation under sustainable development and would find which area of sustainable development is most in need of digital technology. Further, we investigate the operational laws based on Schweizer&ndash;Sklar t-norm and t-conorm and originate aggregation operators based on these deduced operational laws under the environment of bipolar complex fuzzy set that is bipolar complex fuzzy Schweizer&ndash;Sklar power averaging, bipolar complex fuzzy Schweizer&ndash;Sklar power weighted averaging, bipolar complex fuzzy Schweizer&ndash;Sklar power geometric and bipolar complex fuzzy Schweizer&ndash;Sklar power weighted geometric operators and then we deduce techniques of decision-making utilizing these originated operators. Afterward, we tackle a numerical example related to the digital technology implementation under sustainable development by considering artificial data and finding the area of sustainable development which is most in need of digital technology. Moreover, we reveal the impact of one of the digital technologies that are artificial intelligence in the field of healthcare and study a numerical example by considering hypothetical data by employing the originated technique of decision-making. At the last, we do a comparison of the deduced operators with numerous current operators to reveal the superiority and benefits of the deduced operators. &copy; 2023 Elsevier B.V.","LowLevel":"artificial intelligence;decision making;environmental regulations;environmental technology;fuzzy logic;fuzzy sets;health care;mathematical operators;sustainable development","MidLevel":"artificial intelligence;human factors;policy;farming and natural science;medical;liberal arts;human-computer interaction;other","HighLevel":"end users and user experience;business;industries;technology;other","Venue":"Appl. Soft Comput.","Top20AbsAndTags":[281,179,310,479,84,414,486,226,467,174,377,315,278,247,273,368,261,466,345,86],"Top20Abs":[281,179,479,414,310,486,226,174,84,467,261,86,273,377,249,368,252,247,150,466],"Top20Tags":[446,468,156,315,432,278,18,471,308,430,480,339,433,184,135,368,138,436,477,415],"Citation":"Mahmood, T., & Rehman, U. ur. (2023). Digital technology implementation and impact of artificial intelligence based on bipolar complex fuzzy Schweizer\u2013Sklar power aggregation operators. Applied Soft Computing, 143, 110375. https:\/\/doi.org\/10.1016\/j.asoc.2023.110375\n"},{"Title":"Mixed Reality Based Teleoperation of Surgical Robotics","DOI":"10.1109\/ISMR57123.2023.10130178","Publication year":2023,"Abstract":"Many surgical robotic systems are controlled by mechanical based devices that require the operator to remain at a fixed location away from the robot. This restriction in mobility and physical barrier between the surgeon and the robot may reduce procedural efficiency. Thus, we propose an alternative teleoperation approach and mixed reality based system that uses the surgeon's tracked hand poses to control the robot through the use of an untethered head mounted display. We conducted a controlled user study to assess the efficacy of our system. Our experimental results indicate that, for the ring-wire task we tested, there is not a considerable difference in the performance of users compared to existing mechanical based teleoperation devices.","LowLevel":"control engineering computing;helmet mounted displays;surgery;surgical robots;telerobotics","MidLevel":"engineering;robotics;medical;wearables;other;display technology","HighLevel":"displays;technology;industries;other","Venue":"2023 International Symposium on Medical Robotics (ISMR)","Top20AbsAndTags":[69,19,174,234,425,322,170,366,326,420,465,279,407,444,399,440,118,269,253,195],"Top20Abs":[19,69,174,425,322,366,326,420,269,170,261,465,401,118,407,312,248,158,440,195],"Top20Tags":[420,366,399,238,21,425,69,398,206,279,234,174,217,326,364,440,301,80,75,243],"Citation":"Chen, A. C., Hadi, M., Kazanzides, P., & Azimi, E. (2023). Mixed Reality Based Teleoperation of Surgical Robotics. 2023 International Symposium on Medical Robotics (ISMR). https:\/\/doi.org\/10.1109\/ismr57123.2023.10130178\n"},{"Title":"From motivational experience to creative writing: A motivational AR-based learning approach to promoting Chinese writing performance and positive writing behaviours","DOI":"10.1016\/j.compedu.2023.104844","Publication year":2023,"Abstract":"The ability to write is one of the key literacies required by students in relation to 21st century skills and is an emphasis of early education. Although the ability to write is a vital foundation for further education, due to the lack of authentic experience and limited instructional methods, cultivating writing ability in the early stage of education is still a significant challenge for students in elementary schools. Therefore, the majority of students are not able to actively engage in the process of learning to write. To address these problems, this study proposes a motivational AR-based (MAR) learning approach to support students in learning to write, which not only provides an authentic learning environment, but also aims to motivate students' active writing. Its effects were verified through a quasi-experiment. A total of 47 elementary school students from China were invited to participate, and were assigned to either an experimental group (EG) which was exposed to the MAR approach or the control group (CG) which was exposed to the motivational learning approach in a conventional environment (MC). The results showed that the proposed MAR learning approach improved the students' writing performance in terms of feature descriptiveness and thinking innovation. It can be found that, compared with the CG, the achievement gaps between the low- and high-engagement students were much larger in the EG. In addition, the sequential pattern analysis results showed that students who learned with the MAR learning approach concentrated more on the process of observation than those who learned with the MC learning approach, and they tended to exhibit less distracted behaviours. To sum up, the proposed MAR learning approach was effective in terms of facilitating elementary school students&rsquo; writing education. The main contribution of our study is that it provides evidence for the effectiveness of the proposed MAR learning approach and opens up opportunities for future studies which aim to further explore its impacts in different designs of writing learning activities. &copy; 2023 Elsevier Ltd","LowLevel":"computer aided instruction;e-learning;learning systems","MidLevel":"education;medical;training","HighLevel":"industries;use cases","Venue":"Comput Educ","Top20AbsAndTags":[36,163,320,136,185,39,306,164,128,342,114,383,207,88,82,134,87,264,262,145],"Top20Abs":[36,163,136,320,306,88,185,39,164,128,342,114,82,383,264,87,207,171,272,154],"Top20Tags":[454,161,184,446,485,443,207,442,459,341,342,413,135,307,434,188,33,312,363,157],"Citation":"Li, M., Chen, Y.-T., Huang, C.-Q., Hwang, G.-J., & Cukurova, M. (2023). From motivational experience to creative writing: A motivational AR-based learning approach to promoting Chinese writing performance and positive writing behaviours. Computers &amp; Education, 202, 104844. https:\/\/doi.org\/10.1016\/j.compedu.2023.104844\n"},{"Title":"Can Cross-Reality Help Nonspeaking Autistic People Transition to AR Typing?","DOI":"10.1145\/3544549.3585859","Publication year":2023,"Abstract":"About of autistic people are nonspeakers; they cannot use speech to communicate effectively. Pointing to letters on a letterboard held by a Communication and Regulation Partner (CRP) is one alternative method of expressive communication that some members of this population use. In the training of this method, a CRP delivers engaging and customized lessons. Additionally, the CRP provides regulatory, sensory, and attentional support and also works to strengthen the subject's pointing skills. The goal of this training is to equip individuals with the required skills to be able to type independently. Recent studies have proposed using AR to provide opportunities for nonspeakers to practice the motor skills involved in typing. To use such systems, however, there needs to be a transition phase where a CRP teaches their subject how to interact with a virtual letterboard. In this paper, we explore the feasibility of using cross-reality, in which a CRP and nonspeaker can interact with the same virtual objects simultaneously, as a possible means of fostering this transition. We report a study involving 5 nonspeaking autistic subjects with diverse motor skills interacting using a virtual HoloLens 2 letterboard system we developed called HoloBoard. All subjects succeeded in pointing to letters correctly or spelling on the virtual board. We report process and design recommendations based on feedback obtained from subjects and their CRPs.","LowLevel":"computer aided instruction;handicapped aids;medical disorders","MidLevel":"training;medical","HighLevel":"industries;use cases","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[351,128,37,400,455,253,414,188,34,436,103,69,357,479,222,232,408,184,233,196],"Top20Abs":[351,37,128,455,400,253,188,69,414,103,222,357,384,436,408,34,184,232,88,313],"Top20Tags":[93,34,376,9,222,394,219,204,180,117,405,33,253,145,296,419,363,3,214,82],"Citation":"Alabood, L., Dow, T., Kaufman, K. M., Jaswal, V. K., & Krishnamurthy, D. (2023). Can Cross-Reality Help Nonspeaking Autistic People Transition to AR Typing? Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585859\n"},{"Title":"ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms","DOI":"10.1145\/3544548.3581381","Publication year":2023,"Abstract":"We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to existing video or AR\/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches, we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality hand gestural navigation and verbal communication. By overlaying the remote instructor's virtual hands in the local user's MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user\/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively teleoperating a real human. We deploy and evaluate our system in classrooms of physiotherapy training, as well as other application domains such as mechanical assembly, sign language and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.","LowLevel":"gesture recognition;human computer interaction;telecontrol;telerobotics","MidLevel":"robotics;human-computer interaction;human factors;input","HighLevel":"end users and user experience;technology","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[239,303,17,411,415,439,371,375,329,118,234,205,286,172,255,45,304,50,442,250],"Top20Abs":[239,303,411,17,371,415,439,234,375,329,255,205,304,172,286,442,12,118,50,240],"Top20Tags":[53,420,371,242,66,180,339,418,415,221,407,329,118,375,314,419,213,352,210,122],"Citation":"Faridan, M., Kumari, B., & Suzuki, R. (2023). ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581381\n"},{"Title":"Spatiality and Semantics - Towards Understanding Content Placement in Mixed Reality","DOI":"10.1145\/3544549.3585853","Publication year":2023,"Abstract":"Mixed Reality (MR) popularizes numerous situated applications where virtual content is spatially integrated into our physical environment. However, we only know little about what properties of an environment influence the way how people place digital content and perceive the resulting layout. We thus conducted a preliminary study (N = 8) examining how physical surfaces affect organizing virtual content like documents or charts, focusing on user perception and experience. We found, among others, that the situated layout of virtual content in its environment can be characterized by the level of spatial as well as semantic coupling. Consequently, we propose a two-dimensional design space to establish the vocabularies and detail their parameters for content organization. With our work, we aim to facilitate communication between designers or researchers, inform general MR interface design, and provide a first step towards future MR workspaces empowered by blending digital content and its real-world context.","LowLevel":"human computer interaction;human factors;user experience;user interfaces","MidLevel":"human-computer interaction;human factors","HighLevel":"end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[439,418,309,440,302,397,15,64,123,419,224,195,226,303,121,227,400,288,395,407],"Top20Abs":[439,418,309,440,123,15,302,397,224,64,424,419,226,227,303,121,267,395,407,208],"Top20Tags":[142,382,195,415,29,79,37,248,63,70,205,339,59,259,419,251,213,66,242,88],"Citation":"Ellenberg, M. O., Satkowski, M., Luo, W., & Dachselt, R. (2023). Spatiality and Semantics - Towards Understanding Content Placement in Mixed Reality. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585853\n"},{"Title":"WebJump: AR-facilitated Distributed Display of Web Pages","DOI":"10.1145\/3544549.3585669","Publication year":2023,"Abstract":"Head-mounted displays (HMD) like AR glasses support powerful 3D displays and intuitive input modalities. However, there is a lack of collaboration between the HMD and other displays like PC monitors. In this paper, we propose WebJump, a software tool that analyzes HTML web pages and enables UI elements 'jump' from the PC monitor to the HMD. This way, contents like high-resolution images and long texts can take full advantage of the high-quality monitor display. Auxiliary contents like sidebars and menus, on the other hand, are offloaded to the HMD for easier access albeit with a lower display quality. We describe our software tool and explain in detail its implementation. We develop two applications to demonstrate WebJump's feasibility and potential.","LowLevel":"helmet mounted displays;internet;three-dimensional displays;user interfaces","MidLevel":"human-computer interaction;networks;wearables;display technology","HighLevel":"displays;technology;end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[217,180,69,38,80,425,301,75,332,424,248,279,235,393,278,225,450,365,162,243],"Top20Abs":[217,69,180,38,80,425,248,225,278,162,393,332,235,365,75,432,424,243,141,450],"Top20Tags":[80,279,180,238,21,38,217,206,75,424,301,330,224,382,163,244,165,69,251,343],"Citation":"Zeng, X., Wang, X., Gou, Z., Chen, Y., & Zhang, T. (2023). WebJump: AR-facilitated Distributed Display of Web Pages. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585669\n"},{"Title":"Demystifying and Analysing Metaverse Towards Education 4.0","DOI":"10.1109\/ICIPTM57143.2023.10118054","Publication year":2023,"Abstract":"The word Metaverse has influenced many sectors such as healthcare, education, retail and manufacturing and few more industries are there which will be impacted by 2026 as per the research conducted by Gartner. The word \"Metaverse\" especially in education sector came into existence after the COVID-19 epidemic when the humanity were forced to think about the new methodology of educating and teaching. This ecosphere is the combination of technologies which enables multimodal interactions with artificial environment, electronic library and people such as Virtual Reality (VR) and Augmented Reality (AR). It is believed that metaverse will improve collaboration, training process will be enhanced and most importantly it will create a happier workplace. This is only the reason that many corporate giants like Nvidia, facebook, apple, epic Games and companies has shifted towards this pedagogical ecosystem. This technology has the potential which enables absolute incorporating user conversation in actual-time and compelling interactivity with digital artifact. In this paper, we are addressing metaverse in education along with a detailed framework of metaverse in education. It includes a comparative study of conventional education, online education and metaverse education based on parameters like place of learning, resources used, teaching methodology, learning experience, learning target and learning assessment. Competency based education, energize student and positive attitude towards learning. The various challenges of the metaverse in educational sector are also debated. This paper will help the researcher's fraternity to get a deeper insight along with a clear perception of this ecosystem in education.","LowLevel":"computer aided instruction;diseases;epidemics;social networking;teaching","MidLevel":"education;collaboration;medical;training","HighLevel":"industries;use cases","Venue":"2023 3rd International Conference on Innovative Practices in Technology and Management (ICIPTM)","Top20AbsAndTags":[193,377,249,204,442,355,244,328,226,251,354,391,171,215,390,414,136,103,163,306],"Top20Abs":[377,249,204,193,354,442,355,244,226,251,171,328,391,215,306,103,390,163,136,62],"Top20Tags":[193,390,391,164,372,17,33,244,145,219,136,44,232,222,124,87,328,246,336,3],"Citation":"Raj, A., Sharma, V., Rani, S., Singh, T., Shanu, A. K., & Alkhayyat, A. (2023). Demystifying and Analysing Metaverse Towards Education 4.0. 2023 3rd International Conference on Innovative Practices in Technology and Management (ICIPTM). https:\/\/doi.org\/10.1109\/iciptm57143.2023.10118054\n"},{"Title":"Predictive digital twin for offshore wind farms","DOI":"10.1186\/s42162-023-00257-4","Publication year":2023,"Abstract":"As wind turbines continue to grow in size, they are increasingly being deployed offshore. This causes operation and maintenance of wind turbines becoming more challenging. Digitalization is a key enabling technology to manage wind farms in hostile environments and potentially increasing safety and reducing operational and maintenance costs. Digital infrastructure based on Industry 4.0 concept, such as digital twin, enables data collection, visualization, and analysis of wind power analytic at either individual turbine or wind farm level. In this paper, the concept of predictive digital twin for wind farm applications is introduced and demonstrated. To this end, a digital twin platform based on Unity3D for visualization and OPC Unified Architecture (OPC-UA) for data communication is developed. The platform is completed with the Prophet prediction algorithm to detect potential failure of wind turbine components in the near future and presented in augmented reality to enhance user experience. The presentation is intuitive and easy to use. The limitations of the platform include a lack of support for specific features like electronic signature, enhanced failover, and historical data sources. Simulation results based on the Hywind Tampen floating wind farm configuration show our proposed platform has promising potentials for offshore wind farm applications.","LowLevel":"cyber-physical systems;digital twins;offshore installations;power engineering computing;production engineering computing;wind power plants;wind turbines","MidLevel":"education;smart cities;engineering;robotics;power and energy;manufacturing;farming and natural science;other;integration","HighLevel":"use cases;business;industries;technology;other","Venue":"Energy Inform. (Germany)","Top20AbsAndTags":[486,245,429,122,412,281,389,86,266,159,315,41,395,261,479,411,273,465,178,377],"Top20Abs":[486,389,245,479,412,395,315,178,261,266,41,267,385,465,122,4,467,159,373,80],"Top20Tags":[122,429,407,258,388,411,370,486,86,412,281,159,205,45,174,12,322,360,449,419],"Citation":"Haghshenas, A., Hasan, A., Osen, O., & Mikalsen, E. T. (2023). Predictive digital twin for offshore wind farms. Energy Informatics, 6(1). https:\/\/doi.org\/10.1186\/s42162-023-00257-4\n"},{"Title":"Robotic Technology as the Basis of Implementation of Industry 4.0 in Production Processes in China","DOI":"10.1007\/978-3-031-31066-9_1","Publication year":2023,"Abstract":"The implementation of Industry 4.0 has been intensively present in the surrounding countries in the last five to six years. However, its concept is not widespread enough in production processes in the world. Its implementation will improve many aspects of human life in all segments of society. The business paradigm and production models will change at all levels of production processes, including the supply chain. Major changes have taken place in the recent years, such as the transformation of production systems, consumption, delivery, logistics, etc., all due to the implementation of the latest technological discoveries such as: robotics, automation, 3D printing, Internet of Things (IoT), smart sensors, Big Data, Cloud Computing, Radio Frequency Identification (RFID), Virtual and Augmented Reality (AR), Artificial Intelligence (AI), Cyber-Physical Systems (CPS), etc. The implementation strategy of Industry 4.0 consists of adapting industrial production to complete smart automation, which means introducing methods of self-automation, self-configuration, self-diagnosis and elimination of problems, knowledge, and intelligent decision-making. By implementing the &lsquo;&rsquo;Made in China 2025&rsquo;&rsquo; strategy, China has become the first country in the world in terms of robot implementation and vehicle production. The paper provides an analysis of Industry 4.0 patent applications and the trend of robot implementation in the last ten years with the aim of presenting the implementation of Industry 4.0. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"3d printing;decision making;embedded systems;intelligent robots;internet of things;metadata;radio frequency identification;supply chains","MidLevel":"education;internet of things;human factors;telecommunication;data;robotics;business planning and management;developers;input;manufacturing;networks;other;semiconductors","HighLevel":"end users and user experience;business;industries;technology;other","Venue":"Lect. Notes Networks Syst.","Top20AbsAndTags":[388,315,447,429,25,352,281,289,179,159,378,188,2,38,334,174,312,86,18,468],"Top20Abs":[388,429,352,25,281,447,315,289,334,159,179,18,378,38,188,174,468,12,454,2],"Top20Tags":[447,315,291,472,31,487,435,137,312,129,378,158,247,469,175,289,393,25,156,483],"Citation":"Karabegovi\u0107, I., Husak, E., Karabegovi\u0107, E., & Mahmi\u0107, M. (2023). Robotic Technology as the Basis of Implementation of Industry 4.0 in Production Processes in China. Lecture Notes in Networks and Systems, 3\u201318. https:\/\/doi.org\/10.1007\/978-3-031-31066-9_1\n"},{"Title":"Mixed Reality or Simply Mobile? A Case Study on Enabling Less Skilled Workers to Perform Routine Maintenance Tasks","DOI":"10.1016\/j.procs.2022.12.269","Publication year":2023,"Abstract":"The availability of skilled workers has become a severe issue for production companies in developed countries like Germany in recent years. Specialists for maintenance tasks are in strong demand and their workforce should be steered towards the most challenging tasks in maintenance. In this case study we analyze, whether less skilled or not specifically instructed workers can perform routine maintenance tasks without being directly supported by experts. In our setup ten participants try to handle a maintenance routine checklist in three different ways: paper-based printouts, on a mobile tablet, and with a Microsoft HoloLens 2 Mixed Reality (MR) device. The results show that workers fail to complete the task with a mere paper-based approach, but are able to succeed both with the mobile tablet and MR application. Quantitative time measurements and qualitative analyses indicate that the mobile tablet app was easier to learn, did not impair the worker as much as the HoloLens and worked more stable. However, the HoloLens solution showed advantages as hands-free and spatial guiding device, which could be important aspects when better MR hardware will be available and MR interaction principles become common knowledge. All rights reserved Elsevier.","LowLevel":"maintenance engineering;mobile computing;personnel;production engineering computing","MidLevel":"manufacturing;telecommunication;human resources;engineering","HighLevel":"industries;technology;business","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[261,395,419,227,12,131,315,424,397,389,439,303,179,403,418,94,264,388,203,412],"Top20Abs":[395,261,419,315,424,397,227,131,389,439,12,303,403,94,264,418,194,63,246,179],"Top20Tags":[388,370,77,419,84,273,179,86,407,12,45,122,159,205,449,289,411,140,14,93],"Citation":"Wagner, M., Leubner, C., & Strunk, J. (2023). Mixed Reality or Simply Mobile? A Case Study on Enabling Less Skilled Workers to Perform Routine Maintenance Tasks. Procedia Computer Science, 217, 728\u2013736. https:\/\/doi.org\/10.1016\/j.procs.2022.12.269\n"},{"Title":"Spherical Convolution Empowered Viewport Prediction in 360 Video Multicast with Limited FoV Feedback","DOI":"10.1145\/3511603","Publication year":2023,"Abstract":"Field of view (FoV) prediction is critical in 360-degree video multicast, which is a key component of the emerging virtual reality and augmented reality applications. Most of the current prediction methods combining saliency detection and FoV information neither take into account that the distortion of projected 360-degree videos can invalidate the weight sharing of traditional convolutional networks nor do they adequately consider the difficulty of obtaining complete multi-user FoV information, which degrades the prediction performance. This article proposes a spherical convolution-empowered FoV prediction method, which is a multi-source prediction framework combining salient features extracted from 360-degree video with limited FoV feedback information. A spherical convolutional neural network is used instead of a traditional two-dimensional convolutional neural network to eliminate the problem of weight sharing failure caused by video projection distortion. Specifically, salient spatial-temporal features are extracted through a spherical convolution-based saliency detection model, after which the limited feedback FoV information is represented as a time-series model based on a spherical convolution-empowered gated recurrent unit network. Finally, the extracted salient video features are combined to predict future user FoVs. The experimental results show that the performance of the proposed method is better than other prediction methods.","LowLevel":"convolutional neural nets;feature extraction;object detection;recurrent neural nets;time series;video signal processing","MidLevel":"artificial intelligence;computer vision;data;chemical;sensors;semiconductors;networks","HighLevel":"industries;technology","Venue":"ACM Trans. Multimed. Comput. Commun. Appl. (USA)","Top20AbsAndTags":[484,385,453,476,81,274,259,480,260,318,162,379,98,255,50,451,444,116,144,458],"Top20Abs":[484,453,385,259,476,260,81,480,162,318,444,458,109,379,98,274,255,270,190,451],"Top20Tags":[274,143,379,130,255,98,300,5,50,76,387,381,81,116,451,172,71,79,428,99],"Citation":"Li, J., Han, L., Zhang, C., Li, Q., & Liu, Z. (2023). Spherical Convolution Empowered Viewport Prediction in 360 Video Multicast with Limited FoV Feedback. ACM Transactions on Multimedia Computing, Communications, and Applications, 19(1), 1\u201323. https:\/\/doi.org\/10.1145\/3511603\n"},{"Title":"Metaverse and fintech: pathway for innovation and development","DOI":"10.1109\/ICIPTM57143.2023.10117956","Publication year":2023,"Abstract":"The development of the metaverse may have a substantial effect on the financial sector, opening new avenues for firms and investors to interact with digital assets and engage in virtual trade. It's important to note that there are a variety of financial applications for metaverse technology, including virtual banking, trading platforms, and even insurance services. Users may require access to virtual money in a metaverse so they can purchase virtual goods and services. Metaverse banks could provide financial services such as virtual currency exchange, virtual wallet services, and virtual lending. They could also offer traditional banking services, such as loans and savings accounts, but in a virtual context. These services could be used by individuals, businesses, and organizations operating within the metaverse. In stock markets, companies that could benefit from the metaverse's growth include those involved in virtual reality and augmented reality, as well as those that provide the infrastructure required to support the metaverse. TAM (Technology Acceptance Model) is applied for understanding how users perceive and adopt metaverse technology.","LowLevel":"bank data processing;banking;cryptocurrencies;financial data processing;financial management;purchasing;stock markets;technology acceptance model","MidLevel":"human factors;data;business planning and management;other;human-computer interaction;sales and marketing;logistics","HighLevel":"end users and user experience;technology;business;other","Venue":"2023 3rd International Conference on Innovative Practices in Technology and Management (ICIPTM)","Top20AbsAndTags":[193,249,377,226,363,204,328,442,251,354,244,215,448,436,335,411,44,333,359,86],"Top20Abs":[377,193,249,226,354,363,204,442,251,328,244,215,448,335,411,436,307,86,333,268],"Top20Tags":[333,62,359,146,156,378,57,102,13,40,160,282,471,269,210,368,44,129,273,226],"Citation":"Kaur, N., Saha, S., Agarwal, V., & Gulati, S. (2023). Metaverse and Fintech: Pathway for Innovation and Development. 2023 3rd International Conference on Innovative Practices in Technology and Management (ICIPTM). https:\/\/doi.org\/10.1109\/iciptm57143.2023.10117956\n"},{"Title":"Metaverse is the Next Normal and Digital Future: A Systematic Review","DOI":"10.1109\/ICEST56843.2023.10138832","Publication year":2023,"Abstract":"The Digital Future of the World is on the verge of being revolutionized. The Technology, wisdom, and internet thinking of Metaverse will extend the impacts on all aspects of life including education, economy, politics, life, and culture. This study is an overview of the metaverse applications in the near future. The outcomes of this review are aimed to understand the usage and importance of metaverse in various fields by using augmented and virtual realities. Finally, we propose the application of Metaverse in education, economy, politics, and entertainment because of its huge future implications.","LowLevel":"educational computing;entertainment;politics","MidLevel":"education;liberal arts;policy","HighLevel":"industries;business","Venue":"2023 IEEE International Conference on Emerging Trends in Engineering, Sciences and Technology (ICES&amp;T)","Top20AbsAndTags":[377,249,363,193,355,204,328,226,251,442,215,448,244,372,436,241,316,391,64,44],"Top20Abs":[377,249,193,355,363,226,204,328,442,251,448,215,244,391,436,241,64,136,159,171],"Top20Tags":[372,67,83,222,316,168,88,28,87,218,193,46,64,210,377,58,236,51,246,17],"Citation":"Khalid, F. (2023). Metaverse is the Next Normal and Digital Future: A Systematic Review. 2023 IEEE International Conference on Emerging Trends in Engineering, Sciences and Technology (ICES&amp;T). https:\/\/doi.org\/10.1109\/icest56843.2023.10138832\n"},{"Title":"Implementing Virtuality in Production - a Design Science Approach","DOI":"10.1016\/j.procs.2022.12.297","Publication year":2023,"Abstract":"The trend toward smart production is omnipresent in research over recent years and there are different approaches how to making a production smart. One of these approaches is to enrich reality with virtual aspects. Alongside the efforts to make production smarter by implementing augmented and virtual reality devices arise the challenges like motion sickness, usability, the weight of devices, or simply the complexity of the implementation process. A forced implementation of smart applications could lead to the frustration of the applicants and ruin the fruitful ground for improvement. The right position for the implementation of virtuality measures as well as the right dose of implementation is essential for success. This paper aims to connect an existing comprehensive approach with the implementation process of virtuality in production and show that such a scientific-based approach supports such a task. Design science research offers an existing, well-elaborated framework for complex tasks and bears the potential to support a virtuality implementation process. This led to the research question: Does design science as an existing research framework support the virtuality implementation process in production? Design science defines the environment of application as characterized by people (in different roles and capabilities) an organization (with different processes and structures) and technologies (like infrastructure and communication). This environment could be seen as the production environment. Further, we do have a broad knowledge base due to the profound work of the academic community in the field of augmented and virtual reality and a huge methodological toolbox. The existing environment and knowledge base are the basements to create an artifact, evaluate, and reshaping it. With this paper, we suggest a design science research-based procedure to support the implementation of virtuality in production. Design science research comes out from information science and is more and more applied in other research fields. As a result of our work, we saw that following a structured approach like the design science approach has the potential to support companies to successfully implement virtuality measures in production.","LowLevel":"production engineering computing","MidLevel":"manufacturing;engineering","HighLevel":"industries;technology","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[361,372,179,233,114,203,418,12,360,376,115,188,279,475,205,261,334,269,286,414],"Top20Abs":[361,372,179,203,233,429,418,114,115,188,336,12,376,269,261,334,286,279,207,414],"Top20Tags":[370,360,407,12,205,122,45,77,159,179,449,84,419,411,174,181,227,273,362,143],"Citation":"Brunner, M., Jodlbauer, H., Bachmann, N., & Tripathi, S. (2023). Implementing Virtuality in Production - a Design Science Approach. Procedia Computer Science, 217, 988\u2013997. https:\/\/doi.org\/10.1016\/j.procs.2022.12.297\n"},{"Title":"You Can Handle, You Can Teach It: Systematic Review on the Use of Extended Reality and Artificial Intelligence Technologies for Online Higher Education","DOI":"10.3390\/su15043507","Publication year":2023,"Abstract":"Over the past year, defined by the COVID-19 pandemic, we have witnessed a boom in applying key emerging technologies in education. In such challenging situations, technology and education expanded their work together to strengthen and interactively impact the learning process in the online higher education context. From a pedagogical perspective, extended reality (XR) and artificial intelligence (AI) were accessible toolboxes to amplify an active and learner-centered teaching method. Whether and how such activities will continue in a post-COVID-19 situation remains unclear. In this systematic literature review, we document the application of XR and AI in online higher education settings and build up an accurate depiction of their influence after the COVID-19 pandemic outbreak. A significant contribution of the thorough analysis conducted was the corroboration of the growing interest of these fast-emerging technologies and their impact on learner agency and outcomes, making online education more accessible, effective, engaging, collaborative, self-paced, and adapted to the diverse academic trajectories. The momentum brought about by the pandemic has served as an impulse for educators and universities to expand the use of these technologies progressively, meet new challenges, and shape the future of online higher education.","LowLevel":"artificial intelligence;computer aided instruction;diseases;educational institutions;epidemics;further education;teaching","MidLevel":"education;artificial intelligence;training;medical;liberal arts","HighLevel":"industries;technology;use cases","Venue":"Sustainability (Switzerland)","Top20AbsAndTags":[390,171,363,336,308,306,25,56,272,372,88,246,117,87,67,262,145,414,439,329],"Top20Abs":[390,363,171,306,308,336,88,272,204,46,62,57,439,103,87,354,56,246,67,457],"Top20Tags":[308,390,363,171,124,145,58,56,405,372,251,336,262,296,18,33,320,61,115,82],"Citation":"Rangel-de L\u00e1zaro, G., & Duart, J. M. (2023). You Can Handle, You Can Teach It: Systematic Review on the Use of Extended Reality and Artificial Intelligence Technologies for Online Higher Education. Sustainability, 15(4), 3507. https:\/\/doi.org\/10.3390\/su15043507\n"},{"Title":"Digital tools in chemical engineering education: The needs and the desires","DOI":"10.1016\/j.ece.2023.05.002","Publication year":2023,"Abstract":"Educators in chemical engineering have a long and rich history of employing digital tools to solve fundamental engineering problems. Today, with the megatrend of digitalisation, there is a growing set of tools that can be used for chemical engineering education. However, identifying which tool is ideally suited to support teaching a given chemical engineering concept can be challenging. To answer this question a survey was distributed to Heads of Departments at IChemE institutions and members of the IChemE committees focused on digitalisation. The survey respondents rated Microsoft Excel (VBA), commercial simulators, and scripting tools as ideal for teaching core subjects such as mass and energy balances, mass transfer and reaction engineering while respondents found 3D Models, and Virtual\/Augmented Reality models as being most suited for teaching subjects such as process design, safety and sustainability. Mathematical\/programming simplicity, ease of maintenance, and low initial investment costs were identified as key non-technical aspects that will hinder the adoption of a given digital tool. Weighing the benefits of education and non-technical hurdles, the respondents preferred the use of simpler digitalisation platforms such as Excel and scripting languages over the more advanced platforms such as Virtual\/Augmented Reality where possible. It was identified that the widespread adoption of more advanced digitalisation tools will require removal of the above mentioned non-technical barriers as well as other barriers such as tool shareability. &copy; 2023 Institution of Chemical Engineers","LowLevel":"digital devices;engineering education;investments;mass transfer","MidLevel":"education;business planning and management;other","HighLevel":"industries;business;other","Venue":"Educ. Chem. Eng.","Top20AbsAndTags":[135,134,312,179,414,272,246,306,3,86,158,294,421,171,454,185,295,395,233,68],"Top20Abs":[134,135,158,312,272,179,414,294,3,306,171,124,315,421,68,185,395,86,246,442],"Top20Tags":[454,485,184,369,461,161,442,171,134,135,228,295,272,157,362,191,355,273,458,423],"Citation":"Udugama, I. A., Atkins, M., Bayer, C., Carson, J., Dikicioglu, D., Gernaey, K. V., Glassey, J., Taylor, M., & Young, B. R. (2023). Digital tools in chemical engineering education: The needs and the desires. Education for Chemical Engineers, 44, 63\u201370. https:\/\/doi.org\/10.1016\/j.ece.2023.05.002\n"},{"Title":"MonuAR: M.A.R Application for Visualising 3D Monuments","DOI":"10.1109\/ICNWC57852.2023.10127425","Publication year":2023,"Abstract":"The Art and Heritage has been an integral part of our Indian Culture. These art forms and heritage are left unnoticed just because they are very much away. Therefore these sites can be brought back to life in any environment. The heritage properties of art and craft of India can be brought to life from anywhere by using an AR application on the phone. There are times when people visit the monuments and they are in need of a third party to show them the main attractions of the monument. Instead of relying on a third party, this can be done using this app where an AR marker appears on important tourist spots. The software that are used are Unity, Blender, Python and Visual Studio for C#. The software development kit used are Vuforia AR SDK and Mapbox SDK.The software that are used are Unity, Blender, Python and Visual Studio for C#. The software development kit used are Vuforia AR SDK and Mapbox SDK","LowLevel":"art;data visualization;mobile computing;python;software engineering;solid modelling;travel industry","MidLevel":"telecommunication;data;transportation;liberal arts;developers;manufacturing;other","HighLevel":"industries;technology;other","Venue":"2023 International Conference on Networking and Communications (ICNWC)","Top20AbsAndTags":[51,207,405,105,90,88,229,155,6,218,166,34,443,197,216,236,159,109,157,99],"Top20Abs":[51,207,155,105,6,90,218,405,229,166,63,34,161,443,49,159,18,216,197,236],"Top20Tags":[96,51,82,410,405,88,42,376,90,109,403,105,77,275,93,309,22,229,16,136],"Citation":"Sorna, S. D., Bhuvaneswaran, B., Manoj, A. R., & Pooja, S. (2023). MonuAR:M.A.R Application for Visualising 3D Monuments. 2023 International Conference on Networking and Communications (ICNWC). https:\/\/doi.org\/10.1109\/icnwc57852.2023.10127425\n"},{"Title":"Open Source Video-Based Hand-Eye Calibration","DOI":"10.1117\/12.2651160","Publication year":2023,"Abstract":"Augmented reality is becoming prevalent in modern video-based surgical navigation systems. Augmented reality in forms of image-fusion between the virtual objects (i.e. virtual representation of the anatomy derived from preoperative imaging modalities) and the real objects (i.e. anatomy imaged by a spatially-tracked surgical camera) facilitate the visualization and perception of the surgical scene. However, this requires spatial calibration between the external tracking system and the optical axis of the surgical camera, known as hand-eye calibration. With the standard implementation of the most common hand-eye calibration techniques being static-photo-based, the time required for data collection may inhibit the thoroughness and robustness to achieve an accurate calibration. To address these translational issues, we introduce a video-based hand-eye calibration technique with open-source implementation that is accurate and robust. Based on the point-to-line Procrustean registration, a short video of a tracked and pivot-calibrated ball-tip stylus was recorded where, in each frame of the tracked video, the 3D position of the ball-tip (point) and its projection onto the video (line) serve as a calibration data point. We further devise a data sampling mechanism designed to optimize the spatial configuration of the calibration fiducials, leading to consistently high quality hand-eye calibrations. To demonstrate the efficacy of our work, a Monte Carlo simulation was performed to obtain the mean target projection error as a function of the number of calibration data points. The results obtained, exemplified using a Logitech C920 Pro HD Webcam with an image resolution of 640 &times; 480, show that the mean projection error decreased as more data points were used per calibration, and the majority of mean projection errors fell below 4 pixels. An open-source implementation, in the form of a 3D Slicer module, is available on GitHub. &copy; 2023 SPIE.","LowLevel":"calibration;cameras;errors;image fusion;image resolution;intelligent systems;medical imaging;monte carlo methods;open systems","MidLevel":"education;artificial intelligence;human factors;computer vision;medical;graphics;developers;sensors;input","HighLevel":"industries;technology;end users and user experience","Venue":"Progr. Biomed. Opt. Imaging Proc. SPIE","Top20AbsAndTags":[488,285,270,153,326,2,173,98,321,345,277,170,432,43,431,409,350,259,152,186],"Top20Abs":[488,285,270,326,2,321,98,345,152,277,153,409,444,43,259,318,294,186,162,173],"Top20Tags":[488,170,138,438,43,431,432,446,480,207,201,307,470,479,323,473,453,487,464,0],"Citation":"Kemper, T. N., Allen, D. R., Rankin, A., Peters, T. M., & Chen, E. C. S. (2023). Open source video-based hand-eye calibration. Medical Imaging 2023: Image-Guided Procedures, Robotic Interventions, and Modeling. https:\/\/doi.org\/10.1117\/12.2651160\n"},{"Title":"Towards Virtual Displays in the Interventional Radiology Suite: A Feasibility Study","DOI":"10.1117\/12.2654267","Publication year":2023,"Abstract":"Interventional Radiology (IR) is a rapidly advancing field, with complex procedures and techniques being developed at increasingly high rates. As these procedures and the underlying imaging technology continue to evolve, one of the challenges for physicians lies in maintaining optimal visualization of the various displays used to guide the procedure. Many Augmented Reality Surgical Navigation Systems (AR-SNS) have been proposed in the literature that aim to improve the way physicians visualize their patient's anatomy, but there are few that address the problem of space within the IR suite. Our solution is the incorporation of an Augmented Reality \"cockpit\", which streams and renders image data inside virtual displays visualized within the Hololens 2, eliminating the need for physical displays. The benefits of our approach is that sterile free interaction and customization can be performed using hand gestures and voice commands, and the physician can optimize the positioning of the display without the need to worry about physical interference from other equipment. For proof of concept, we performed a user study to validate the suitability of our approach in the context of liver tumour ablation procedures. We found there was no significant differences in insertion accuracy or time between the proposed approach and the traditional method. This indicates that visualization of US imaging using our approach is an adequate replacement to the traditional physical display, and paves the way for the next iteration of the system, which is to quantify the benefits of our approach when used in multi-modality procedures. &copy; 2023 SPIE.","LowLevel":"iterative methods;medical imaging;navigation systems;radiology;visualization","MidLevel":"education;artificial intelligence;telecommunication;medical;data;navigation","HighLevel":"industries;technology;use cases","Venue":"Progr. Biomed. Opt. Imaging Proc. SPIE","Top20AbsAndTags":[0,408,470,43,326,178,323,424,63,169,294,4,179,189,433,278,191,225,290,80],"Top20Abs":[408,0,178,326,43,470,225,278,189,248,179,294,424,169,286,191,80,228,224,364],"Top20Tags":[170,43,453,470,0,323,207,138,488,433,373,285,464,430,463,431,480,442,74,462],"Citation":"Allen, D. R., Cattari, N., Cambranis Romero, J. N., Peters, T. M., & Chen, E. C. S. (2023). Towards virtual displays in the interventional radiology suite: a feasibility study. Medical Imaging 2023: Image-Guided Procedures, Robotic Interventions, and Modeling. https:\/\/doi.org\/10.1117\/12.2654267\n"},{"Title":"Global phase insensitive loss function for deep learning in holographic imaging and projection applications","DOI":"10.1117\/12.2648040","Publication year":2023,"Abstract":"Holographic imaging and projection are increasingly used for important applications such as augmented reality,1 3D microscopy2 and imaging through optical fibres.3 However, there are emerging applications that require control or detection of phase, where deep learning techniques are used as faster alternatives to conventional hologram generation algorithms or phase-retrieval algorithms.4 Although conventional mean absolute error (MAE) loss function or mean squared error (MSE) can directly compare complex values for absolute control of phase, there is a class of problems whose solutions are degenerate within a global phase factor, but whose relative phase between pixels must be preserved. In such cases, MAE is not suitable because it is sensitive to global phase differences. We therefore develop a 'global phase insensitive' loss function that estimates the global phase factor between predicted and target outputs and normalises the predicted output to remove this factor before calculating MAE. As a case study we demonstrate &le; 0.1% error in the recovery of complex-valued optical fibre transmission matrices via a neural network. This global phase insensitive loss function will offer new opportunities for deep learning-based holographic image reconstruction, 3D holographic projection for augmented reality and coherent imaging through optical fibres. &copy; 2023 SPIE.","LowLevel":"complex networks;deep learning;errors;holograms;image reconstruction;imaging systems;learning systems;light transmission;mean square error;optical fibers","MidLevel":"education;artificial intelligence;human factors;telecommunication;computer vision;data;medical;graphics;construction;input;display technology","HighLevel":"industries;technology;displays;end users and user experience","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[413,317,47,265,456,235,181,344,228,438,343,270,74,301,453,433,444,158,278,488],"Top20Abs":[47,317,265,456,181,235,344,270,343,301,228,413,438,444,278,273,252,433,225,158],"Top20Tags":[438,323,74,290,223,433,0,341,453,169,488,456,138,480,317,441,483,228,445,201],"Citation":"Zheng, Y., & Gordon, G. S. D. (2023). Global phase insensitive loss function for deep learning in holographic imaging and projection applications. AI and Optical Data Sciences IV. https:\/\/doi.org\/10.1117\/12.2648040\n"},{"Title":"Implementation of varifocal occlusion using lens arrays and focus-tunable lenses","DOI":"10.1117\/12.2649558","Publication year":2023,"Abstract":"Occlusion technology has grown its importance for enhancing immersive augmented reality experiences by improving mutual depth perceptions between the real and virtual scenes. Among various methods for implementing occlusion in augmented reality displays, the 4f system method has gained a lot of attention for its capability to produce a sharp occlusion effect. However, this method has a drawback of having a large form factor and difficulty in achieving sharp occlusion when implemented to display multiple-depth images. Previous studies have applied a lens array to a 4f system to reduce the form factor.1, 2 In this work, we numerically and experimentally investigate the use of a pair of focus-tunable lenses (FTLs) along with a lens array 4f system to achieve sharp occlusion at multiple levels of depths in addition. &copy; 2023 SPIE.","LowLevel":"approximation theory;depth perception;microlenses","MidLevel":"optics;artificial intelligence;human factors;display technology","HighLevel":"displays;technology;end users and user experience","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[437,81,20,379,158,393,317,170,80,278,422,300,285,432,332,228,0,225,74,141],"Top20Abs":[437,81,379,20,170,158,393,80,422,300,278,225,285,208,317,74,211,175,248,98],"Top20Tags":[437,141,169,158,162,373,201,223,432,317,228,393,285,431,482,441,445,290,170,74],"Citation":"Chae, M., Shin, J., Jo, Y., Jeong, Y., & Lee, B. (2023). Implementation of varifocal occlusion using lens arrays and focus-tunable lenses. Advances in Display Technologies XIII. https:\/\/doi.org\/10.1117\/12.2649558\n"},{"Title":"Towards a Mixed Virtual Reality Environment Implementation to Enable Industrial Robot Programming Competencies within a Cyber-Physical Factory","DOI":"10.1109\/EDUCON54358.2023.10125175","Publication year":2023,"Abstract":"This work aims to present a roadmap towards the Cyber-Physical implementation at Tecnologico de Monterrey. This document describes the approach to conceptualize, design, and implement a mixed virtual environment that fulfills the Paradigm of Industry 4.0. This implementation has the following benefits: 1) The Cyber-Learning-Factory that emulates a real, controlled, Industry 4.0 workplace, 2) It allows the students to design a solution to manufacture and assembly a product from a realistic product specification 3) It allows the students to play similar roles to those found in industry positions. This concept was validated on a project to build a product that integrates autonomous, industrial and collaborative robots, Product and Process Digital Twins, flexible CNC machinery and Additive Manufacturing, Simulation stations and CAD\/CAM\/CAE applications, Horizontal and Vertical Integration (MES and ERP), Augmented-Virtual and Mixed Reality aids and all the IT infrastructure and applications supporting IoT, Cyber Security, Cloud Computing and Big Data &amp; Analytics.","LowLevel":"big data;cad\/cam;cloud computing;computer aided engineering;computer aided instruction;computerised numerical control;cyber-physical systems;digital twins;groupware;industrial robots;internet of things;production engineering computing;production facilities;robot programming","MidLevel":"education;internet of things;smart cities;training;simulation;engineering;collaboration;data;robotics;graphics;developers;manufacturing;networks;other","HighLevel":"industries;technology;use cases;other","Venue":"2023 IEEE Global Engineering Education Conference (EDUCON)","Top20AbsAndTags":[281,122,370,411,361,86,352,159,174,289,137,315,205,447,12,362,388,114,134,2],"Top20Abs":[370,361,281,122,315,2,159,44,388,12,411,129,414,49,137,104,440,447,207,114],"Top20Tags":[122,174,289,362,411,378,281,205,86,129,298,352,159,407,38,258,388,359,17,271],"Citation":"V\u00e1zquez-Hurtado, C., Altamirano-Avila, E., Roman-Flores, A., & Vargas-Martinez, A. (2023). Towards a Mixed Virtual Reality Environment Implementation to Enable Industrial Robot Programming Competencies within a Cyber-Physical Factory. 2023 IEEE Global Engineering Education Conference (EDUCON). https:\/\/doi.org\/10.1109\/educon54358.2023.10125175\n"},{"Title":"Invited Paper: Intelligent Agent Support for Achieving Low Latency in Cloud-Native NextG Mobile Core Networks","DOI":"10.1145\/3571306.3571386","Publication year":2023,"Abstract":"Next-generation mobile core networks are being designed to support a variety of latency sensitive applications based on emerging virtual, augmented or mixed reality technologies. A cloud-native approach for 5G core has been proposed to meet the diverse service requirements of NextG while reducing both CAPEX and OPEX. In this context, microservice architecture for network function virtualization is generally considered to be suitable for meeting NextG service requirements. Despite many advantages, the cloud-native core raises new challenges in the design of NextG systems for latency critical applications. An approach to achieving diverse QoS requirements is proposed in this paper. Specifically, the design is based on an orchestrator called the MEC-Intelligent Agent (MEC-IA) which enables dynamic compute resource distribution and network slice assignment in the core for improved QoS. The MEC-IA framework realizes resource management by intelligently assigning UEs to the access and mobility management function (AMF) while also performing slice provisioning. Simulation results are presented for the proposed MEC-IA framework showing the median control plane delay reduced by a factor of 1.67 &#215;. Further, robustness of the system improves significantly, reflecting a better overall user experience since the percentage connection dropped at 3 &#215; traffic volume reduces by 1.5 &#215; and slices assignment increases by 1.4 &#215; across all slices, even when the traffic arrival is skewed.","LowLevel":"5g mobile communication;cloud computing;mobility management;quality of service;resource allocation;telecommunication traffic;virtualization","MidLevel":"simulation;telecommunication;business planning and management;business performance metrics;input;geospatial;networks","HighLevel":"industries;technology;business;use cases","Venue":"ICDCN2023: Proceedings of the 24th International Conference on Distributed Computing and Networking","Top20AbsAndTags":[346,298,335,404,268,5,257,307,271,426,451,447,448,92,209,340,423,350,466,416],"Top20Abs":[346,307,335,404,298,268,271,447,426,257,448,41,466,340,5,256,301,273,423,451],"Top20Tags":[298,209,404,5,268,257,264,423,271,346,335,426,315,416,428,8,289,266,258,111],"Citation":"Choudhury, S., Das, S., Paul, S., Seskar, I., & Raychaudhuri, D. (2023). Invited Paper: Intelligent Agent Support for Achieving Low Latency in Cloud-Native NextG Mobile Core Networks. Proceedings of the 24th International Conference on Distributed Computing and Networking. https:\/\/doi.org\/10.1145\/3571306.3571386\n"},{"Title":"ML-Aided Dynamic Clustering and Classification of UEs as VBs in D2D Communication Networks","DOI":"10.1109\/WiSPNET57748.2023.10134336","Publication year":2023,"Abstract":"With the next generation of mobile devices and streaming services such as Virtual Reality, Augmented Reality, and Meta being available worldwide, the network's data rate, latency, and connectivity must be improved. Even though 5G provides the user with the required Quality of Service (QoS) in terms of data rate and reduced delays by transmitting signals in the higher frequencies (called New Radio (NR)), it faces a lot of attenuation, leading to a short communication range. However, to meet goals set by utilising 6G requirements, many technological advancements and components must be incorporated into the network. The dense disposition of small cells will help reduce the network traffic in hotspot areas and increase the coverage and spectral efficiency. Nonetheless, the current deployment of 5G Base Stations (BSs) and small cells is static and cannot move around even though they are deployed in hot spot areas, leading to high operational costs. Furthermore, more than these static deployments of base stations can be required in an unpredictable scenario of extreme crowd movement. To overcome these issues, Device to Device (D2D) Communication with the dynamic deployment of Virtual Base stations (VBSs) can be called upon, which can be achieved by using User Equipment (UE) such as phones or laptops to mimic the functions of a Base Station (BS). Therefore, in this paper, a User Equipment based Virtual Base Station (UE-VBS) is studied, which will act as a secondary base station and, in turn, help alleviate the traffic load in the network. Specifically, as one UE cannot relieve the entire network traffic load, the network area is split into different clusters by using an unsupervised Machine Learning (ML) clustering technique(i.e., K-Means with Mean Shift Clustering), and a single UE is selected to act as a VBS for that cluster with the utilisation of supervised ML classification techniques (i.e., Decision Trees, Logistic Regression, Linear Discriminant Analysis And Quadratic Discriminant Analysis, Linear Support Vector). In our work, we utilise the K-means along with mean shift clustering techniques to cluster simulated network areas accurately. Also, we use and compare different classification machine learning techniques to predict\/classify whether user equipment can be employed as a VBS and become UE-VBs. Our simulation study reveals that the Decision Tree algorithm achieves the highest accuracy in categorising the eligible UEs as UE-VBs.","LowLevel":"5g mobile communication;6g mobile communication;cellular radio;decision trees;logistic regression;pattern classification;pattern clustering;quality of service;telecommunication computing;telecommunication traffic;unsupervised learning","MidLevel":"artificial intelligence;telecommunication;computer vision;medical;business performance metrics;input;other","HighLevel":"industries;technology;business;other","Venue":"2023 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET)","Top20AbsAndTags":[268,257,256,416,404,447,466,458,427,402,335,307,448,423,116,209,259,446,273,137],"Top20Abs":[268,256,257,447,404,466,416,458,402,307,427,335,209,116,259,448,486,273,423,137],"Top20Tags":[257,335,428,423,416,115,166,427,346,271,448,277,326,404,282,256,28,209,268,264],"Citation":"Ioannou, I., Nagaradjane, P., Khalifeh, A., Christophorou, C., Vassiliou, V., Sashank, G. V. S., Jain, C., & Pitsillides, A. (2023). ML-Aided Dynamic Clustering and Classification of UEs as VBs in D2D Communication Networks. 2023 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET). https:\/\/doi.org\/10.1109\/wispnet57748.2023.10134336\n"},{"Title":"dVPose: Automated Data Collection and Dataset for 6D Pose Estimation of Robotic Surgical Instruments","DOI":"10.1109\/ISMR57123.2023.10130238","Publication year":2023,"Abstract":"We present dVPose, a realistic multi-modality dataset intended for use in the development and evaluation of real-time single-shot deep-learning based 6D pose estimation algorithms on a head mounted display (HMD). In addition to the dataset, our contribution includes an automated (robotic) data collection platform that integrates an accurate optical tracking system to provide the ground-truth poses. We collected a comprehensive set of data for vision-based 6D pose estimation, including images and poses of the extra-corporeal portions of the instruments and endoscope of a da Vinci surgical robot. The images are collected using the multi-camera rig of the Microsoft HoloLens 2 HMD, mounted on a UR10 robot, and the corresponding poses are collected by optically tracking both the instruments\/endoscope and HMD. The intended application is to enable markerless localization of the HMD with respect to the da Vinci robot, considering that the instruments and endoscope are among the few robotic components that are not covered by sterile drapes. Our dataset features synchronized images from the RGB, depth, and grayscale cameras of the HoloLens 2 device. It is unique in that it provides medically focused images, provides images from a HoloLens 2 device where object tracking is a fundamental task, and provides data from multiple visible-light cameras in addition to depth. Furthermore, the automated data collection platform can be easily adapted to collect images and ground-truth poses of other objects.","LowLevel":"cameras;computer vision;deep learning (artificial intelligence);endoscopes;helmet mounted displays;image colour analysis;image sensors;medical robotics;object tracking;optical tracking;pose estimation;robot vision;surgery","MidLevel":"artificial intelligence;computer vision;medical;robotics;graphics;liberal arts;wearables;sensors;input;other;display technology","HighLevel":"displays;technology;industries;other","Venue":"2023 International Symposium on Medical Robotics (ISMR)","Top20AbsAndTags":[194,48,217,69,180,396,387,364,371,326,381,379,173,435,283,255,12,81,116,300],"Top20Abs":[194,48,217,396,69,180,371,387,364,435,381,326,379,267,12,173,127,201,116,300],"Top20Tags":[326,387,415,255,381,379,396,220,95,81,18,280,160,69,48,115,98,71,143,347],"Citation":"Greene, N., Luo, W., & Kazanzides, P. (2023). dVPose: Automated Data Collection and Dataset for 6D Pose Estimation of Robotic Surgical Instruments. 2023 International Symposium on Medical Robotics (ISMR). https:\/\/doi.org\/10.1109\/ismr57123.2023.10130238\n"},{"Title":"HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies","DOI":"10.1145\/3544549.3585738","Publication year":2023,"Abstract":"We contribute interaction techniques for augmenting mixed reality (MR) visualizations with smartphone proxies. By combining head-mounted displays (HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D charts with precise touch input, haptics feedback, high-resolution 2D graphics, and physical manipulation. Our approach aims to complement both MR and physical visualizations. Most current MR visualizations suffer from unreliable tracking, low visual resolution, and imprecise input. Data physicalizations on the other hand, although allowing for natural physical manipulation, are limited in dynamic and interactive modification. We demonstrate how mobile devices such as smartphones or tablets can serve as physical proxies for MR data interactions, creating dynamic visualizations that support precise manipulation and rich input and output. We describe 6 interaction techniques that leverage the combined physicality, sensing, and output capabilities of HMDs and smartphones, and demonstrate those interactions via a prototype system. Based on an evaluation, we outline opportunities for combining the advantages of both MR and physical charts.","LowLevel":"data visualization;haptic interfaces;helmet mounted displays;human computer interaction;image resolution;medical computing;mobile computing;smartphones;touch screens","MidLevel":"telecommunication;data;medical;graphics;liberal arts;wearables;input;human-computer interaction;display technology","HighLevel":"displays;technology;industries;end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[419,80,348,439,248,397,75,418,157,279,360,162,142,227,4,395,178,440,364,432],"Top20Abs":[80,419,348,439,397,162,360,418,227,365,395,142,157,440,4,178,75,224,412,432],"Top20Tags":[80,279,248,348,419,339,301,42,93,126,364,376,422,180,309,75,167,240,82,221],"Citation":"Chulpongsatorn, N., Willett, W., & Suzuki, R. (2023). HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585738\n"},{"Title":"Large-Scale Dynamic Spectrum Access with IEEE 1900.5.2 Spectrum Consumption Models","DOI":"10.1109\/WCNC55385.2023.10118670","Publication year":2023,"Abstract":"Next generation wireless services and applications, including Augmented Reality, Internet-of-Things, and Smart-Cities, will increasingly rely on Dynamic Spectrum Access (DSA) methods that can manage spectrum resources rapidly and efficiently. Advances in regulatory policies, standardization, networking, and wireless technology are enabling DSA methods on a more granular basis in terms of time, frequency, and geographical location which are key for the operation of 5G and beyond-5G networks. In this context, this paper proposes a novel DSA algorithm that leverages IEEE 1900.5.2 Spectrum Consumption Models (SCMs) which offer a mechanism for RF devices to: (i) \"announce\" or \"declare\" their intention to use the spectrum and their needs in terms of interference protection; and (ii) determine compatibility (i.e., non-interference) with existing devices. In this paper, we develop an SCM-based DSA algorithm for spectrum deconfliction in large-scale wireless network environments and evaluate this algorithm in terms of computation time, efficiency of spectrum allocation, and number of device reconfigurations due to interference using a custom simulation platform. The results demonstrate the benefits of using SCMs and their capabilities to perform fine grained spectrum assignments in dynamic and dense communication environments.","LowLevel":"5g mobile communication;ieee standards;next generation networks;radio networks;radio spectrum management;resource allocation;telecommunication computing","MidLevel":"telecommunication;business planning and management;input;geospatial;standards;networks;other","HighLevel":"business;industries;technology;standards;other","Venue":"2023 IEEE Wireless Communications and Networking Conference (WCNC)","Top20AbsAndTags":[209,447,266,268,404,416,257,256,271,448,159,427,346,264,220,426,406,38,310,385],"Top20Abs":[447,209,416,266,256,268,404,257,406,220,253,325,426,38,136,290,448,129,159,264],"Top20Tags":[257,209,271,404,346,268,428,264,427,317,426,475,231,258,266,416,335,330,298,326],"Citation":"Netalkar, P., Zahabee, A., Caicedo Bastidas, C. E., Kadota, I., Stojadinovic, D., Zussman, G., Seskar, I., & Raychaudhuri, D. (2023). Large-Scale Dynamic Spectrum Access with IEEE 1900.5.2 Spectrum Consumption Models. 2023 IEEE Wireless Communications and Networking Conference (WCNC). https:\/\/doi.org\/10.1109\/wcnc55385.2023.10118670\n"},{"Title":"Partially-Coherent Neural Holography with Fast Spatial Light Modulators","DOI":"10.1117\/12.2655404","Publication year":2023,"Abstract":"Holographic near-eye displays are a promising technology to provide realistic and visually comfortable imagery with improved user experience, but their coherent light sources limit the image quality and restrict the types of patterns that can be generated. A partially-coherent mode, supported by emerging fast spatial light modulators (SLMs), has potential to overcome these limitations. However, these SLMs often have a limited phase control precision, which current computer-generated holography (CGH) techniques are not equipped to handle. In this work, we present a flexible CGH framework for fast, highly-quantized SLMs. This framework is capable of incorporating a wide range of content, including 2D and 2.5D RGBD images, 3D focal stacks, and 4D light fields, and we demonstrate its effectiveness through state-of-the-art simulation and experimental results. &copy; 2023 SPIE.","LowLevel":"computer-generated holography;e-learning;image enhancement;light modulation;light modulators;light sources;machine learning","MidLevel":"education;artificial intelligence;computer vision;medical;graphics;input;display technology","HighLevel":"displays;technology;industries","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[278,201,235,431,332,149,446,290,441,74,317,344,277,158,228,265,141,169,422,437],"Top20Abs":[278,201,235,332,277,300,441,295,431,317,265,344,290,437,228,74,123,422,446,316],"Top20Tags":[278,446,149,235,201,459,341,438,332,74,169,307,290,223,141,369,161,443,431,31],"Citation":"Choi, S., Gopakumar, M., Peng, Y., Kim, J., O\u2019Toole, M., & Wetzstein, G. (2023). Partially coherent neural holography with fast spatial light modulators. Emerging Digital Micromirror Device Based Systems and Applications XV. https:\/\/doi.org\/10.1117\/12.2655404\n"},{"Title":"Development of a mixed reality method for underground pipelines in digital mechanics experiments","DOI":"10.1016\/j.tust.2022.104833","Publication year":2023,"Abstract":"The underground pipeline network (UPN) is an essential underground structure and infrastructure. Its full-cycle digital twin system is an important part of the smart city. However, traditional information transmission between humans and computers lacks understanding and interaction in the digital twin experiments. From the perspective of perception, a space interaction system based on mixed reality (MR) technology is used to solve the problem of interaction for traditional mechanical experiments. Firstly, during the design phase, Building Information Modeling (BIM) is used to integrate models and information storage, including physical geometry data and material information. Secondly, the analytical algorithm is constructed for sensors by data filtering and material mechanics theory; based on sensors communication, C#\/C++, and Socket to develop human-computer interactive methods through Game Engines and Mixed Reality Toolkits (MRTK). Finally, based on comparing the mechanical parameters, the testing of digital pipeline experiments by spatial anchors. Twenty-four experimenters are selected to test this method. The advantages and prospects of spatial interaction with MR are summarized through qualitative and quantitative analysis. This study realizes the data perception of the UPN and maps it to the real environment, which can provide a technical reference for engineering digital experiments. All rights reserved Elsevier.","LowLevel":"building information modelling;construction industry;design engineering;digital twins;interactive systems;pipelines;production engineering computing;smart cities","MidLevel":"education;oil and gas;smart cities;engineering;construction;liberal arts;input;manufacturing;human-computer interaction;other","HighLevel":"end users and user experience;use cases;industries;technology;other","Venue":"Tunn. Undergr. Space Technol. Inc. Trenchless Technol. Res. (Netherlands)","Top20AbsAndTags":[395,122,181,486,389,439,224,146,418,287,86,136,411,419,35,261,407,374,7,365],"Top20Abs":[395,486,224,122,287,439,181,389,7,136,419,424,365,418,374,467,261,153,376,446],"Top20Tags":[181,146,389,395,86,407,122,411,310,35,370,388,273,418,362,205,377,45,210,58],"Citation":"Li, W., Wang, Y., Yang, H., Ye, Z., Li, P., Aron Liu, Y., & Wang, L. (2023). Development of a mixed reality method for underground pipelines in digital mechanics experiments. Tunnelling and Underground Space Technology, 132, 104833. https:\/\/doi.org\/10.1016\/j.tust.2022.104833\n"},{"Title":"Industrial Metaverse: Supporting remote maintenance with avatars and digital twins in collaborative XR environments","DOI":"10.1145\/3544549.3585835","Publication year":2023,"Abstract":"We present a 5G mixed reality toolbox that supports hands-free remote assistance in industrial settings. It provides mixed reality and virtual reality views for on-site and office workers linked via a shared digital space. Working on actual machines in a real production line, our system uses the actual CAD-data of those machines to provide for a realistic prototyping-environment. We focus on data-scarcity with cloud-services to protect intellectual property, while embracing the possibilities offered by new technology, such as remote rendering over wireless networks. The presented prototype exhibits several key characteristics of an industrial metaverse application.","LowLevel":"avatars;cad;cloud computing;digital twins;industrial property;production engineering computing;rendering","MidLevel":"smart cities;engineering;presence;graphics;manufacturing;human-computer interaction;networks;other","HighLevel":"end users and user experience;use cases;industries;technology;other","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[429,366,249,377,157,439,12,239,329,418,412,174,355,178,440,251,215,403,45,126],"Top20Abs":[366,157,249,355,12,215,377,303,251,440,304,447,239,93,268,345,178,429,174,395],"Top20Tags":[377,407,122,388,439,429,418,370,412,331,362,181,249,159,143,45,205,289,327,12],"Citation":"Oppermann, L., Buchholz, F., & Uzun, Y. (2023). Industrial Metaverse. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585835\n"},{"Title":"Reach Prediction using Finger Motion Dynamics","DOI":"10.1145\/3544549.3585773","Publication year":2023,"Abstract":"The ability to predict the object the user intends to grasp or to recognize the one she is already holding offers essential contextual information and may help to leverage the effects of point-to-point latency in interactive environments. This paper investigates the feasibility and accuracy of recognizing un-instrumented objects based on hand kinematics during reach-to-grasp and transport actions. In a data collection study, we recorded the hand motions of 16 participants while reaching out to grasp and then moving real and synthetic objects. Our results demonstrate that even a simple LSTM network can predict the time point at which the user grasps an object with 23 ms precision and the current distance to it with a precision better than 1 cm. The target's size can be determined in advance with an accuracy better than 97%. Our results have implications for designing adaptive and fine-grained interactive user interfaces in ubiquitous and mixed-reality environments.","LowLevel":"data acquisition;human computer interaction;interactive systems;object recognition;recurrent neural nets;ubiquitous computing;user interfaces","MidLevel":"education;artificial intelligence;human factors;computer vision;input;human-computer interaction","HighLevel":"industries;technology;end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[487,415,400,198,98,340,318,451,385,433,105,153,53,110,221,321,440,382,72,419],"Top20Abs":[487,415,198,400,98,385,153,433,105,340,318,194,221,451,440,19,120,143,301,261],"Top20Tags":[415,110,400,58,339,239,381,210,175,82,242,144,421,53,75,36,177,231,55,221],"Citation":"Valkov, D., Kockwelp, P., Daiber, F., & Kr\u00fcger, A. (2023). Reach Prediction using Finger Motion Dynamics. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585773\n"},{"Title":"VanityX: An Agile 3D Rendering Platform Supporting Mixed Reality","DOI":"10.3390\/app13095468","Publication year":2023,"Abstract":"VanityX is a prototype, low-level, real-time 3D rendering and computing platform. Unlike most XR solutions, which integrate several commercial and\/or open-source products, such as game engines, XR libraries, runtime, and services, VanityX is a platform ready to adapt to any business domain including anthropology and medicine. The design, architecture, and implementation are presented, which are based on CPU and GPU asymmetric multiprocessing with explicit synchronization and collaboration of parallel tasks and a predictable transfer of pipeline resources between processors. The VanityX API is based on DirectX 12 and native programming languages C++20 and HLSL 6, which, in conjunction with explicit parallel processing, the asynchronous loading and explicit managing of graphic resources, and effective algorithms, results in great performance and resource utilization close to metal. Surface-based rendering, direct volume rendering (DVR), and mixed reality (MR) on the HoloLens 2 immersive headset are currently supported. Our MR applications are directly compiled and deployed to HoloLens 2 allowing for better programming experiences and software engineering practices such as testing, debugging, and profiling. The VanityX server provides various computational and rendering services to its clients running on HoloLens 2. The use and test cases are in many business domains including anthropology and medicine. Our future research challenges will primarily, via the MetaverseMed project, focus on opening new opportunities for implementing innovative MR-based scenarios in medical procedures, especially in education, diagnostics, and surgical operations.","LowLevel":"application program interfaces;multiprocessing systems;parallel processing;rendering;software engineering","MidLevel":"education;data;graphics;developers;semiconductors","HighLevel":"industries;technology","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[327,227,395,126,439,464,397,360,411,410,331,194,246,219,408,257,185,424,301,41],"Top20Abs":[327,395,439,227,360,397,464,194,126,408,246,257,185,356,41,424,412,335,410,411],"Top20Tags":[392,327,219,410,331,181,239,297,300,148,167,170,50,126,433,411,357,130,150,321],"Citation":"Zoraja, I., Bonkovic, M., Papic, V., & Sunderam, V. (2023). VanityX: An Agile 3D Rendering Platform Supporting Mixed Reality. Applied Sciences, 13(9), 5468. https:\/\/doi.org\/10.3390\/app13095468\n"},{"Title":"Spelland: Situated Language Learning with a Mixed-Reality Spelling Game through Everyday Objects","DOI":"10.1145\/3544549.3583830","Publication year":2023,"Abstract":"This work explores the use of mixed-reality (MR) technology to enable situated language learning using everyday objects in the environment around the learners. The learning method is based on Presentation, Practice, and Production (PPP), which cultivates the habit of independent learning through repetition, practice, and demonstration. In our game design, the learners first interact with real-world objects via MR, and the objects' spelling and their pronunciation will appear (Presentation), the learners repeat the pronunciation (Practice) to collect the letters of this objects, and finally the learner use the collected letters to spell out the target words (Production), which then transform into interactive 3D objects. We designed the learning experience and content tools using gestural UI, voice input, and object-to-word engine. Children in the preliminary user study found the game to be immersive, helpful in learning the spelling of the everyday objects and the target words, and additionally showed increased interests in learning about other nearby objects after playing the game.","LowLevel":"computer aided instruction;educational courses;human computer interaction;linguistics;serious games;user interfaces","MidLevel":"education;training;simulation;input;human-computer interaction","HighLevel":"industries;technology;use cases;end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[313,163,409,114,79,116,487,57,83,415,439,164,105,418,365,246,363,357,244,150],"Top20Abs":[313,163,116,114,79,487,415,164,83,105,409,418,115,87,57,442,292,244,363,39],"Top20Tags":[145,239,134,113,439,409,440,236,7,204,150,414,114,33,313,405,82,407,154,244],"Citation":"Hsu, C., Chen, Y., Liu, Y.-J., Chang, Y.-C., & Lee, M.-J. (2023). Spelland: Situated Language Learning with a Mixed-Reality Spelling Game through Everyday Objects. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583830\n"},{"Title":"BirdViewAR: Surroundings-aware Remote Drone Piloting Using an Augmented Third-person Perspective","DOI":"10.1145\/3544548.3580681","Publication year":2023,"Abstract":"We propose BirdViewAR, a surroundings-aware remote drone-operation system that provides significant spatial awareness to pilots through an augmented third-person view (TPV) from an autopiloted secondary follower drone. The follower drone responds to the main drone's motions and directions using our optimization-based autopilot, allowing the pilots to clearly observe the main drone and its imminent destination without extra input. To improve their understanding of the spatial relationships between the main drone and its surroundings, the TPV is visually augmented with AR-overlay graphics, where the main drone's spatial statuses are highlighted: its heading, altitude, ground position, camera field-of-view (FOV), and proximity areas. We discuss BirdViewAR's design and implement its proof-of-concept prototype using programmable drones. Finally, we conduct a preliminary outdoor user study and find that BirdViewAR effectively increased spatial awareness and piloting performance.","LowLevel":"autonomous aerial vehicles;cameras;control engineering computing;mobile robots;remotely operated vehicles;robot vision","MidLevel":"engineering;automotive;computer vision;robotics;aviation and aerospace;input","HighLevel":"industries;technology","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[399,165,224,374,50,10,159,102,322,26,303,42,319,425,243,142,24,366,121,100],"Top20Abs":[399,165,224,50,374,10,159,102,24,303,42,142,100,349,189,329,243,26,162,166],"Top20Tags":[399,374,396,322,371,420,381,60,379,220,425,347,120,401,366,19,174,407,98,269],"Citation":"Inoue, M., Takashima, K., Fujita, K., & Kitamura, Y. (2023). BirdViewAR: Surroundings-aware Remote Drone Piloting Using an Augmented Third-person Perspective. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580681\n"},{"Title":"A Mixed Reality application to support the design of custom prostheses","DOI":"10.1016\/j.procs.2022.12.300","Publication year":2023,"Abstract":"The definition of an innovative category of implantable devices, characterized not only by a fully customization but also by improved osteoconductive and functionalized bioactive surfaces, needs to be supported by a systematic approach. In the present work, a new human-machine interface based on the Mixed Reality (MR) is proposed and focused on the implantation of a fully customized prosthesis. By means of an informal and exploratory focus group (i.e., without using a structured questionnaire), the limitations belonging to the current procedure were highlighted and a first list of user needs was subsequently defined. The MR interface was then considered to be the most suitable solution to match the gathered requirements. However, the doctors proposed also to develop a desktop interface for a finer and easier manipulation of 3D models. The proposed MR application offers several advantages from the possibility to display 3D anatomical structures and 3D models of custom prostheses in an immersive environment to the optimized communication and data exchange among the players (medical staff, doctors and engineers). A mock-up of the MR applications is presented in this work to show the results of the design stage, before the deployment of the application. All rights reserved Elsevier.","LowLevel":"bone;human computer interaction;medical computing;prosthetics;solid modelling;user interfaces","MidLevel":"manufacturing;human-computer interaction;medical","HighLevel":"end users and user experience;industries","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[439,419,418,227,395,365,424,340,360,138,45,246,403,323,112,205,64,303,288,294],"Top20Abs":[439,419,227,395,418,424,365,360,340,246,303,403,288,412,26,64,45,138,208,356],"Top20Tags":[133,294,186,210,417,91,112,301,314,331,122,382,72,242,37,436,42,376,418,36],"Citation":"Gattullo, M., Piccininni, A., Evangelista, A., Guglielmi, P., Boccaccio, A., Cusanno, A., Uva, A. E., & Palumbo, G. (2023). A Mixed Reality application to support the design of custom prostheses. Procedia Computer Science, 217, 1018\u20131027. https:\/\/doi.org\/10.1016\/j.procs.2022.12.300\n"},{"Title":"Development of a distributed MR-IoT method for operations and maintenance of underground pipeline network","DOI":"10.1016\/j.tust.2022.104935","Publication year":2023,"Abstract":"The underground pipeline network (UPN) is an essential infrastructure and plays an irreplaceable role in national defense and urban activities. The complexity of structural environment and management makes its operation and maintenance difficult. To solve this problem, a distributed mixed reality (MR) and internet of things (IoT) system is developed through game thinking. Firstly, digital models are created based on design drawings and real-world environments, and then an MR system for the UPN is built by the game engine and the OpenXR platform. Secondly, an IoT cloud platform is built to connect with the MR system based on the API sets and cloud services; the data communication between sensors and MR devices is linked with the Socket method, and the data filtering model is constructed by the Kalman algorithm to realize the information exchange between the field workers and the backend managers. Finally, the National Center for Materials Service Safety at the University of Science and Technology Beijing (NCMS_USTB) is used as the experimental site to test this system, and its underground sewage and rainwater pipeline network are used to simulate the key problems in the operation and maintenance. The effect of the application shows that there is potential technical complementarity between the MR and IoT, and the distributed MR-IoT approach can be used as a new technical reference for the operation and maintenance of the UPN. All rights reserved Elsevier.","LowLevel":"application program interfaces;cloud computing;internet of things;pipelines;structural engineering computing","MidLevel":"internet of things;oil and gas;engineering;developers;networks","HighLevel":"industries;technology","Venue":"Tunn. Undergr. Space Technol. Inc. Trenchless Technol. Res. (Netherlands)","Top20AbsAndTags":[412,129,360,439,418,227,397,315,266,403,246,175,424,64,447,419,35,345,365,261],"Top20Abs":[412,360,129,439,397,418,227,315,266,246,403,447,424,419,389,345,35,365,261,131],"Top20Tags":[298,258,412,129,271,123,289,38,220,110,286,175,121,378,266,146,359,31,181,429],"Citation":"Li, W., Ye, Z., Wang, Y., Yang, H., Yang, S., Gong, Z., & Wang, L. (2023). Development of a distributed MR-IoT method for operations and maintenance of underground pipeline network. Tunnelling and Underground Space Technology, 133, 104935. https:\/\/doi.org\/10.1016\/j.tust.2022.104935\n"},{"Title":"AEDLE: Designing Drama Therapy Interface for Improving Pragmatic Language Skills of Children with Autism Spectrum Disorder Using AR","DOI":"10.1145\/3544549.3585809","Publication year":2023,"Abstract":"This research proposes AEDLE, a new interface combining AR with drama therapy - an approved method of improving pragmatic language skills - to offer effective, universal, and accessible language therapy for children with Autism Spectrum Disorder (ASD). People with ASD commonly have a disability in pragmatic language and experience difficulty speaking. However, although therapy in childhood is necessary to prevent long-term social isolation due to such constraints, the limited number of therapists forbids doing so. Technology-based therapy can be a solution, but studies on utilizing digital therapy to improve pragmatic language are still insufficient. We conducted a preliminary user study with an ASD child and a therapist to investigate how the child with ASD reacts to drama therapy using AEDLE. We observed that our ASD child actively participated in AEDLE-mediated drama therapy, used our insights to recommend design suggestions for AR-based drama therapy, and explored various ways to utilize AEDLE.","LowLevel":"handicapped aids;human computer interaction;medical disorders;natural language processing;patient treatment","MidLevel":"artificial intelligence;human-computer interaction;data;medical","HighLevel":"end users and user experience;technology;industries","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[222,210,443,253,115,71,3,91,180,93,34,83,136,140,384,45,188,51,112,400],"Top20Abs":[222,443,210,115,71,3,253,91,83,180,34,93,136,140,384,188,51,45,284,15],"Top20Tags":[313,210,253,93,214,63,367,351,58,64,108,339,415,286,376,133,455,314,397,180],"Citation":"Park, J., Bae, G., Park, J., Park, S. K., Kim, Y. S., & Lee, S. (2023). AEDLE: Designing Drama Therapy Interface for Improving Pragmatic Language Skills of Children with Autism Spectrum Disorder Using AR. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585809\n"},{"Title":"Enlarged mid-air image display based on slim DOE waveguide","DOI":"10.1117\/12.2648892","Publication year":2023,"Abstract":"Currently, mid-air display (MAD) technology is of a great interest to practitioners. Potential application in consumer products with large aperture \"floating\" image displays, like TV, monitor, ATM, vending machine, home appliance, etc., and contactless user interface for remote control increase their attractiveness. In order to obtain enlarged mid-air image size at maintaining large horizontal field of view (FoV) and high light display efficiency, the following challenges are to be solved: developing high fill-factor diffractive optical elements (DOE) architecture with optimal size of out-coupling aperture, and designing custom-made projection optics with specified exit pupil matched to in-coupling DOE. As a possible solution to abovementioned problems, the authors propose a MAD based on commercially available projector source, custom-made projection optics and designed corner DOE waveguide architecture with focusing Fresnel lens. The mid-air image is formed at the back focal plane of the Fresnel lens, between the viewer and the display. For mid-air image with 5-inch diagonal and 32&deg; horizontal FoV, we take waveguide out-coupling aperture of 245 x 145 mm2 and Fresnel lens with back focal length of 220 mm, and obtain image brightness ~1000 cd\/m2 due to custom projection optics. Basic contactless user interaction was also implemented. &copy; 2023 SPIE.","LowLevel":"consumer products;domestic appliances;optical instrument lenses;remote control;waveguides","MidLevel":"collaboration;input;optics;consumer products;display technology","HighLevel":"industries;technology;displays;use cases","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[422,332,437,66,444,344,270,141,153,169,324,74,225,364,228,317,278,76,201,441],"Top20Abs":[422,437,332,66,444,270,344,141,324,248,364,76,74,153,278,225,106,228,430,169],"Top20Tags":[169,223,214,441,228,344,332,201,437,158,291,74,430,481,361,0,235,317,470,440],"Citation":"Danilova, S., Malyshev, I., Shtykov, S., Muravyev, N., Aspidov, A., Keum, C., Bae, J., & Lee, S. (2023). Enlarged mid-air image display based on slim DOE waveguide. Advances in Display Technologies XIII. https:\/\/doi.org\/10.1117\/12.2648892\n"},{"Title":"Blended Collaboration: Communication and Cooperation Between Two Users Across the Reality-Virtuality Continuum","DOI":"10.1145\/3544549.3585881","Publication year":2023,"Abstract":"Mixed reality (MR) technologies provide enormous potential for collaboration between multiple users across the Reality-Virtuality continuum. We evaluate communication in a MR-based two-user collaboration task, in which the users have to move an object through an obstacle without collision. We used a blended reality environment, in which one user is immersed in virtual reality, whereas the other uses mobile augmented reality. Both users have different abilities and information and mutually depend on each other for successful completion of the task. Communication consensus can either be achieved by using speech, visual widgets, or a combination of both. The results indicate that speech plays a fundamental role. The usage of widgets served as an extension rather than a replacement of language. However, the combination of speech and widgets improved the clearness of communication with less miscommunication. These results provide important indications about how to design blended collaboration across the Reality-Virtuality continuum.","LowLevel":"groupware;mobile computing","MidLevel":"telecommunication;collaboration","HighLevel":"industries;use cases","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[304,211,303,388,419,267,279,219,195,352,240,360,246,439,368,365,208,440,395,418],"Top20Abs":[304,211,419,388,352,303,360,439,195,246,440,395,397,208,418,368,365,313,410,227],"Top20Tags":[314,279,337,36,303,211,29,17,304,267,239,142,104,131,199,13,240,329,193,125],"Citation":"Kruse, L., Wittig, J., Finnern, S., Gundlach, M., Iserlohe, N., Ariza, O., & Steinicke, F. (2023). Blended Collaboration: Communication and Cooperation Between Two Users Across the Reality-Virtuality Continuum. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585881\n"},{"Title":"Real-Time 3D Neuroendoscopic Guidance Using SLAM: First Clinical Studies","DOI":"10.1117\/12.2654595","Publication year":2023,"Abstract":"Purpose: Neurosurgical techniques often require accurate targeting of deep-brain structures even in the presence of deformation due intervention and egress of cerebrospinal fluid (CSF) during surgical access. Prior work reported simultaneous localization and mapping (SLAM) methods for endoscopic guidance using 3D reconstruction. In this work, methods for correcting the geometric distortion of a neuroendoscope are reported in a form that have been translated intraoperative use in first clinical studies. Furthermore, SLAM methods are evaluated in first clinical studies for real-time 3D endoscopic navigation with near real-time registration in the presence of deep-brain tissue deformation. Methods: A custom calibration jig with swivel mounts was designed and manufactured for neuroendoscope calibration in the operating room. The process is potentially suitable to intraoperative use while maintaining sterility of the endoscope, although the current calibration system was used in the operating room (OR) immediately following the case for offline analysis. A 6&times;7 checkerboard pattern was used to obtain corner locations for calibration, and the method was evaluated in terms of reprojection error (RPE). Neuroendoscopic video was acquired under an IRB-approved clinical study, demonstrating rich vascular features and other structures on the interior walls of the lateral ventricles for 3D point-cloud reconstruction. Geometric accuracy was evaluated in terms of projected error (PE) on a ground truth surface defined from MR or cone-beam CT (CBCT) images. Results: Intraoperative neuroendoscope calibration was achieved with sub-pixel [0.61 &plusmn; 0.20 px] error. The calibration yielded a focal length of 816.42 px and 822.71 px in X and Y directions respectively, along with radial distortion coefficients of -0.432 (first order term [k1]) and 0.158 (second order term [k2]). The 3D reconstruction was performed successfully with a PE of 0.23 &plusmn; 0.15 mm compared to the ground truth surface. Conclusions: The system for neuroendoscopic guidance based on SLAM 3D point-cloud reconstruction provided a promising platform for the development of 3D neuroendoscopy. The studies reported in this work presented an important means of neuroendoscope calibration in the OR and provided preliminary evidence for accurate 3D video reconstruction in first clinical studies. Future work aims to further extend the clinical evaluation and improve reconstruction accuracy using ventricular shape priors. &copy; 2023 SPIE.","LowLevel":"calibration;cerebrospinal fluid;computer vision;computerized tomography;endoscopy;errors;image reconstruction;medical imaging;neurosurgery;stereo image processing;three dimensional computer graphics;transplantation","MidLevel":"human factors;computer vision;data;medical;graphics;construction;sensors;other","HighLevel":"industries;technology;other;end users and user experience","Venue":"Progr. Biomed. Opt. Imaging Proc. SPIE","Top20AbsAndTags":[433,285,340,43,191,323,467,2,189,301,321,186,318,173,207,451,470,153,431,356],"Top20Abs":[433,285,340,43,191,189,318,301,321,323,467,2,186,451,356,350,136,397,152,270],"Top20Tags":[138,464,433,43,453,207,432,323,170,0,470,480,285,431,307,183,341,463,457,446],"Citation":"Vagdargi, P., Uneri, A., Jones, C. K., Zhang, X., Wu, P., Han, R., Sisniega, A., Lee, J., Helm, P. A., Luciano, M. G., Anderson, W. S., Hager, G., & Siewerdsen, J. H. (2023). Real-time 3D neuroendoscopic guidance using SLAM: first clinical studies. Medical Imaging 2023: Image-Guided Procedures, Robotic Interventions, and Modeling. https:\/\/doi.org\/10.1117\/12.2654595\n"},{"Title":"Beam tracking and image steering by Texas Instruments Phase Light Modulator based on camera input for lidar and AR applications","DOI":"10.1117\/12.2651414","Publication year":2023,"Abstract":"Micro Electro Mechanical System (MEMS) spatial light modulators enables adaptive and fast beam and image steering. For lidar applications, Texas Instruments Phase Light Modulator (TI-PLM) is paired with real-time calculation and display of Computer Generated Holograms (CGH) by CUDA-OpenGL interoperability assisted by YOLOv4-tiny network model for object detection and recognition. The real-time object recognition, CGH calculation, and display framework replaces conventional raster scanning with camera-input based and foveated beam steering while having a beam scan rate beyond the frame rate of TI-PLM. For Augmented Reality (AR) application, the same framework is used for image steering based on gaze information of eye. With Texas Instruments Digital Micromirror Device (TI-DMD), image is steered into a part of field of view by following movement of eye. The diffractive image steering enabled by TI-DMD increases FOV while not sacrificing resolution of the image displayed. &copy; 2023 SPIE.","LowLevel":"application programming interfaces;cameras;light modulation;object detection;object recognition;optical radar;three-dimensional displays","MidLevel":"human factors;optics;computer vision;developers;input;geospatial;display technology","HighLevel":"displays;technology;end users and user experience","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[158,413,428,278,317,74,235,290,277,141,324,212,332,76,438,487,169,228,162,393],"Top20Abs":[428,413,278,277,76,212,487,74,141,162,425,422,127,290,235,20,265,157,259,393],"Top20Tags":[158,235,317,169,324,438,74,223,413,290,141,332,228,437,441,433,138,393,149,0],"Citation":"Deng, X., Tang, C.-I., & Takashima, Y. (2023). Beam tracking and image steering by Texas Instruments phase light modulator based on camera input for lidar and AR applications. Emerging Digital Micromirror Device Based Systems and Applications XV. https:\/\/doi.org\/10.1117\/12.2651414\n"},{"Title":"Demo Abstract: Edge-based Augmented Reality Guidance System for Retinal Laser Therapy via Feature Matching","DOI":"10.1145\/3583120.3589814","Publication year":2023,"Abstract":"In ophthalmology, retinal laser therapy is a treatment for retinopathy that requires the use of magnifying lens to treat damaged regions of retinal landmarks, hence creating challenges of inverted magnified images and requiring prolonged training. Augmented Reality (AR) can benefit clinicians during retinal laser therapy by guiding them with retinal landmark holograms and contextual information. Though recent developments in AR magnification show that a direct overlay of the magnified scenes can be achieved, retinal laser therapy requires high precision and visual acuity while maintaining the visual perception of the rest of the environment. Therefore, we demonstrate an AR-based selective magnification system that provides contextual and visualization-based guidance to clinicians. An edge-computing architecture is developed for detecting and matching the feature points between the magnified image and color fundus image of the retina to identify the magnified region of retinal landmarks. We showcase how our AR guidance system can assist clinicians during retinal laser therapy.","LowLevel":"biomedical optical imaging;diseases;eye;feature extraction;laser applications in medicine;medical image processing;visual perception","MidLevel":"computer vision;data;medical;chemical;developers;input;other","HighLevel":"industries;technology;other","Venue":"IPSN23: The 22nd International Conference on Information Processing in Sensor Networks","Top20AbsAndTags":[107,138,394,285,2,210,318,98,234,132,451,437,69,76,178,461,74,189,20,99],"Top20Abs":[107,138,394,285,2,210,437,318,74,323,98,189,451,91,223,20,438,99,43,132],"Top20Tags":[107,186,356,294,98,326,234,69,143,387,381,76,72,132,259,318,99,260,178,425],"Citation":"Eom, S., Janamsetty, R., Hadziahmetovic, M., Pajic, M., & Gorlatova, M. (2023). Demo Abstract: Edge-based Augmented Reality Guidance System for Retinal Laser Therapy via Feature Matching. The 22nd International Conference on Information Processing in Sensor Networks. https:\/\/doi.org\/10.1145\/3583120.3589814\n"},{"Title":"Advanced visualization of ergonomic assessment data through industrial Augmented Reality","DOI":"10.1016\/j.procs.2022.12.346","Publication year":2023,"Abstract":"The industrial transition to the 4.0 paradigm defines new scenarios in which the operator plays a central role within the industrial ecosystem. Thanks to the enabling technologies of Industry 4.0, it is possible to effectively improve operators' working conditions by applying the Human-Centered approach. Nowadays, one of the main challenges is to reduce work-related musculoskeletal disorders resulting from ergonomically incorrect working conditions in order to prevent the occurrence of occupational diseases. To this end, we developed a software tool that leverages a low-cost D-RGB camera (Kinect v2) to track the human body and an Augmented Reality (AR) visualization system based on Microsoft HoloLens 2. The tool assesses postural ergonomic risk in real-time according to the Rapid Upper Limb Assessment (RULA) metric. The proposed AR application allows a three-dimensional visualization of postures, which can be observed directly superimposed on the operator's body in the real scene. This approach aims to optimize the understanding of postures by creating a link between real information (operator's body) and virtual information (virtual skeleton, RULA score, and angles) by providing a simple and immediate user interface for ergonomists. All rights reserved Elsevier.","LowLevel":"cameras;diseases;employee welfare;ergonomics;occupational health;occupational safety;production engineering computing;software tools;user interfaces","MidLevel":"human factors;human resources;engineering;medical;inspection, safety and quality;developers;input;manufacturing;human-computer interaction","HighLevel":"end users and user experience;use cases;business;industries;technology","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[179,205,174,122,12,137,261,54,45,399,53,439,159,311,239,94,66,452,373,436],"Top20Abs":[179,205,174,261,54,12,137,122,178,399,53,311,135,4,239,159,157,316,373,452],"Top20Tags":[94,205,179,388,68,122,54,45,370,242,407,360,66,419,227,273,443,174,12,348],"Citation":"Evangelista, A., Manghisi, V. M., Romano, S., De Giglio, V., Cipriani, L., & Uva, A. E. (2023). Advanced visualization of ergonomic assessment data through industrial Augmented Reality. Procedia Computer Science, 217, 1470\u20131478. https:\/\/doi.org\/10.1016\/j.procs.2022.12.346\n"},{"Title":"\"Periodic Fable Discovery\" Using Tangible Interactions and Augmented Reality to Promote STEM Subjects","DOI":"10.1145\/3569009.3572804","Publication year":2023,"Abstract":"This pictorial presents Periodic Fable (PF), an educational game's design and graphic interface that promotes a constructivist approach to engage young children with Science, Technology, Engineering, and Math (STEM) subjects. The game presents children with scientific content supported with an exploratory activity using physical cubes manipulable through Tangible Interaction and Augmented Reality. The game's objective is to entertain children while engaging them with the basics of chemistry and the Periodic Table. We reflect upon the combination of these immersive technologies, game-play mechanics, and aesthetics geared towards conveying accurate scientific information through a ludic and entertaining approach. The quantitative and qualitative results of a study with 20 children, showed significant positive results in the participants' learning outcomes and engagement, thereby encouraging us to continue evaluating our design system as a tool that can promote STEM Education.","LowLevel":"computer aided instruction;computer games;entertainment;stem","MidLevel":"education;liberal arts;training","HighLevel":"industries;use cases","Venue":"TEI '23: Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction","Top20AbsAndTags":[115,164,51,140,111,180,236,136,196,147,113,400,79,125,204,292,287,284,34,188],"Top20Abs":[115,180,140,136,111,164,51,292,188,400,443,196,147,34,113,284,314,125,236,79],"Top20Tags":[204,354,164,147,51,33,236,196,145,410,414,405,287,93,244,59,17,154,9,114],"Citation":"Camara Olim, S. M., Nisi, V., & Rubegni, E. (2023). \u201cPeriodic Fable Discovery\u201d Using Tangible Interactions and Augmented Reality to Promote STEM Subjects. Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction. https:\/\/doi.org\/10.1145\/3569009.3572804\n"},{"Title":"Augmented Reality For Education Based On Markerless Dynamic Rendering","DOI":"10.1109\/ICNWC57852.2023.10127337","Publication year":2023,"Abstract":"Augmented Reality (AR) technology has the potential to revolutionize education by providing a new way for students to visualize and interact with complex concepts. In this project, a system is proposed to develop an AR smartphone application that allows students to visualize objects and scenarios that the teacher is teaching in real-time. The application will employ the smartphone's camera and sensors to materialize a user-friendly and easy-to-use dynamic AR experience, with the teacher allowing the students to simply access their smartphone to project 3D models of objects or scenarios onto a flat surface. Students will be able to view these models from any angle and interact with them in a variety of ways, such as by rotating them or zooming in on specific details. In addition to enhancing student's understanding of the material being taught, the AR application will also provide an engaging and immersive learning experience. The distinguishing factor is the storage of the 3D assets on the cloud that will equip the educator with the option of pre-planning and customizing their entire lesson as well as storing any number of models. This can help to increase student engagement and motivation, leading to better retention of the material being taught. Overall, the proposed AR smartphone application has the potential to significantly improve the way students learn and understand complex concepts, making education more effective and enjoyable for all.","LowLevel":"computer aided instruction;mobile computing;smartphones;teaching","MidLevel":"education;liberal arts;telecommunication;training","HighLevel":"industries;use cases","Venue":"2023 International Conference on Networking and Communications (ICNWC)","Top20AbsAndTags":[134,39,114,56,163,87,124,306,272,342,88,103,204,369,320,136,51,414,188,112],"Top20Abs":[134,88,39,163,306,87,124,114,272,369,56,188,342,204,46,62,103,320,136,184],"Top20Tags":[93,51,164,89,103,77,56,410,376,222,3,33,147,115,129,320,204,339,105,27],"Citation":"Rakshit, S., Iyer, A., Raj.C, S. R., Elizabeth.D, S., & Vaidyanathan, A. (2023). Augmented Reality For Education Based On Markerless Dynamic Rendering. 2023 International Conference on Networking and Communications (ICNWC). https:\/\/doi.org\/10.1109\/icnwc57852.2023.10127337\n"},{"Title":"Integrated Registration and Occlusion Handling Based on Deep Learning for Augmented-Reality-Assisted Assembly Instruction","DOI":"10.1109\/TII.2022.3189428","Publication year":2023,"Abstract":"Augmented reality (AR) can convert complex work instructions into virtual-reality fusion contents for assembly guidance. In the past, AR registration and occlusion were usually implemented separately, with low robustness and poor timeliness. This article proposes a novel deep learning scheme, named AR-CenterNet, to integrate AR registration and occlusion handling. The proposed method mainly includes two stages, i.e., the neural network prediction stage and the AR processing stage. In the first stage, AR-CenterNet is designed for keypoint detection and depth map prediction. In the second stage, the pose matrix of the physical camera is solved with the predicted keypoints and the depth map of the virtual scene is compared with the predicted depth map for occlusion handling. The experiments demonstrate that our method is robust against different conditions for assisted assembly. This article can provide a new solution method for AR virtual-reality fusion based on monocular images.","LowLevel":"cameras;deep learning (artificial intelligence);feature extraction;image fusion;image registration;object detection;pose estimation","MidLevel":"artificial intelligence;computer vision;medical;graphics;liberal arts;chemical;input;other","HighLevel":"industries;technology;other","Venue":"IEEE Trans. Ind. Inform. (USA)","Top20AbsAndTags":[379,430,387,321,48,370,132,381,98,137,358,163,300,255,402,453,138,425,116,49],"Top20Abs":[430,379,321,387,370,453,48,402,137,49,163,300,170,467,98,138,483,358,381,123],"Top20Tags":[132,387,98,379,381,255,425,160,415,143,95,48,18,99,71,115,79,172,116,76],"Citation":"Li, W., Wang, J., Liu, M., Zhao, S., & Ding, X. (2023). Integrated Registration and Occlusion Handling Based on Deep Learning for Augmented-Reality-Assisted Assembly Instruction. IEEE Transactions on Industrial Informatics, 19(5), 6825\u20136835. https:\/\/doi.org\/10.1109\/tii.2022.3189428\n"},{"Title":"Demonstrating CleAR Sight: Transparent Interaction Panels for Augmented Reality","DOI":"10.1145\/3544549.3583891","Publication year":2023,"Abstract":"In this work, we demonstrate our concepts for transparent interaction panels in augmented-reality environments. Mobile devices can support interaction with head-mounted displays by providing additional input channels, such as touch &amp; pen input and spatial device input, and also an additional, personal display. However, occlusion of the physical context, other people, or the virtual content can be problematic. To address this, we previously introduced CleAR Sight, a concept and research platform for transparent interaction panels to support interaction in HMD-based mixed reality. Here, we will demonstrate the different interaction and visualization techniques supported in CleAR Sight that facilitate basic manipulation, data exploration, and sketching &amp; annotation for various use cases such as 3D volume visualization, collaborative data analysis, and smart home control.","LowLevel":"data visualization;helmet mounted displays;human computer interaction","MidLevel":"human-computer interaction;data;wearables;display technology","HighLevel":"displays;technology;end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[104,424,38,364,180,279,224,37,217,157,440,243,142,69,75,444,419,7,250,4],"Top20Abs":[424,104,224,37,38,348,157,419,142,364,279,440,162,7,89,180,178,304,228,200],"Top20Tags":[180,279,364,424,38,75,104,217,165,238,206,21,301,288,224,259,250,440,344,422],"Citation":"B\u00fcschel, W., Krug, K., Klamka, K., & Dachselt, R. (2023). Demonstrating CleAR Sight: Transparent Interaction Panels for Augmented Reality. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583891\n"},{"Title":"Bubbleu: Exploring Augmented Reality Game Design with Uncertain AI-based Interaction","DOI":"10.1145\/3544548.3581270","Publication year":2023,"Abstract":"Object detection, while being an attractive interaction method for Augmented Reality (AR), is fundamentally error-prone due to the probabilistic nature of the underlying AI models, resulting in sub-optimal user experiences. In this paper, we explore the effect of three game design concepts, Ambiguity, Transparency, and Controllability, to provide better gameplay experiences in AR games that use error-prone object detection-based interaction modalities. First, we developed a base AR pet breeding game, called Bubbleu that uses object detection as a key interaction method. We then implemented three different variants, each according to the three concepts, to investigate the impact of each design concept on the overall user experience. Our user study results show that each design has its own strengths and can improve player experiences in different ways such as decreasing perceived errors (Ambiguity), explaining the system (Transparency), and enabling users to control the rate of uncertainties (Controllability).","LowLevel":"artificial intelligence;computer games;object detection;user experience","MidLevel":"artificial intelligence;computer vision;human factors;liberal arts","HighLevel":"end users and user experience;technology;industries","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[27,142,111,415,419,70,224,400,114,198,251,116,0,236,189,83,410,200,382,164],"Top20Abs":[27,419,111,224,70,400,142,415,114,189,0,200,82,87,418,116,410,164,113,104],"Top20Tags":[415,308,251,254,142,365,204,72,259,382,353,339,5,18,379,58,198,143,147,29],"Citation":"Kim, M., Lee, K., Balan, R., & Lee, Y. (2023). Bubbleu: Exploring Augmented Reality Game Design with Uncertain AI-based Interaction. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581270\n"},{"Title":"Design android-based learning media using augmented reality technology to support ethnomathematics materials at junior high school","DOI":"10.1063\/5.0105904","Publication year":2023,"Abstract":"The era of the industrial revolution 4.0 has changed human life a lot because most of them use information and communication technology. However, from the many changes that occur, augmented reality technology becomes a solution and challenge to make interactive learning media, especially those that require visual media or require visiting places that are not accessible. Augmented reality is one of the technologies that is growing rapidly in the 4.0 era where the technology can present objects as if they are real in front of us using only Android. The use of android-based learning media using augmented reality technology in addition to helping in the interactive learning process is also a means of equal distribution of education in Indonesia where many schools are still lacking in terms of learning media infrastructure and the high cost of visiting museums. The method used in designing this media uses the ADDIE approach method, namely Analysis, Design, Develop, Implement and Evaluate with limitations without evaluation. The results of the research can produce android-based ethnomathematics learning media design. This means that this android-based learning media is affordable for all people because on average all junior high school students have an android smartphone. [The copyright for the referenced work is owned by Author(s). Copies of full-text articles should only be made or obtained from the publisher or authorized sources.]","LowLevel":"android;computer aided instruction;mobile computing;museums;production engineering computing;smartphones","MidLevel":"cultural heritage;training;engineering;telecommunication;liberal arts;developers;manufacturing","HighLevel":"industries;technology;use cases","Venue":"AIP Conf. Proc. (USA)","Top20AbsAndTags":[9,114,216,262,105,39,56,163,33,320,151,87,244,369,419,57,207,145,306,150],"Top20Abs":[9,114,262,163,39,33,105,87,244,369,150,320,306,207,185,245,136,145,52,56],"Top20Tags":[216,56,82,151,105,93,9,388,419,370,46,302,360,410,405,27,164,3,89,254],"Citation":"Rochmadi, T., Richardo, R., Abdullah, A. A., Wijaya, A., & Nurkhamid. (2023). Design android-based learning media using augmented reality technology to support ethnomathematics materials at junior high school. THE 3RD INTERNATIONAL CONFERENCE ON SCIENCE, MATHEMATICS, ENVIRONMENT, AND EDUCATION: Flexibility in Research and Innovation on Science, Mathematics, Environment, and Education for Sustainable Development. https:\/\/doi.org\/10.1063\/5.0105904\n"},{"Title":"Remote sensing image processing technology based on mobile augmented reality technology in surveying and mapping engineering","DOI":"10.1007\/s00500-021-05650-3","Publication year":2023,"Abstract":"With the continuous advancement of science and technology, the improvement of mobile terminal hardware performance and the large-scale popularization of smart phones have brought new experiences and methods to surveying and mapping work. This article mainly studies the application of remote sensing image processing technology based on mobile augmented reality technology in surveying and mapping engineering. First, perform grayscale processing on the image in the experiment, then remove the noise in the image and smooth the image through the median filter method and finally use the Canny operator to perform edge detection to obtain a binarized image containing only the target object, and this is done by image feature extraction. After using three-dimensional scanning modeling to extract the image feature points, the target manager is used for sample analysis. Obtain the projection matrix through the interface, and then perform coordinate conversion to complete the positioning of the target scene. In this paper, the BRISK feature point detection algorithm with fast speed and small calculation is used to detect the target, and SVM is used for remote sensing feature classification. Experimental data show that the recognition success rate of the algorithm is 84%. The results show that mobile augmented reality technology and remote sensing image processing technology can improve the efficiency and accuracy of surveying and mapping engineering, and have strong ease of use and stability.","LowLevel":"edge detection;feature extraction;image processing;median filters;remote sensing;support vector machines","MidLevel":"artificial intelligence;computer vision;data;chemical;sensors","HighLevel":"industries;technology","Venue":"Soft Comput. (Germany)","Top20AbsAndTags":[98,318,438,456,20,116,99,461,270,451,345,290,435,341,453,234,462,473,486,321],"Top20Abs":[98,438,318,20,456,461,99,270,116,453,435,345,290,341,451,234,462,201,24,486],"Top20Tags":[381,99,98,71,387,132,318,255,353,81,274,358,321,116,119,425,421,379,172,220],"Citation":"Lu, W., Zhao, L., & Xu, R. (2021). Remote sensing image processing technology based on mobile augmented reality technology in surveying and mapping engineering. Soft Computing, 27(1), 423\u2013433. https:\/\/doi.org\/10.1007\/s00500-021-05650-3\n"},{"Title":"Simulation of speckle in pixelated hologram image recovery: application for augmented-reality retinal projection device","DOI":"10.1117\/12.2649933","Publication year":2023,"Abstract":"Our team works on a disruptive concept of Near Eye Display for Augmented Reality (AR) applications. This device requires distributions of holographic elements described as Emissive Points Distributions (EPDs) to create a composite planar wavefront emitted towards the eye. The crystalline lens focuses this signal onto the retina in a mix of diffraction and refraction processes, to form the pixels of an image. We experimentally recorded an image of the letter \"R\" with pixelated holograms. At the reading of this image, we observe speckle that partially alters the image. Using image processing on the experimental results, we can suppress this speckle and recover the initial \"R\", which validates our concept. We develop a simulation tool based on Fourier optics to better understand the emergence of this speckle noise. With the knowledge of the recording process and the form of the hologram given by microscopy, we simulate the electric field En reflected by the different holographic elements from a unique collimated laser. Each field En encodes an angular pixel of the recorded image. The sum of these optical beams in field and\/or in intensity allows us to analyze the role of the different optical elements in the generation of a speckle. In particular, the role of the cross interferences between different EPDs is questioned. The experimental analysis is brought for periodic EPDs but can be extended to the case of random EPDs. It gives some insights into some possible evolutions of our concept in terms of optical implementation. &copy; 2023 SPIE.","LowLevel":"electric fields;holograms;holographic displays;integrated optics;ophthalmology;optical data processing;pixels","MidLevel":"medical;data;power and energy;graphics;optics;display technology","HighLevel":"displays;technology;industries","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[228,344,235,265,332,278,317,201,413,138,437,290,225,158,444,393,431,76,169,461],"Top20Abs":[228,344,265,235,225,332,278,437,76,317,201,393,461,444,343,220,413,433,20,414],"Top20Tags":[138,290,344,441,0,223,169,235,228,201,431,332,438,149,278,413,215,247,158,464],"Citation":"Rainouard, F., Colard, M., Haeberl\u00e9, O., & Martinez, C. (2023). Simulation of speckle in pixelated hologram image recovery: application for AR retinal projection device. Practical Holography XXXVII: Displays, Materials, and Applications. https:\/\/doi.org\/10.1117\/12.2649933\n"},{"Title":"Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile Augmented Reality","DOI":"10.1145\/3572864.3580337","Publication year":2023,"Abstract":"Lighting understanding plays an important role in virtual object composition, including mobile augmented reality (AR) applications. Prior work often targets recovering lighting from the physical environment to support photorealistic AR rendering. Because the common workflow is to use a back-facing camera to capture the physical world for overlaying virtual objects, we refer to this usage pattern as back-facing AR. However, existing methods often fall short in supporting emerging front-facing mobile AR applications, e.g., virtual try-on where a user leverages a front-facing camera to explore the effect of various products (e.g., glasses or hats) of different styles. This lack of support can be attributed to the unique challenges of obtaining 360&#176; HDR environment maps, an ideal format of lighting representation, from the front-facing camera and existing techniques. In this paper, we propose to leverage dual-camera streaming to generate a high-quality environment map by combining multi-view lighting reconstruction and parametric directional lighting estimation. Our preliminary results show improved rendering quality using a dual-camera setup for front-facing AR compared to a commercial solution.","LowLevel":"cameras;image reconstruction;lighting;mobile computing;realistic images;rendering","MidLevel":"telecommunication;computer vision;graphics;construction;input","HighLevel":"industries;technology","Venue":"HotMobile '23: Proceedings of the 24th International Workshop on Mobile Computing Systems and Applications","Top20AbsAndTags":[48,123,225,243,81,208,105,16,327,119,142,316,198,300,167,340,267,111,440,173],"Top20Abs":[48,123,16,243,208,81,142,49,440,225,267,199,119,163,316,151,111,300,198,327],"Top20Tags":[370,225,318,379,98,327,48,96,130,410,167,178,72,220,387,97,36,208,29,105],"Citation":"Zhao, Y., Fanello, S., & Guo, T. (2023). Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile Augmented Reality. Proceedings of the 24th International Workshop on Mobile Computing Systems and Applications. https:\/\/doi.org\/10.1145\/3572864.3580337\n"},{"Title":"The Effects of Interaction Mode and Individual Differences on Usability and User Experience of Mobile Augmented Reality Navigation","DOI":"10.1109\/ACCESS.2023.3271522","Publication year":2023,"Abstract":"In the context of the rapid development of navigation technology and the deepening of users' diversified needs, as an emerging public service, mobile AR (Augmented Reality) navigation is supposed to focus on human-computer interaction and user experience. To extract the influencing factors of efficient service of mobile AR navigation, we constructed an experimental method for the usability of mobile AR navigation and the users' emotional experience based on behavior-emotion analysis. In this study, the user types were divided according to the differences in Mental Cutting Ability and Gender. We explored the effects of Interaction Mode, Mental Cutting Ability, and Gender on the usability of mobile AR navigation and the users' PAD (Please-Arousal-Dominance) three-dimensional emotion through the objective performance and subjective scoring of users when completing AR navigation tasks. The results showed that the Interaction Mode and Mental Cutting Ability had significant effects on the usability of mobile AR navigation and the users' emotional experience; the Ease of Learning, Ease of Use in usability indicators, and the Arousal experience of three-dimensional emotion were significantly affected by Gender. Based on the experimental results, we excavated the mechanism of effects between various factors, extracted the behavioral and emotional trends of different types of users, broadened the research scope of mobile AR navigation-related fields, and finally summarized the design strategies from the perspective of human-robot-environment.","LowLevel":"emotion recognition;human-robot interaction;mobile computing","MidLevel":"robotics;human factors;telecommunication;input","HighLevel":"end users and user experience;technology;industries","Venue":"IEEE Access (USA)","Top20AbsAndTags":[49,139,63,19,118,79,205,352,171,166,101,102,328,326,142,125,195,419,174,210],"Top20Abs":[49,139,63,118,171,328,166,79,101,326,102,352,111,180,87,419,142,103,410,195],"Top20Tags":[19,255,205,407,352,159,174,142,339,29,365,248,63,415,382,147,435,13,37,419],"Citation":"Hu, S., Rong, L., Han, J., Zhang, D., & Jiang, W. (2023). The Effects of Interaction Mode and Individual Differences on Usability and User Experience of Mobile Augmented Reality Navigation. IEEE Access, 11, 41783\u201341795. https:\/\/doi.org\/10.1109\/access.2023.3271522\n"},{"Title":"Low-cost mobile augmented reality service for building information modeling","DOI":"10.1016\/j.autcon.2022.104662","Publication year":2023,"Abstract":"Informed decision-making is crucial for construction site operators. Cyber-physical systems, including various technologies such as augmented reality and building automation tools, are gathering popularity within the infrastructure management sector. However, they are expensive and inaccessible to adopt for most small organizations. This paper describes a prototype of low-cost mobile augmented reality service for BIM and demonstrated its usability for pipe maintenance by supporting inspection, workflow management, data reduction, and augmented reality. User views about the decision support, integration and ease of use of the prototype were also collected from the organisations in the UK and South Korea. The results showed that integrating the augmented reality service with the building automation tool connected to the BIM server enhances decision-making for on-site operations by generating a closed loop. This paper also highlights the need for developing low-cost digital solutions to foster the digitalization of construction organisations with limited budgets. All rights reserved Elsevier.","LowLevel":"building information modelling;building management systems;construction industry;cyber-physical systems;decision making;maintenance engineering;mobile computing;project management;structural engineering computing","MidLevel":"education;human factors;engineering;telecommunication;robotics;business planning and management;construction;manufacturing","HighLevel":"industries;technology;business;end users and user experience","Venue":"Autom. Constr. (Netherlands)","Top20AbsAndTags":[389,181,146,179,35,429,281,315,412,436,286,166,468,68,122,90,190,335,355,137],"Top20Abs":[389,181,146,35,315,179,468,286,166,355,137,335,402,418,68,273,395,63,374,118],"Top20Tags":[146,181,389,35,412,122,429,190,179,281,90,360,362,339,13,139,382,352,55,89],"Citation":"Um, J., Park, J. min, Park, S. yeon, & Yilmaz, G. (2023). Low-cost mobile augmented reality service for building information modeling. Automation in Construction, 146, 104662. https:\/\/doi.org\/10.1016\/j.autcon.2022.104662\n"},{"Title":"Supporting collaborative discussions in surgical teleconsulting through augmented reality head mounted displays","DOI":"10.1145\/3544548.3580714","Publication year":2023,"Abstract":"Although Augmented Reality (AR) has been touted as the future of surgery, its contribution to distributed collaboration such as in surgical teleconsulting has not been articulated. We propose AR-Head Mounted Displays (AR-HMD) to tackle two previously-identified challenges: operating surgeons needing to view and interact with imaging systems that reside away from the operative field, and, their lack of gesturing tools to point and annotate on the shared images and physical environment. We report on a controlled lab experiment where 12 expert gynecology surgeons perform a tumor localisation task guided by a remote radiologist (confederate) via an AR-HMD. We find that bringing the shared images to the place of work reduces the need for clarifications and provides opportunistic access to information when required, and, that pointing and annotating provides opportunities to further support verbal instruction in deictic communication. Our results inform the design of intraoperative AR-HMD systems for surgical telecollaboration.","LowLevel":"groupware;gynaecology;helmet mounted displays;medical image processing;surgery;tumors","MidLevel":"computer vision;collaboration;data;medical;wearables;farming and natural science;other;display technology","HighLevel":"displays;use cases;industries;technology;other","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[234,364,217,180,371,326,425,170,38,152,303,178,356,80,294,135,301,17,440,137],"Top20Abs":[364,217,234,180,326,371,425,170,303,135,38,152,137,440,49,178,237,179,356,411],"Top20Tags":[356,279,234,71,326,199,425,217,337,371,301,186,21,288,243,294,206,230,238,143],"Citation":"Maria, S., Mentis, H. M., Canlorbe, G., & Avellino, I. (2023). Supporting Collaborative Discussions In Surgical Teleconsulting Through Augmented Reality Head Mounted Displays. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580714\n"},{"Title":"Virtual and Augmented Reality for Environmental Sustainability: A Systematic Review","DOI":"10.1145\/3544548.3581147","Publication year":2023,"Abstract":"In recent years, extended reality (XR) technology has seen a rise in use in environmental subjects, i.e., climate change or biodiversity loss, as a potential tool to inform and engage the public with current and future environmental issues. However, research on the potential of XR technology for environmental sustainability is still in the early stages, and there is no clear synthesis of the methods studied in this field. To provide a clearer view of existing approaches and research objectives, we systematically reviewed current literature dealing with XR use in environmental topics. Although the results indicate that the volume of literature exploring XR in environmental applications is increasing, empirical evidence of its impact is limited, hindering the possibility of presently drawing significant conclusions on its potential benefits. Based on our analyses, we identified thematic, theoretical, and methodological knowledge gaps and provide a guideline to aid future research in the field.","LowLevel":"climate mitigation;environmental science computing;sustainable development","MidLevel":"policy;engineering","HighLevel":"technology;business","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[308,314,327,257,306,252,92,123,305,117,391,193,396,168,65,389,388,237,33,410],"Top20Abs":[327,308,314,257,306,92,123,252,396,193,305,65,389,117,168,391,22,207,388,56],"Top20Tags":[193,11,354,305,85,236,222,310,314,308,459,446,282,257,475,373,168,374,372,46],"Citation":"Cosio, L. D., Buruk, O. \u201cOz,\u201d Fern\u00e1ndez Galeote, D., Bosman, I. D. V., & Hamari, J. (2023). Virtual and Augmented Reality for Environmental Sustainability: A Systematic Review. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581147\n"},{"Title":"Tailor Twist: Assessing Rotational Mid-Air Interactions for Augmented Reality","DOI":"10.1145\/3544548.3581461","Publication year":2023,"Abstract":"Mid-air gestures, widely used in today's Augmented Reality (AR) applications, are prone to the \"gorilla arm\" effect, leading to discomfort with prolonged interactions. While prior work has proposed metrics to quantify this effect and means to improve comfort and ergonomics, these works usually only consider simplistic, one-dimensional AR interactions, like reaching for a point or pushing a button. However, interacting with AR environments also involves far more complex tasks, such as rotational knobs, potentially impacting ergonomics. This paper advances the understanding of the ergonomics of rotational mid-air interactions in AR. For this, we contribute the results of a controlled experiment exposing the participants to a rotational task in the interaction space defined by their arms' reach. Based on the results, we discuss how novel future mid-air gesture modalities benefit from our findings concerning ergonomic-aware rotational interaction.","LowLevel":"ergonomics;gesture recognition;human computer interaction","MidLevel":"human-computer interaction;human factors;input","HighLevel":"end users and user experience;technology","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[422,393,177,153,242,122,224,87,142,200,84,106,348,180,175,248,79,53,172,118],"Top20Abs":[422,393,177,224,242,106,200,142,153,180,172,79,175,27,415,118,250,104,234,53],"Top20Tags":[242,122,339,180,366,53,415,213,195,302,365,84,314,279,142,29,210,248,37,118],"Citation":"Sch\u00f6n, D., Kosch, T., M\u00fcller, F., Schmitz, M., G\u00fcnther, S., Bommhardt, L., & M\u00fchlh\u00e4user, M. (2023). Tailor Twist: Assessing Rotational Mid-Air Interactions for Augmented Reality. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581461\n"},{"Title":"Augmented Reality in Service of Human Operations on the Moon: Insights from a Virtual Testbed","DOI":"10.1145\/3544549.3585860","Publication year":2023,"Abstract":"Future astronauts living and working on the Moon will face extreme environmental conditions impeding their operational safety and performance. While it has been suggested that Augmented Reality (AR) Head-Up Displays (HUDs) could potentially help mitigate some of these adversities, the applicability of AR in the unique lunar context remains underexplored. To address this limitation, we have produced an accurate representation of the lunar setting in virtual reality (VR) which then formed our testbed for the exploration of prospective operational scenarios with aerospace experts. Herein we present findings based on qualitative reflections made by the first 6 study participants. AR was found instrumental in several use cases, including the support of navigation and risk awareness. Major design challenges were likewise identified, including the importance of redundancy and contextual appropriateness. Drawing on these findings, we conclude by outlining directions for future research aimed at developing AR-based assistive solutions tailored to the lunar setting.","LowLevel":"aerospace computing;head up displays;space research","MidLevel":"education;medical;aviation and aerospace;display technology","HighLevel":"industries;displays","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[101,118,60,49,102,92,288,64,227,123,57,180,135,67,62,286,69,54,179,45],"Top20Abs":[101,49,60,118,123,92,102,67,54,179,410,227,180,135,234,288,62,57,45,104],"Top20Tags":[286,64,106,374,118,288,254,276,101,69,163,38,210,363,282,248,75,44,21,224],"Citation":"Becker, L., Nilsson, T., Demedeiros, P., & Rometsch, F. (2023). Augmented Reality in Service of Human Operations on the Moon: Insights from a Virtual Testbed. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585860\n"},{"Title":"Virtual reality, augmented reality and mixed reality: For astronaut mental health; and space tourism, education and outreach","DOI":"10.1016\/j.actaastro.2022.12.016","Publication year":2023,"Abstract":"Virtual reality (VR), augmented reality (AR) and mixed reality (MR) can evoke a sense of presence, where a person can feel as though they are physically present in a virtual environment. The intention of this research is to understand whether VR\/AR\/MR can create human connection and experience through interactions with virtual environments to assist in the prevention and treatment of psychological impacts to astronaut health during long-term and long-distance space missions (experiencing Earth from space); and for space tourism, education and outreach purposes as a cost-effective way for people to experience space; to help people understand how important the space industry is to our lives; and to inspire future careers (experiencing space from Earth). Qualitative literature review was undertaken to understand current and potential future VR\/AR\/MR technologies and how they are being used in psychology; in space; for astronaut mental health in space; for two-way communication; for general tourism, education and outreach purposes; and for space tourism, space education and space outreach purposes. Recommendations are proposed for further study into VR\/AR\/MR as a tool for astronaut mental health and wellbeing; and for further development of VR\/AR\/MR experiences for space tourism, education and outreach purposes. All rights reserved Elsevier.","LowLevel":"psychology;space research;travel industry","MidLevel":"education;medical;transportation;aviation and aerospace","HighLevel":"industries","Venue":"Acta Astronaut. (Netherlands)","Top20AbsAndTags":[439,304,210,328,200,227,118,286,365,155,90,395,197,246,101,60,176,42,397,418],"Top20Abs":[304,439,118,197,200,286,365,328,155,210,227,90,176,306,42,199,440,395,246,60],"Top20Tags":[328,65,286,210,363,101,44,394,47,374,459,68,256,96,372,163,90,245,195,246],"Citation":"Holt, S. (2023). Virtual reality, augmented reality and mixed reality: For astronaut mental health; and space tourism, education and outreach. Acta Astronautica, 203, 436\u2013446. https:\/\/doi.org\/10.1016\/j.actaastro.2022.12.016\n"},{"Title":"User Acceptance of Augmented Reality in Education: An Analysis Based on the TAM Model","DOI":"10.1007\/978-3-031-20429-6_44","Publication year":2023,"Abstract":"Augmented reality (AR) is increasingly recognized in various fields, especially in educational processes. Previous research has found that learning through AR technology will help students understand knowledge more creatively and reach a high level of commitment to the learning system. In contrast, the acceptance behavior of AR in an educational setting has been examined by a limited amount of research. Therefore, it is necessary to understand the vitality of AR adoption to motivate learners to use this creative technology in education. In this regard, this study determined the factors of AR acceptance based on the technology acceptance model (TAM). A total of 91 surveys were conducted. The results reveal the positive impact of attitude and perceived usefulness (PU) on intentional behavior (BI) in the adoption of AR in education. The proposed model explains 72% of the variance in behavioral intention to use AR in learning.","LowLevel":"computer aided instruction;human factors;technology acceptance model","MidLevel":"training;human-computer interaction;human factors","HighLevel":"end users and user experience;use cases","Venue":"Proceedings of the 2nd International Conference on Emerging Technologies and Intelligent Systems: ICETIS 2022. Lecture Notes in Networks and Systems (573)","Top20AbsAndTags":[57,359,184,163,87,410,46,196,246,306,30,114,88,216,102,103,185,56,171,124],"Top20Abs":[57,184,359,163,46,410,87,88,30,306,246,102,216,103,196,171,363,114,185,82],"Top20Tags":[57,359,154,146,355,33,87,204,196,195,244,114,147,405,145,320,17,410,9,306],"Citation":"Bourhim, E. M., & Labti, O. (2022). User Acceptance of Augmented Reality in Education: An Analysis Based on the TAM Model. Proceedings of the 2nd International Conference on Emerging Technologies and Intelligent Systems, 481\u2013490. https:\/\/doi.org\/10.1007\/978-3-031-20429-6_44\n"},{"Title":"A New Augmented Reality System for Calculating Social Distancing between Children at School","DOI":"10.3390\/electronics12020358","Publication year":2023,"Abstract":"Social distancing is one of the most important ways to prevent many diseases, especially the respiratory system, where the latest internationally spread is coronavirus disease, and it will not be the last. The spreading of this pandemic has become a major threat to human life, especially to the elderly and people suffering from chronic diseases. During the Corona pandemic, medical authorities were keen to control the spread through social distancing and monitoring it in markets, universities, and schools. This monitoring was mostly used to estimate the distance with the naked eye and interfere with estimating the distance on the observer only. In this study, a computer application was designed to monitor social distancing in closed areas, especially in schools and kindergartens, using a fast, effective and unobtrusive technique for children. In addition to this system, we use augmented reality to help to determine the location of violation of social distancing. This system was tested, and the results were accurate exceeding 98.5%.","LowLevel":"diseases;educational institutions;epidemics;medical computing;paediatrics","MidLevel":"education;medical","HighLevel":"industries","Venue":"Electronics (Switzerland)","Top20AbsAndTags":[390,171,145,202,391,330,312,139,237,164,180,277,316,253,161,147,336,241,136,18],"Top20Abs":[202,390,145,312,237,316,253,171,180,330,277,19,18,226,161,139,77,222,391,164],"Top20Tags":[145,262,336,171,391,390,246,108,363,34,124,210,42,133,376,152,47,71,140,139],"Citation":"Alshaweesh, O., Wedyan, M., Alazab, M., Abu-Salih, B., & Al-Jumaily, A. (2023). A New Augmented Reality System for Calculating Social Distancing between Children at School. Electronics, 12(2), 358. https:\/\/doi.org\/10.3390\/electronics12020358\n"},{"Title":"Simulating Wearable Urban Augmented Reality Experiences in VR: Lessons Learnt from Designing Two Future Urban Interfaces","DOI":"10.3390\/mti7020021","Publication year":2023,"Abstract":"Augmented reality (AR) has the potential to fundamentally change how people engage with increasingly interactive urban environments. However, many challenges exist in designing and evaluating these new urban AR experiences, such as technical constraints and safety concerns associated with outdoor AR. We contribute to this domain by assessing the use of virtual reality (VR) for simulating wearable urban AR experiences, allowing participants to interact with future AR interfaces in a realistic, safe and controlled setting. This paper describes two wearable urban AR applications (pedestrian navigation and autonomous mobility) simulated in VR. Based on a thematic analysis of interview data collected across the two studies, we find that the VR simulation successfully elicited feedback on the functional benefits of AR concepts and the potential impact of urban contextual factors, such as safety concerns, attentional capacity, and social considerations. At the same time, we highlight the limitations of this approach in terms of assessing the AR interface's visual quality and providing exhaustive contextual information. The paper concludes with recommendations for simulating wearable urban AR experiences in VR.","LowLevel":"mobile robots","MidLevel":"robotics","HighLevel":"technology","Venue":"Multimodal Technol. Interact. (Switzerland)","Top20AbsAndTags":[35,200,475,383,49,102,65,53,210,139,165,64,195,219,349,152,93,238,234,120],"Top20Abs":[200,383,49,475,102,165,349,65,53,139,152,234,64,210,316,87,93,159,306,195],"Top20Tags":[399,19,322,396,379,269,120,381,401,398,374,339,347,208,238,193,203,64,487,210],"Citation":"Tran, T. T. M., Parker, C., Hoggenm\u00fcller, M., Hespanhol, L., & Tomitsch, M. (2023). Simulating Wearable Urban Augmented Reality Experiences in VR: Lessons Learnt from Designing Two Future Urban Interfaces. Multimodal Technologies and Interaction, 7(2), 21. https:\/\/doi.org\/10.3390\/mti7020021\n"},{"Title":"ARctic Escape: Promoting Social Connection, Teamwork, and Collaboration Using a Co-Located Augmented Reality Escape Room","DOI":"10.1145\/3544549.3585841","Publication year":2023,"Abstract":"We present ARctic Escape, a co-located augmented reality (AR) escape room designed to promote collaboration between dyads through play. While physical escape rooms provide groups with fun, social experiences, they require a gameplay venue, props, and a game master, all of which detract from their ease of access. Existing AR escape rooms demonstrate that AR can make escape room experiences easier to access. Still, many AR escape rooms are single-player and therefore fail to maintain the social and collaborative elements of their physical counterparts. This paper presents ARctic Escape, a two-person AR escape room with clues emphasizing player interaction and teamwork. We evaluated ARctic Escape by conducting semi-structured interviews with four dyads to learn about participants' interpersonal dynamics and experiences during gameplay. We found that participants thought the experience was fun, collaborative, promoted discussion, and inspired new social dynamics, but sometimes the escape room's reliance on virtual content was disorienting.","LowLevel":"computer games;human computer interaction","MidLevel":"liberal arts;human-computer interaction","HighLevel":"end users and user experience;industries","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[26,386,27,208,236,316,102,79,199,234,164,304,125,219,60,211,440,14,139,111],"Top20Abs":[26,386,208,234,79,27,199,102,304,211,440,60,125,236,139,104,164,316,49,219],"Top20Tags":[204,302,72,365,254,339,147,79,249,351,414,26,236,164,14,410,97,29,66,111],"Citation":"Knoll, T., Liaqat, A., & Monroy-Hern\u00e1ndez, A. (2023). ARctic Escape: Promoting Social Connection, Teamwork, and Collaboration Using a Co-Located Augmented Reality Escape Room. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585841\n"},{"Title":"XAIR: A Framework of Explainable AI in Augmented Reality","DOI":"10.1145\/3544548.3581500","Publication year":2023,"Abstract":"Explainable AI (XAI) has established itself as an important component of AI-driven interactive systems. With Augmented Reality (AR) becoming more integrated in daily lives, the role of XAI also becomes essential in AR because end-users will frequently interact with intelligent services. However, it is unclear how to design effective XAI experiences for AR. We propose XAIR, a design framework that addresses when, what, and how to provide explanations of AI output in AR. The framework was based on a multi-disciplinary literature review of XAI and HCI research, a large-scale survey probing 500+ end-users' preferences for AR-based explanations, and three workshops with 12 experts collecting their insights about XAI design in AR. XAIR's utility and effectiveness was verified via a study with 10 designers and another study with 12 end-users. XAIR can provide guidelines for designers, inspiring them to identify new design opportunities and achieve effective XAI designs in AR.","LowLevel":"artificial intelligence;human computer interaction;interactive systems","MidLevel":"education;artificial intelligence;liberal arts;input;human-computer interaction","HighLevel":"industries;technology;end users and user experience","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[353,308,418,79,27,135,251,53,104,407,224,49,210,114,33,236,175,45,161,102],"Top20Abs":[353,418,27,407,104,49,308,79,224,135,53,114,69,78,45,33,236,102,405,110],"Top20Tags":[308,210,251,18,339,391,415,319,372,352,254,75,55,79,175,409,224,394,353,273],"Citation":"Xu, X., Yu, A., Jonker, T. R., Todi, K., Lu, F., Qian, X., Evangelista Belo, J. M., Wang, T., Li, M., Mun, A., Wu, T.-Y., Shen, J., Zhang, T., Kokhlikyan, N., Wang, F., Sorenson, P., Kim, S., & Benko, H. (2023). XAIR: A Framework of Explainable AI in Augmented Reality. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581500\n"},{"Title":"Augmented Reality and Waste Reduction: Enhancing the Recycling Process for Mobile E-Waste in Automotive Manufacturing","DOI":"10.1109\/ICACCS57279.2023.10112913","Publication year":2023,"Abstract":"Electronic waste (e-waste) includes many items, such as waste from electronic devices, mobile parts, electronic appliances, and other household appliances that have reached the end of their life. India is the world's fifth-largest producer of e-waste, according to UN data. While many technologies and industries have been developed to recycle these enormous amounts of electronic waste, the challenge of separating various WEEE materials remains. To encourage the reuse of damaged or unused mobile phones or mobile parts, they can be utilized in automobile industries for applications such as cameras used in parking and GPS with voice assistance. Our proposed system utilizes augmented reality to gather information about unused parts that are suitable for desired applications in the automobile industry. By using augmented reality, our system can efficiently identify and categorize the parts needed for each application, reducing the time and cost of identifying and separating the parts manually. The aim of our proposed project is to reduce the production and recycling costs of mobile parts while also creating a hazard-free environment by reusing e-waste in the automobile industry.","LowLevel":"automobile industry;environmental science computing;recycling;waste recovery;waste reduction;weee directive","MidLevel":"engineering;automotive;data;industrial equipment;other","HighLevel":"industries;technology;other","Venue":"2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)","Top20AbsAndTags":[11,144,342,159,140,157,216,44,139,214,2,103,320,436,361,148,111,1,204,178],"Top20Abs":[11,144,159,342,216,44,139,157,214,361,103,111,2,436,320,289,204,172,115,268],"Top20Tags":[67,11,261,157,258,140,399,22,190,310,112,388,101,1,178,282,420,242,411,374],"Citation":"Sureshkumar, S., Rani, P. K., C P, A., Kumar, B. A., & R, K. (2023). Augmented Reality and Waste Reduction: Enhancing the Recycling Process for Mobile E-Waste in Automotive Manufacturing. 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS). https:\/\/doi.org\/10.1109\/icaccs57279.2023.10112913\n"},{"Title":"Analysis of emotions in the use of augmented reality technologies in education: A systematic review","DOI":"10.1002\/cae.22593","Publication year":2023,"Abstract":"Currently, the availability and usefulness of computer applications developed supporting the teaching-learning process have increased and are progressively being used in different branches of education. Given the importance of emotions in learning, it is appropriate to review the influence of the use of technology in this field. Out of the many existing technologies, augmented reality (AR) has been of great interest to this study due to its great potential in learning. Therefore, the present study carries out a systematic mapping, whose objective is to review how AR technology influences the emotional state of the student in the learning process. The study indicates that the application of AR technology has both advantages and disadvantages. On the one hand, the use of AR on students produces enjoyment and interest due to visual feedback provided by AR, enthusiasm about the use of innovative technology tools, and curiosity when they view and interact with virtual objects in 3D. The attractive visualizations used and feedback generated by AR applications can reduce the cognitive load and increase student motivation in the learning process. On the other hand, AR may present some negative aspects, such as ergonomic problems, which produce that users prone to dizziness or nausea may reject the use of AR devices and a complex interaction when users must use multiple devices. Besides this, it should not be forgotten that their application may entail technological requirements and associated costs that may be difficult for some schools to afford. &#169; 2023 Wiley Periodicals LLC.","LowLevel":"cognition;computer aided instruction;ergonomics;human factors;teaching","MidLevel":"education;human factors;training","HighLevel":"industries;use cases;end users and user experience","Venue":"Comput. Appl. Eng. Educ. (USA)","Top20AbsAndTags":[163,57,320,306,114,62,82,88,46,296,128,171,185,414,136,51,246,207,145,134],"Top20Abs":[163,306,57,88,62,114,296,82,46,320,171,128,207,139,49,185,124,369,134,184],"Top20Tags":[320,33,154,88,145,114,56,57,51,222,62,363,306,196,3,117,136,82,372,17],"Citation":"G\u00f3mez\u2010Rios, M. D., Paredes\u2010Velasco, M., Hern\u00e1ndez\u2010Bele\u00f1o, R. D., & Fuentes\u2010Pinargote, J. A. (2022). Analysis of emotions in the use of augmented reality technologies in education: A systematic review. Computer Applications in Engineering Education, 31(1), 216\u2013234. Portico. https:\/\/doi.org\/10.1002\/cae.22593\n"},{"Title":"Wizard of Oz Prototyping for Interactive Spatial Augmented Reality in HCI Education: Experiences with Rapid Prototyping for Interactive Spatial Augmented Reality","DOI":"10.1145\/3544549.3573861","Publication year":2023,"Abstract":"In this paper we present our findings that emerged from Wizard of Oz (WOz) prototyping workshops of an undergrad course on UX design between 2017 - 2022. The purpose of these workshops is both to familiarize the students with WOz as a rapid low-complexity prototyping method and to facilitate them in designing for interactive systems beyond typical graphical user interfaces. In these workshops, students develop and test Wizard of Oz prototypes for interactive spatial augmented reality (SAR). Over the past six years, in total 93 prototypes have been created on the course. We analyzed the prototypes that emerged from the workshops based on input, purpose, play characteristics and projection surfaces. Based on the analysis and our experiences during the workshops, we conclude that the workshops stimulate creativity, enable students to rapidly create interactive SAR interfaces without being limited by their technical skills and time limitations and provide them with a tool that they apply beyond the scope of the prototyping course. We discuss advantages and limitations of the use of WOz prototyping for interactive Spatial Augmented Reality.","LowLevel":"computer science education;graphical user interfaces;human computer interaction;interactive systems","MidLevel":"education;graphics;developers;input;human-computer interaction","HighLevel":"industries;technology;end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[53,224,125,153,272,319,88,47,306,419,200,103,134,128,331,46,320,92,261,82],"Top20Abs":[53,224,153,125,272,88,319,200,306,419,103,47,46,261,134,331,163,442,270,92],"Top20Tags":[232,421,105,210,154,58,75,88,128,185,32,242,175,86,409,454,422,224,314,424],"Citation":"Mast, D., Roidl, A., & Jylha, A. (2023). Wizard of Oz Prototyping for Interactive Spatial Augmented Reality in HCI Education. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3573861\n"},{"Title":"TitleIX: Step Up &amp; Step In! A Mobile Augmented Reality Game Featuring Interactive Embodied Conversational Agents for Sexual Assault Bystander Intervention Training on US College Campuses","DOI":"10.1145\/3544549.3583832","Publication year":2023,"Abstract":"Despite the efforts of existing Title IX training programs in the US, current intervention and prevention programs fail to address the problems caused by sexual violence on US college campuses. To address this issue, we designed a mobile augmented reality (AR) game - TitleIX: Step Up &amp; Step In! - that encourages students to become more active and supportive of bystanders through innovative game play, while aiming to improve current sexual assault bystander intervention training. Utilizing AR technology and embodied conversational agents (ECAs), this game provides highly immersive scenario based training for sexual assault bystander intervention while connecting the users to realistic campus experiences. This inventive game design leverages innovative technology to increase awareness of real-world problems; specifically, sexual harassment targeting women and LGBTQ+ students on college campuses. The design implemented in this paper can inform the future construction of AR serious games for social justice.","LowLevel":"chatbots;computer based training;educational institutions;gender issues;mobile learning;serious games","MidLevel":"education;training;simulation;medical;farming and natural science;other","HighLevel":"industries;use cases;other","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[111,100,139,83,104,164,236,241,79,140,414,196,400,103,204,184,134,51,147,380],"Top20Abs":[111,100,139,164,83,79,91,414,140,241,236,380,196,184,104,147,310,204,82,457],"Top20Tags":[145,320,233,103,400,236,171,150,51,115,204,125,241,196,140,56,164,262,390,54],"Citation":"Schlesener, E. A., Lancaster, C. M., Barwulor, C., Murmu, C., & Schulenberg, K. (2023). TitleIX: Step Up &amp; Step In! A Mobile Augmented Reality Game Featuring Interactive Embodied Conversational Agents for Sexual Assault Bystander Intervention Training on US College Campuses. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583832\n"},{"Title":"Memento Player: Shared Multi-Perspective Playback of Volumetrically-Captured Moments in Augmented Reality","DOI":"10.1145\/3544549.3585588","Publication year":2023,"Abstract":"Capturing and reliving memories allow us to record, understand and share our past experiences. Currently, the most common approach to revisiting past moments is viewing photos and videos. These 2D media capture past events that reflect a recorder's first-person perspective. The development of technology for accurately capturing 3D content presents an opportunity for new types of memory reliving, allowing greater immersion without perspective limitations. In this work, we adopt 2D and 3D moment-recording techniques and build a moment-reliving experience in AR that combines both display methods. Specifically, we use AR glasses to record 2D point-of-view (POV) videos, and volumetric capture to reconstruct 3D moments in AR. We allow seamless switching between AR and POV videos to enable immersive moment reliving and viewing of high-resolution details. Users can also navigate to a specific point in time using playback controls. Control is synchronized between multiple users for shared viewing.","LowLevel":"image reconstruction;image resolution;video signal processing","MidLevel":"computer vision;data;graphics;construction;sensors;semiconductors","HighLevel":"industries;technology","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[10,274,305,50,189,225,419,162,331,415,451,97,340,433,178,92,106,101,267,300],"Top20Abs":[10,305,189,419,225,50,274,162,97,92,451,415,331,106,433,251,49,267,0,109],"Top20Tags":[300,370,50,274,358,219,73,345,318,72,225,265,379,220,148,340,186,178,347,100],"Citation":"Liu, Y., Ritchie, J., Kratz, S., Sra, M., Smith, B. A., Monroy-Hern\u00e1ndez, A., & Vaish, R. (2023). Memento Player: Shared Multi-Perspective Playback of Volumetrically-Captured Moments in Augmented Reality. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585588\n"},{"Title":"Effect of augmented reality-based virtual educational robotics on programming students' enjoyment of learning, computational thinking skills, and academic achievement","DOI":"10.1016\/j.compedu.2022.104721","Publication year":2023,"Abstract":"The use of educational robotics for programming education has been shown to be effective in fostering students' computational thinking (CT) skills. However, physical educational robots are expensive, which may limit their wide use in the classroom. This study used augmented reality technology to develop a virtual educational robotic system (AR Bot for short), which offers 3D visual learning feedback to strengthen spatial ability, as well as delayed feedback and auto-scoring feedback to promote students' deeper CT processes. To examine the impact of AR Bot on programming learning, this study used a quasi-experimental design to compare an experimental group of 41 first-year university students who used AR Bot and a control group of 34 first-year university students who used Scratch. We assessed the impact of the two CT tools on students' internal learning processes (enjoyment of learning), CT skills (problem decomposition, algorithm design, and algorithm efficiency skills), and academic achievement. The results showed that students who used AR Bot had higher enjoyment of learning, algorithm design skills, and algorithm efficiency skills but not higher problem decomposition skills and academic achievement than students who used Scratch. Enjoyment of learning led to higher problem decomposition, algorithm design, and algorithm efficiency skills but not academic achievement. Problem decomposition and algorithm design skills, but not algorithm efficiency skills, led to academic achievement. The theoretical and practical implications of the proposed tool and other CT tools in programming education are discussed. All rights reserved Elsevier.","LowLevel":"computer aided instruction;computer science education;educational institutions;educational robots;physics education","MidLevel":"education;training;engineering;robotics;developers","HighLevel":"industries;technology;use cases","Venue":"Comput. Educ. (Netherlands)","Top20AbsAndTags":[232,185,306,369,188,103,87,163,88,46,320,114,134,357,312,145,414,246,154,56],"Top20Abs":[232,185,306,188,88,103,369,163,87,46,357,114,312,414,320,145,56,154,134,246],"Top20Tags":[232,52,154,185,306,88,246,241,89,405,342,56,171,204,196,55,33,145,51,17],"Citation":"Ou Yang, F.-C., Lai, H.-M., & Wang, Y.-W. (2023). Effect of augmented reality-based virtual educational robotics on programming students\u2019 enjoyment of learning, computational thinking skills, and academic achievement. Computers &amp; Education, 195, 104721. https:\/\/doi.org\/10.1016\/j.compedu.2022.104721\n"},{"Title":"Augmented reality android based: Education of modern and traditional instruments","DOI":"10.1016\/j.procs.2022.12.136","Publication year":2023,"Abstract":"Musical instruments in Indonesia are divided into two types, namely traditional musical instruments and modern musical instruments, which in this study will be discussed with more than ten musical instruments. Today, there are still many Indonesian people who do not know traditional musical instruments and modern musical instruments due to lack of knowledge and the lack of means to provide visual access to both traditional and modern musical instruments. The purpose of this research is to help people recognize traditional musical instruments and modern musical instruments digitally easily. The method used in this research is the Marker-Based AR method, because it takes a marker on each object to display a clear object in the Augmented Reality application to find information from an image directly. The results of this study are in the form of an Android-based Augmented Reality application with a page display feature, displaying information on 3D objects of traditional musical instruments and modern musical instruments with writing and sound, as well as showing how to use the application. All rights reserved Elsevier.","LowLevel":"music;musical instruments","MidLevel":"audio","HighLevel":"technology","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[250,157,284,425,176,337,375,105,201,463,116,246,77,214,169,97,487,49,48,326],"Top20Abs":[250,157,425,176,463,201,116,284,105,375,246,214,337,77,487,169,326,49,455,205],"Top20Tags":[284,337,375,3,16,250,27,197,105,82,392,42,376,275,77,327,410,89,129,299],"Citation":"Alamsyah, D. P., Parulian, J. M., & Herliana, A. (2023). Augmented reality android based: Education of modern and traditional instruments. Procedia Computer Science, 216, 266\u2013273. https:\/\/doi.org\/10.1016\/j.procs.2022.12.136\n"},{"Title":"Designing Interactive Shoes for Tactile Augmented Reality","DOI":"10.1145\/3582700.3582728","Publication year":2023,"Abstract":"Augmented Footwear has become an increasingly common research area. However, as this is a comparatively new direction in HCI, researchers and designers are not able to build upon common platforms. We discuss the design space of shoes for augmented tactile reality, focussing on physiological and biomechanical factors as well as technical considerations. We present an open source example implementation from this space, intended as an experimental platform for vibrotactile rendering and tactile AR and provide details on experiences that could be evoked with such a system. Anecdotally, the new prototype provided experiences of material properties like compliance, as well as altered perception of their movements and agency. We intend our work to lower the barrier of entry for new researchers and to support the field of tactile rendering in footwear in general by making it easier to compare results between studies.","LowLevel":"biomechanics;footwear;haptic interfaces;human computer interaction;rendering","MidLevel":"medical;inspection, safety and quality;graphics;input;human-computer interaction;other","HighLevel":"end users and user experience;use cases;industries;technology;other","Venue":"AHs23: Proceedings of the Augmented Humans Conference 2023","Top20AbsAndTags":[403,210,263,167,259,411,331,219,365,240,304,464,279,388,418,410,64,209,53,200],"Top20Abs":[209,365,210,403,304,388,465,267,418,64,259,442,411,170,263,42,240,200,159,41],"Top20Tags":[167,314,422,421,424,419,32,112,455,348,331,420,301,101,219,240,345,443,239,210],"Citation":"Wittchen, D., Martinez-Missir, V., Mavali, S., Sabnis, N., Reed, C. N., & Strohmeier, P. (2023). Designing Interactive Shoes for Tactile Augmented Reality. Augmented Humans Conference. https:\/\/doi.org\/10.1145\/3582700.3582728\n"},{"Title":"Augmented Reality: An Emergent Technology for Students' Learning Motivation for Chemical Engineering Laboratories during the COVID-19 Pandemic","DOI":"10.3390\/su15065175","Publication year":2023,"Abstract":"In higher education, the learning of Unit Operations in Chemical Engineering and the development of practical activities became a real challenge. Therefore, the use of emerging technologies became necessary to develop practical laboratory activities of the Unit Operations due to the inaccessibility to the equipment infrastructure. In this study, Project-Based Learning methodology was assisted with the Augmented Reality (AR) technology for the development of subjects. The development of a real educational experiment for the application of a basic topic of the course as a project for each subject was proposed. The results were presented using the Zappar application, and a unique rubric was used for the evaluation of project. The evaluation of students' motivation for learning was measured using Keller's Attention, Relevance, Confidence and Satisfaction (ARCS) model of motivation by Instructional Materials Motivation Survey (IMMS). The attention, confidence and satisfaction demonstrate an acceptable reliability in comparison to relevance, which was considered as moderate reliability. Above 96% of students considered that the activities, materials, and organization of information used for the AR project caught their attention and encouraged their interest towards the fundamentals applied in the project. Around 80% of students expressed concern about the ease of AR technology use, and understood the learning aim of the project. Above 85% of students recognized the relevance of activities and their usefulness, and considered AR as a meaningful educational tool. 90% of students considered that AR technology helped them to develop the subject competencies. Cronbach's Alpha was used to indicate an acceptable reliability of IMMS instrument. Regarding IMMS, values were superior to 0.7, which could be considered acceptable. For the individual ARCS dimensions, values of Cronbach's alpha reached values of 0.94.","LowLevel":"computer aided instruction;diseases;educational courses;epidemics;further education;human factors","MidLevel":"education;medical;human factors;training","HighLevel":"industries;use cases;end users and user experience","Venue":"Sustainability (Switzerland)","Top20AbsAndTags":[163,185,454,161,82,306,134,320,44,88,57,342,114,147,184,272,87,39,103,62],"Top20Abs":[163,185,88,454,82,161,306,134,342,44,320,272,87,147,184,39,62,57,46,188],"Top20Tags":[391,88,390,154,145,171,363,383,405,134,103,262,57,196,204,114,33,320,329,82],"Citation":"Guaya, D., Meneses, M. \u00c1., Jaramillo-Fierro, X., & Valarezo, E. (2023). Augmented Reality: An Emergent Technology for Students\u2019 Learning Motivation for Chemical Engineering Laboratories during the COVID-19 Pandemic. Sustainability, 15(6), 5175. https:\/\/doi.org\/10.3390\/su15065175\n"},{"Title":"A state-of-the-art survey on Augmented Reality-assisted Digital Twin for futuristic human-centric industry transformation","DOI":"10.1016\/j.rcim.2022.102515","Publication year":2023,"Abstract":"The combination of Augmented Reality (AR) and Digital Twin (DT) has begun to show its potential nowadays, leading to a growing research interest in both academia and industry. Especially under the current human-centric trend, AR embraces the potential to integrate operators into the new generation of Human Cyber-Physical System (HCPS), in which DT is a pillar component. Some review articles have focused on this topic and discussed the benefits of combining AR and DT, but all of them are limited to a specific domain. To fill the gap, this research conducts a state-of-the-art survey (till 17-July-2022) from the AR-assisted DT perspective across different sectors of the industrial field, covering a total of 118 selected publications. Firstly, application scenarios and functions of AR-assisted DT are summarized by following the engineering lifecycle, among which production process, service design, and Human-Machine Interaction (HMI) are hot topics. Then, improvements specifically brought by AR are analyzed according to three dimensions, namely virtual twin, hybrid twin, and cognitive twin, respectively. Finally, challenges and future perspectives of AR-assisted DT for futuristic human-centric industry transformation are proposed, including promoting product design, robotic-related works, cyber-physical interaction, and human ergonomics. All rights reserved Elsevier.","LowLevel":"cyber-physical systems;digital twins;ergonomics;human computer interaction;product design;production engineering computing;reviews","MidLevel":"education;smart cities;human factors;engineering;robotics;manufacturing;standards;human-computer interaction;other","HighLevel":"end users and user experience;use cases;industries;technology;standards;other","Venue":"Robot. Comput.-Integr. Manuf. (Netherlands)","Top20AbsAndTags":[389,429,205,281,412,84,45,174,179,12,352,407,159,49,19,439,374,252,66,110],"Top20Abs":[389,205,412,429,374,49,12,84,252,281,110,179,439,159,19,45,174,102,311,229],"Top20Tags":[429,407,205,33,362,388,411,352,84,45,174,86,412,370,66,281,339,179,210,95],"Citation":"Yin, Y., Zheng, P., Li, C., & Wang, L. (2023). A state-of-the-art survey on Augmented Reality-assisted Digital Twin for futuristic human-centric industry transformation. Robotics and Computer-Integrated Manufacturing, 81, 102515. https:\/\/doi.org\/10.1016\/j.rcim.2022.102515\n"},{"Title":"LiDAR-Based Augmented Reality for the Development of Test Scenarios on Safety for Autonomous Operation of a Shunting Locomotive","DOI":"10.1109\/ICARSC58346.2023.10129540","Publication year":2023,"Abstract":"As public interest in autonomous driving systems grows, safety is becoming a critical issue. Extensive testing is therefore required before these systems can be deployed in real-world traffic. Simulation-based testing has proven to be a valuable tool for evaluating autonomous systems, but there remains a gap between simulation and real-world testing. A system may be well tested in simulation, but in real-world testing it may encounter sudden events that result in unpredictable behavior, and equipment may be damaged in the event of a malfunction. This paper describes a novel augmentation interface for Light Detection And Ranging (LiDAR) sensor data that aims to bridge this gap. The interface is designed to generate realistic test scenarios on live data streams, making it ideal for investigating special and borderline cases. The interface has been developed for the Robot Operating System (ROS) using the Point Cloud Library and is intended to be used for testing an autonomous shunting locomotive. With this interface, a data stream from a 3D LiDAR can be augmented with any given object represented by a point cloud, allowing for the use of data from both simulation and real-world environments.","LowLevel":"mobile robots;optical radar;robot programming","MidLevel":"geospatial;robotics;developers;optics","HighLevel":"displays;technology","Venue":"2023 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)","Top20AbsAndTags":[451,212,301,439,148,374,118,182,158,194,288,486,440,152,399,2,60,350,142,293],"Top20Abs":[451,212,374,118,301,486,148,288,419,439,152,350,194,182,18,276,440,60,139,157],"Top20Tags":[379,399,239,174,322,60,182,297,429,212,243,158,173,49,288,440,347,486,19,451],"Citation":"Kohlisch, N., Koch, P., & May, S. (2023). LiDAR-Based Augmented Reality for the Development of Test Scenarios on Safety for Autonomous Operation of a Shunting Locomotive. 2023 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC). https:\/\/doi.org\/10.1109\/icarsc58346.2023.10129540\n"},{"Title":"Usability Evaluation of an Augmented Reality Sensorimotor Assessment Tool for Astronauts","DOI":"10.1109\/AERO55745.2023.10115780","Publication year":2023,"Abstract":"As next-generation space exploration missions require increased autonomy from astronaut crews, real-time diagnostics of astronaut health and performance are essential for mission operations, especially for determining Extravehicular Activity (EVA) readiness. Due to the disruption of the sensorimotor and neurovestibular systems in microgravity, astronauts exhibit significant impairments in their functional abilities that require distinct adaptation timelines. Physiological decrements can significantly affect operations during and shortly after gravity transitions, when performance risks are greatest. An Augmented Reality (AR) system may be a viable solution to the resource, volumetric, and time constraints of space operations by allowing holographic visual cueing to replace physical objects used in traditional Earth-based assessments. This paper presents the development and preliminary usability testing of a non-intrusive system that employs Augmented Reality and inertial measurement units (IMU) to evaluate sensorimotor and neurovestibular performance throughout mission timelines. The Augmented Reality Operations Readiness Assessment (AURORA) includes distinct assessments for static, dynamic, and operational balance and hand-eye coordination. During the AR development process of AURORA, human-in-the-loop usability experiments were conducted with first-time users (n=15) to gather feedback regarding the user interactions within the application. Usability questions were modeled after established usability surveys (e.g., SUS, NASA TLX) to probe for perceived ease of use, system capabilities, user interface readability, comfort, future use intention, and overall reactions to the system affordances. Thirteen participants reported that the system effectively described assessment requirements via animations and task instructions so they could self-administer assessments. Twelve participants reported their overall experience using the system as positive and rated the process of learning the hand gestures and operating the application as moderately and extremely easy. Questionnaire results indicate that the system has very strong potential for supporting self-administered physiological assessments while maintaining relatively low levels of mental and physical effort. Throughout the study several usability issues were discovered. Four participants struggled to properly execute the trained hand gestures and had issues with depth perception thereby limiting their ability to effectively interact with the system. Explorations into AR design and button interactions will be considered to support users who struggled with system interactions. Further training and instructional protocols will also be implemented to support users. Findings from the usability testing support recommendations for future designers of AR systems and the creation of a full beta version to support validation testing of the derived performance metrics. The research also lays the foundation for countermeasures development and AR training to mitigate impairments to astronauts' functional abilities that may impact operations.","LowLevel":"aerospace computing;data visualization;gesture recognition;human computer interaction;human factors;neurophysiology;space research","MidLevel":"education;human factors;data;medical;aviation and aerospace;input;human-computer interaction","HighLevel":"industries;technology;end users and user experience","Venue":"2023 IEEE Aerospace Conference","Top20AbsAndTags":[286,101,64,70,175,171,65,180,45,374,172,136,153,410,287,366,120,87,205,176],"Top20Abs":[286,101,70,171,175,64,45,172,180,374,136,153,287,65,120,410,137,87,176,229],"Top20Tags":[101,286,65,415,66,242,106,198,254,366,53,122,407,64,180,121,288,195,339,325],"Citation":"Weiss, H., & Stirling, L. (2023). Usability Evaluation of an Augmented Reality Sensorimotor Assessment Tool for Astronauts. 2023 IEEE Aerospace Conference. https:\/\/doi.org\/10.1109\/aero55745.2023.10115780\n"},{"Title":"Dementia Eyes: Co-Design and Evaluation of a Dementia Education Augmented Reality Experience for Medical Workers","DOI":"10.1145\/3544548.3581009","Publication year":2023,"Abstract":"Dementia describes a syndrome of cognitive degeneration, and Behavioural and Psychological Symptoms of Dementia (BPSD) is the non-cognitive symptom. BPSD can be improved by care services. To aid better care service, we explore the potential of using Augmented Reality (AR) to support dementia education for medical workers in three steps: (1) We explore medical workers' perspective on dementia care lived experience and XR, (2) we co-design an educational experience containing an AR-based application and a 5-min activity with medical workers, (3) we evaluate the effectiveness of the system through a mixed method study. Our result shows that the AR experience successfully touches participants, and motivates them to reflect on the provision of care service. On this basis, we discuss the elements and challenges of designing XR-enabled dementia education for users unfamiliar with novel technology, and the potential of using XR in clinical education.","LowLevel":"biomedical education;cognition;computer aided instruction;continuing professional development;health care;human factors;medical computing;medical disorders;personnel;psychology","MidLevel":"education;training;human factors;human resources;medical","HighLevel":"industries;business;use cases;end users and user experience","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[306,330,327,314,257,67,308,391,335,439,140,360,125,390,171,47,305,296,78,51],"Top20Abs":[330,306,327,335,340,112,314,257,67,439,391,125,360,390,171,27,12,188,88,363],"Top20Tags":[140,47,145,376,390,408,328,9,87,306,196,154,34,204,114,33,320,262,42,367],"Citation":"Shen, X., Pai, Y. S., Kiuchi, D., Bao, K., Aoki, T., Meguro, H., Oishi, K., Wang, Z., Wakisaka, S., & Minamizawa, K. (2023). Dementia Eyes: Co-Design and Evaluation of a Dementia Education Augmented Reality Experience for Medical Workers. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581009\n"},{"Title":"Towards Enhancing Children's Science Education using Augmented Reality and Computer Vision","DOI":"10.1109\/EDUCON54358.2023.10125242","Publication year":2023,"Abstract":"Today's technological advancements in mobile technologies and the growing number of mobile devices are extremely beneficial in the mobile learning process. This study is a work in progress that discusses the possibilities of integrating Augmented Reality (AR) and computer vision (CV) into science education which uses deep learning to detect animals in real-time and teach children to classify animals, and learn about their habitat, sound, and important facts. In this study, the Design Science Research (DSR) is used which is a pragmatic approach to creating substantial knowledge for problem-solving through the development of artifacts. The mobile application - AnimalCircle - was developed following the DSR method, and initial users' study was conducted to investigate the efficacy of the use of AR and CV in mobile learning on children's science education and if it can enhance children learning experience. Semi-structured interviews were conducted with children studying at primary level class in Kathmandu district of Nepal between the age groups of 6-12. The findings show that children are positive toward usage of AnimalCircle app in their learning process because they find it beneficial and effective in their learning. However, most children also found it difficult and complained of getting confused during their initial usage. Therefore, significant efforts are required to improve the usage of these technologies in the mobile app and to provide a child-friendly learning experience.","LowLevel":"computer aided instruction;computer vision;deep learning (artificial intelligence);mobile computing;mobile learning;teaching","MidLevel":"education;artificial intelligence;training;telecommunication;computer vision;medical;liberal arts;other","HighLevel":"industries;technology;use cases;other","Venue":"2023 IEEE Global Engineering Education Conference (EDUCON)","Top20AbsAndTags":[83,163,180,103,320,51,140,136,292,34,443,114,222,394,57,161,372,284,87,134],"Top20Abs":[163,83,180,103,320,292,140,136,51,222,114,34,394,443,284,87,185,188,134,306],"Top20Tags":[103,95,387,160,93,82,320,415,33,164,18,204,339,51,222,425,56,372,220,139],"Citation":"Joshi, S., Agbo, F. J., & Jormanainen, I. (2023). Towards Enhancing Children\u2019s Science Education using Augmented Reality and Computer Vision. 2023 IEEE Global Engineering Education Conference (EDUCON). https:\/\/doi.org\/10.1109\/educon54358.2023.10125242\n"},{"Title":"SkillsLab+ - Proof of Concept for Medical Augmented Reality Teaching Application with Haptic Feedback","DOI":"10.1109\/EDUCON54358.2023.10125161","Publication year":2023,"Abstract":"The classic SkillsLab is a training environment in which medical students can train procedures, examinations, hand movements, therapy, and treatment on anatomically correct models or simulated body parts. This training takes place in a stress-free environment, as there is no responsibility, risk, or time pressure. In addition, detailed explanations and instructions are provided by medical experts. SkillsLab+ is a digitalized version of the classic SkillsLab and currently implemented as proof of concept. The course concept is transferred into an augmented reality application, which currently includes nine stations of approaches to medical teaching. These include, among others, the injection into an arm and clinical abdominal exam. The purpose of this application is to allow medical students to work on the learning content independently at their own pace and even from home. Furthermore, the additional integration of haptic gloves allows students to feel and touch virtual objects. A distinction between different surface structures such as hard bones or soft skin can be made via vibrational and force feedback. The evaluation of a user questionnaire shows that a majority of the students felt supported in the preparation and follow-up of the medical content and can imagine using SkillsLab+ in distance learning and in medical teaching to deepen their knowledge in the medical application field.","LowLevel":"biomedical education;bone;computer aided instruction;computer based training;data gloves;distance learning;force feedback;haptic interfaces;medical computing;medical image processing;teaching","MidLevel":"education;training;human-computer interaction;computer vision;data;medical;input;farming and natural science","HighLevel":"industries;technology;use cases;end users and user experience","Venue":"2023 IEEE Global Engineering Education Conference (EDUCON)","Top20AbsAndTags":[47,82,414,408,163,272,340,87,320,439,369,114,134,390,342,436,397,463,88,246],"Top20Abs":[340,47,439,408,82,163,463,234,272,414,117,88,369,207,87,152,390,114,342,445],"Top20Tags":[167,390,140,455,419,301,421,126,408,33,47,145,397,376,71,424,234,9,436,356],"Citation":"Gie\u00dfer, C., Schmitt, J., Limbach, M., Otterbach, J., & Br\u00fcck, R. (2023). SkillsLab+ - Proof of Concept for Medical Augmented Reality Teaching Application with Haptic Feedback. 2023 IEEE Global Engineering Education Conference (EDUCON). https:\/\/doi.org\/10.1109\/educon54358.2023.10125161\n"},{"Title":"Augmented Reality for Programming Teaching: An Exploratory Study","DOI":"10.1109\/EDUCON54358.2023.10125256","Publication year":2023,"Abstract":"The growing adoption of instructional technology in education has been driven by its contribution to society's development. Our study aimed to examine the use of Augmented Reality (AR) in teaching Python programming principles. The study consisted of three phases: a systematic review of AR using the PRISMA statement, the development of a mobile AR application called \"EDUpy,\" and the evaluation of EDUpy through two experiences with undergraduate students enrolled in different course units. The study found 30 studies in literature focused on the introduction of AR in educational contexts, ranging from kindergarten to higher education. Additionally, 57 students were enrolled in the experiences and the results showed that EDUpy was suitable in terms of user experience and effective in achieving students' learning goals. Further research is necessary to determine the impact of the application on students' soft skills, such as creativity, engagement, motivation, teamwork, and self-learning.","LowLevel":"computer aided instruction;computer science education;educational courses;further education;human factors;python;teaching","MidLevel":"education;training;human factors;developers;other","HighLevel":"end users and user experience;use cases;industries;technology;other","Venue":"2023 IEEE Global Engineering Education Conference (EDUCON)","Top20AbsAndTags":[185,320,163,136,134,87,46,306,82,128,369,114,246,62,124,57,103,272,171,47],"Top20Abs":[163,306,46,185,82,369,136,320,128,87,134,62,124,103,57,246,272,171,114,161],"Top20Tags":[154,232,124,87,185,383,405,128,33,171,222,114,320,246,241,196,17,136,272,51],"Citation":"Branco, J., & Pombo, N. (2023). Augmented Reality for Programming Teaching: An Exploratory Study. 2023 IEEE Global Engineering Education Conference (EDUCON). https:\/\/doi.org\/10.1109\/educon54358.2023.10125256\n"},{"Title":"V-Light: Leveraging Edge Computing For The Design of Mobile Augmented Reality Games","DOI":"10.1145\/3582437.3582456","Publication year":2023,"Abstract":"We explore the future of synchronous, multiplayer mobile AR gaming through our game V-Light, which extends current mobile AR game capacities using edge computing. Mobile AR games are currently limited by on-board processing power, while offloading operations to the cloud introduces high latency costs. This is a critical issue for games needing real-time response to player input. V-Light demonstrates how mobile AR games can leverage the power of edge computing, bringing computational resources closer to the user, keeping latency low and bandwidth high. We share our development toolkit, analyze the design and development of V-Light through the lens of an existing model for shared-world mobile AR, and demonstrate that edge computing can provide a \"time machine\" that lets game designers prototype mobile AR games for devices that do not yet exist.","LowLevel":"cloud computing;computer games;edge computing;mobile computing","MidLevel":"telecommunication;liberal arts;networks","HighLevel":"industries;technology","Venue":"FDG '23: Proceedings of the 18th International Conference on the Foundations of Digital Games","Top20AbsAndTags":[346,335,164,113,83,123,79,271,264,147,268,103,151,107,70,236,139,51,110,115],"Top20Abs":[346,164,113,335,79,83,264,147,151,123,103,268,271,110,70,115,134,139,447,26],"Top20Tags":[147,346,8,123,335,164,264,410,97,204,258,51,254,5,271,14,129,359,93,289],"Citation":"Hammad, N., Eiszler, T., Gazda, R., Cartmell, J., Harpstead, E., & Hammer, J. (2023). V-Light: Leveraging Edge Computing For The Design of Mobile Augmented Reality Games. Proceedings of the 18th International Conference on the Foundations of Digital Games. https:\/\/doi.org\/10.1145\/3582437.3582456\n"},{"Title":"A Stand-Alone Augmented Reality Intervention for Chronic Pain Using Embodied Systolic Stimulation","DOI":"10.1109\/NER52421.2023.10123746","Publication year":2023,"Abstract":"Chronic pain presents a tremendous personal, societal, and financial burden. Treatment options are often limited to managing symptoms as opposed to treating the causes, and the consequences of pharmacological treatments can be immensely harmful as evidenced by the on-going opioid epidemic. Virtual Reality interventions and digital therapeutics provide a new tool for therapists but are currently limited to pain-distraction or digitizing traditional approaches that are similarly focused on pain management. However, recent research suggests that potentially neurorestorative, non-pharmacological treatments are conceivable using personalized, embodied, multimodal feedback. In particular, i) providing visual feedback of the affected body part, ii) increasing its embodiment using multisensory stimulation, and iii) timing stimulation to the systole of the ECG cycle have been linked with analgesic effects in induced acute and chronic pain. Based on these findings and methods, we here propose a stand-alone solution that aims to provide neurorestorative feedback for individuals suffering from chronic pain by visualizing interoceptive signals and mapping them onto the patients affected body part using Augmented Reality, tapping into the three aforementioned principles. Our initial findings indicate the feasibility, acceptance, and stimulus adherence of the device and intervention, whose efficacy will be evaluated in an upcoming randomized controlled clinical trial.","LowLevel":"bioelectric potentials;diseases;electrocardiography;epidemics;medical computing;medical disorders;medical signal processing;neurophysiology;patient monitoring;patient treatment","MidLevel":"medical;data;sensors;inspection, safety and quality","HighLevel":"industries;technology;use cases","Venue":"2023 11th International IEEE\/EMBS Conference on Neural Engineering (NER)","Top20AbsAndTags":[384,91,455,72,168,112,167,220,150,294,463,253,84,230,356,250,0,340,390,439],"Top20Abs":[91,220,455,384,72,167,168,0,112,84,144,150,463,216,240,443,355,439,87,44],"Top20Tags":[384,455,417,165,253,230,152,356,250,61,133,394,210,336,294,168,326,145,262,390],"Citation":"Kannape, O. A., Pierret, J., Leeb, R., Cardin, S., Bourban, F., Mensi, S., Lebrun, Y., Merlini, N., Dorier, A., Moriot, V., Touillet, A., & Serino, A. (2023). A Stand-Alone Augmented Reality Intervention for Chronic Pain Using Embodied Systolic Stimulation. 2023 11th International IEEE\/EMBS Conference on Neural Engineering (NER). https:\/\/doi.org\/10.1109\/ner52421.2023.10123746\n"},{"Title":"Development and Evaluation of Augmented Reality Learning Content for Pneumatic Flow: Case Study on Brake Operating Unit of Railway Vehicle","DOI":"10.1109\/ACCESS.2023.3273605","Publication year":2023,"Abstract":"The Brake operating unit (BOU) of a railway vehicle is one of the important systems for controlling the braking of the train. Because this system uses compressed air, it is difficult to understand and train the system. The existing education method involves learning the pneumatic flow of various control air in a 2D pneumatic circuit diagram based on a maintenance manual. However, in the actual braking system, it was difficult to learn effectively because the air flows in 3D. In order to solve these problems, the improvement of the training technique using the new 3D augmented reality (AR) was performed. In this study, to increase the learning effect of air brake flow, a technique for simultaneously displaying the pneumatic flow in 2D circuit diagram and 3D model was proposed. First, the distance ratio for simultaneous display can be determined using the proposed streamline matching variable calculation algorithm (SLMVC) that uses position and animation duration as input variables. Second, to avoid the complexity of using the 24 variables of the Particle System module in Unity, an existing universal 3D platform, a continuously emission property correction algorithm (CEPC) that can output particle objects as a streamline using only 4 properties (e.g., start lifetime, start speed, emission rate over time, start delay). As a result, the following 6 different types of BOU air pressure could be simultaneously displayed in 2D and 3D (e.g., AC, BC, SR, SBR, AS1, AS2). Therefore, maintenance staff can effectively learn complex pneumatic flow. To verify the usability of the developed content, a survey using the NASA-TLX technique was conducted targeting 60 maintenance staff. As a result of the comparison between Group A using the existing maintenance manual and Group B using the developed AR content, the perceived workload decreased by 28%. In particular, the frustration part decreased by 64% and the performance part decreased by 62%, indicating that the usability of AR content was very good.","LowLevel":"aerospace computing;brakes;compressed air systems;computer aided instruction;computer animation;maintenance engineering;pneumatic systems;railways","MidLevel":"education;training;aviation and aerospace;graphics;manufacturing;other","HighLevel":"industries;technology;use cases;other","Venue":"IEEE Access (USA)","Top20AbsAndTags":[206,422,144,275,189,240,171,118,179,131,66,414,261,389,143,233,136,393,437,315],"Top20Abs":[206,422,144,240,189,66,171,275,179,131,389,143,261,118,414,393,437,337,45,315],"Top20Tags":[275,65,374,286,9,254,351,33,82,204,179,134,410,118,284,265,136,89,405,376],"Citation":"Kwon, H. J., Kim, K. S., & Kim, C. S. (2023). Development and Evaluation of Augmented Reality Learning Content for Pneumatic Flow: Case Study on Brake Operating Unit of Railway Vehicle. IEEE Access, 11, 46173\u201346184. https:\/\/doi.org\/10.1109\/access.2023.3273605\n"},{"Title":"Eliciting Security &amp; Privacy-Informed Sharing Techniques for Multi-User Augmented Reality","DOI":"10.1145\/3544548.3581089","Publication year":2023,"Abstract":"The HCI community has explored new interaction designs for collaborative AR interfaces in terms of usability and feasibility; however, security &amp; privacy (S&amp;P) are often not considered in the design process and left to S&amp;P professionals. To produce interaction proposals with S&amp;P in mind, we extend the user-driven elicitation method with a scenario-based approach that incorporates a threat model involving access control in multi-user AR. We conducted an elicitation study in two conditions, pairing AR\/AR experts in one condition and AR\/S&amp;P experts in the other, to investigate the impact of each pairing. We contribute a set of expert-elicited interactions for sharing AR content enhanced with access control provisions, analyze the benefits and tradeoffs of pairing AR and S&amp;P experts, and present recommendations for designing future multi-user AR interactions that better balance competing design goals of usability, feasibility, and S&amp;P in collaborative AR.","LowLevel":"authorisation;data privacy;groupware;human computer interaction","MidLevel":"security;human-computer interaction;collaboration;policy","HighLevel":"end users and user experience;technology;business;use cases","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[80,303,113,53,135,54,58,70,234,69,79,419,142,100,114,27,250,17,118,224],"Top20Abs":[80,303,53,135,234,70,419,49,27,79,58,429,114,151,224,142,54,60,415,87],"Top20Tags":[377,80,29,279,100,242,17,239,303,267,329,304,203,388,250,113,125,131,199,331],"Citation":"Rajaram, S., Chen, C., Roesner, F., & Nebeling, M. (2023). Eliciting Security &amp; Privacy-Informed Sharing Techniques for Multi-User Augmented Reality. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581089\n"},{"Title":"The Impact of the Use of Augmented Reality on Online Purchasing Behavior Sustainability: The Saudi Consumer as a Model","DOI":"10.3390\/su15065448","Publication year":2023,"Abstract":"This study aimed to examine the impact of augmented reality (AR) on the purchasing behavior of Saudi customers using analytic-descriptive methods and data from a snowball sample of 812 online buyers. Positive correlations were found between AR factors (hermeneutic, embodiment, and background) and dimensions (quality, fun, and creativity) and the purchase experience. Young women aged 17-26 mainly use AR for buying clothes and accessories, and the majority of the sample shops are available locally through mobile apps. The findings indicate that AR has a significant influence on buying decisions and suggest its potential use in marketing communications. The results also reveal that gender, social status, education level, and monthly income have an impact on participants' responses to AR, with women and those who are married having more favorable views. Clothing and accessories were found to be the most frequently purchased products through AR. There were no significant differences based on age or the number of family members. Participants reported positively about their AR experience, and their concerns and anxiety did not affect their purchasing experience. Based on the main study's results, a number of recommendations can be made: Saudi businesses need to use AR in their marketing communication strategies to meet consumer needs and trends. To maximize the benefits of brand awareness, they should use AR techniques and adopt this technology for products that depend on design in their production. When using AR in general and in light of the theories that have been studied, it is important to think about the cultural traits and dimensions of Saudi consumers and conduct further exploratory research before implementation.","LowLevel":"consumer behaviour;electronic commerce;mobile computing;purchasing","MidLevel":"telecommunication;sales and marketing;logistics","HighLevel":"industries;business","Venue":"Sustainability (Switzerland)","Top20AbsAndTags":[368,156,57,62,49,219,40,13,70,333,60,125,42,103,87,163,88,65,100,110],"Top20Abs":[368,49,57,62,88,156,60,70,13,40,125,219,87,103,110,42,184,46,135,171],"Top20Tags":[333,13,368,40,42,355,156,269,346,160,93,8,164,327,339,142,209,147,219,51],"Citation":"AL Hilal, N. S. H. (2023). The Impact of the Use of Augmented Reality on Online Purchasing Behavior Sustainability: The Saudi Consumer as a Model. Sustainability, 15(6), 5448. https:\/\/doi.org\/10.3390\/su15065448\n"},{"Title":"Developing an Augmented Reality Lunar Surface Navigation System","DOI":"10.1109\/AERO55745.2023.10115923","Publication year":2023,"Abstract":"For astronauts, lunar Extravehicular Activity (EVA) operations are extraordinarily challenging to perform. Physically, they must perform demanding tasks while clothed in exhaustingly heavy space suits. Mentally, they must perform complicated tasks without error while exploring unfamiliar and uncharted locations. With the high value of each moment spent on the moon, immense time pressure also increases the mental load placed on these astronauts. The Rhode Island School of Design (RISD) team participated in the 2022 NASA Spacesuit User Interface Technologies for Students (SUITS) Challenge and developed an augmented reality lunar surface navigation system, aiming at reducing the cognitive load for astronauts during their EVA operations. The overall design approach is to unify interfaces, separate planes for interactions, and maximize automation. The navigation system features an intuitive navigational aid, a three-dimensional path guide which is overlaid on the landscape, and a minimized user interface. Hardware components such as a tab-traversal style control are also incorporated to increase usability.","LowLevel":"aerospace safety;cognition;data visualization;lunar surface;space research;space vehicles;user interfaces","MidLevel":"education;human factors;automotive;data;medical;aviation and aerospace;inspection, safety and quality;human-computer interaction;other","HighLevel":"end users and user experience;use cases;industries;technology;other","Venue":"2023 IEEE Aerospace Conference","Top20AbsAndTags":[118,65,286,70,176,63,288,64,419,408,401,399,189,139,121,94,397,76,248,303],"Top20Abs":[118,65,70,286,176,419,63,64,408,288,399,139,76,189,303,291,18,248,79,401],"Top20Tags":[286,288,118,242,64,339,374,309,126,179,65,84,80,121,439,382,232,331,145,397],"Citation":"Ahner-McHaffie, K. T., Yang, S., Tan, Y., & Zhou, B. (2023). Developing an Augmented Reality Lunar Surface Navigation System. 2023 IEEE Aerospace Conference. https:\/\/doi.org\/10.1109\/aero55745.2023.10115923\n"},{"Title":"CC-Glasses: Color Communication Support for People with Color Vision Deficiency Using Augmented Reality and Deep Learning","DOI":"10.1145\/3582700.3582707","Publication year":2023,"Abstract":"People who suffer from color vision deficiency (CVD) can face difficulties when communicating with others by failing to identify target objects referred by their color names. While most existing studies on CVD compensation have focused on the issue of color contrast loss. Although there are approaches can provide clues of color name to users, these techniques either require training, or cannot protect users' privacy, i.e., the fact of having CVD. In this paper, based on augmented reality (AR) and deep learning technologies, we propose a novel system to provide supporting information to users affected by CVD for color communication assistance. The state-of-the-art deep neural network (DNN) model for referring segmentation (RS) is adopted to generate supporting information, and AR glasses are utilized for information presentation. To improve the performance of the proposed system further, a new dataset is constructed based on a novel concept called Color-Object Noun Pair. The results of evaluation experiments show that the new dataset can enhance the performance of the adopted DNN model, and the proposed system can help users affected by CVD successfully identify target objects by their color names.","LowLevel":"color vision;deep learning (artificial intelligence);image colour analysis;image segmentation","MidLevel":"artificial intelligence;computer vision;medical;graphics;liberal arts;input;other","HighLevel":"industries;technology;other","Venue":"AHs23: Proceedings of the Augmented Humans Conference 2023","Top20AbsAndTags":[99,438,280,202,288,149,325,160,132,379,116,451,441,415,350,271,144,79,0,400],"Top20Abs":[99,202,438,288,280,325,116,0,144,149,350,400,379,132,451,415,487,271,79,139],"Top20Tags":[415,160,379,115,18,425,255,280,387,308,325,339,428,81,381,260,143,271,483,460],"Citation":"Zhu, Z., Li, J., Tang, Y., Go, K., Toyoura, M., Kashiwagi, K., Fujishiro, I., & Mao, X. (2023). CC-Glasses: Color Communication Support for People with Color Vision Deficiency Using Augmented Reality and Deep Learning. Augmented Humans Conference. https:\/\/doi.org\/10.1145\/3582700.3582707\n"},{"Title":"PointShopAR: Supporting Environmental Design Prototyping Using Point Cloud in Augmented Reality","DOI":"10.1145\/3544548.3580776","Publication year":2023,"Abstract":"We present PointShopAR, a novel tablet-based system for AR environmental design using point clouds as the underlying representation. It integrates point cloud capture and editing in a single AR workflow to help users quickly prototype design ideas in their spatial context. We hypothesize that point clouds are well suited for prototyping, as they can be captured more rapidly than textured meshes and then edited immediately in situ on the capturing device. We based the design of PointShopAR on the practical needs of six architects in a formative study. Our system supports a variety of point cloud editing operations in AR, including selection, transformation, hole filling, drawing, morphing, and animation. We evaluate PointShopAR through a remote study on usability and an in-person study on environmental design support. Participants were able to iterate design rapidly, showing the merits of an integrated capture and editing workflow with point clouds in AR environmental design.","LowLevel":"computational geometry;computer animation;notebook computers","MidLevel":"graphics;artificial intelligence;developers;input","HighLevel":"technology","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[451,350,67,98,224,275,318,5,321,418,65,340,298,471,234,53,427,135,353,114],"Top20Abs":[451,350,98,224,67,318,418,321,234,65,340,5,53,45,68,123,353,114,120,118],"Top20Tags":[148,351,275,39,106,211,308,261,95,379,410,284,415,72,18,48,422,353,387,219],"Citation":"Wang, Z., Nguyen, C., Asente, P., & Dorsey, J. (2023). PointShopAR: Supporting Environmental Design Prototyping Using Point Cloud in Augmented Reality. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580776\n"},{"Title":"Comparison of Physical Activity Training Using Augmented Reality and Conventional Therapy on Physical Performance following a Total Knee Replacement: A Randomized Controlled Trial","DOI":"10.3390\/app13020894","Publication year":2023,"Abstract":"There is growing interest in using augmented reality (AR)-based training for rehabilitation programs, while it remains unclear whether physical exercises using AR can be more effective than conventional therapy for patients with total knee replacement (TKR). This study, therefore, aimed to compare the effects of AR-based training to conventional therapist-based training on the physical performance of early-stage rehabilitation in patients after a TKR. It was a double-blind randomized controlled trial with repeated measures (pre-surgery, post-surgery, and post-intervention). Twenty-four participants with TKR were allocated to either AR-based training or therapist-based training. Both groups received a training program for 30 min per session, three sessions per week, for four weeks. The outcome measures included the range of motion (ROM), muscle strength, balance, and perceived pain. The results showed significant improvements in all measured outcomes in both groups (&lt;i&gt;p&lt;\/i&gt;&lt; 0.05). However, despite our hypothesis that ART would be more effective than the TKR, no significant differences in all the outcomes were found between the two groups. While there was some evidence showing that performing physical exercises using AR could improve physical performance in patients with TKR after surgery, a comparison with conventional therapy did not show superior effectiveness. However, AR could be used to provide real-time feedback and motivation appropriate for home-training programs.","LowLevel":"biomechanics;medical computing;muscle;patient rehabilitation;prosthetics;surgery","MidLevel":"medical","HighLevel":"industries","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[33,417,294,384,40,152,390,37,455,168,140,108,465,210,13,394,383,185,234,165],"Top20Abs":[33,417,294,384,40,140,455,37,390,168,380,165,13,465,210,108,113,383,394,185],"Top20Tags":[417,133,294,152,397,326,384,186,168,356,108,234,376,465,210,140,42,336,371,69],"Citation":"Yu, J.-H., Nekar, D. M., Kang, H.-Y., Lee, J.-W., & Oh, S.-Y. (2023). Comparison of Physical Activity Training Using Augmented Reality and Conventional Therapy on Physical Performance following a Total Knee Replacement: A Randomized Controlled Trial. Applied Sciences, 13(2), 894. https:\/\/doi.org\/10.3390\/app13020894\n"},{"Title":"Augmented reality navigation application to promote tourism to local state attraction \"Lawang Sewu\"","DOI":"10.1016\/j.procs.2022.12.193","Publication year":2023,"Abstract":"The writing of this paper and the creation of this application is a means to improve and add on facilities on the local state-run attraction. This subject matter was picked in conjunction with the decline of tourism in the said historical site due to a pandemic that rankles Indonesia's tourism industry and drives it to the ground. Augmented Reality as a method of deliverance was picked due to its popularity and freshness in the tourism market. The research method was conducted starting with analysis, development, implementation, then evaluation. Analysis was done using sourcing available to the public and a questioner. Implementation was done using Unity, Vuforia, and Maya. And with the positive responses of 66.6 percent of the sample market finding enjoyment in the use of this application in their exploration during User Acceptance testing with the existing prototype. This shows that the resulting use of this application improves user enjoyment in experiencing the state attraction. All rights reserved Elsevier.","LowLevel":"building management systems;buildings;energy conservation;space cooling;temperature measurement;thermal comfort;travel industry","MidLevel":"education;human factors;transportation;inspection, safety and quality;power and energy;construction;sensors;farming and natural science;other","HighLevel":"end users and user experience;use cases;industries;technology;other","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[155,166,64,25,392,87,86,207,30,105,35,388,134,361,96,410,34,145,181,51],"Top20Abs":[155,166,64,207,25,392,30,388,361,105,87,134,35,410,208,145,267,359,20,261],"Top20Tags":[146,96,86,392,190,68,322,64,181,109,194,481,54,264,288,345,166,348,445,18],"Citation":"Pranoto, H., Saputra, P. P., Sadekh, M., Darmadi, H., & Yanfi, Y. (2023). Augmented reality navigation application to promote tourism to local state attraction \u201cLawang Sewu.\u201d Procedia Computer Science, 216, 757\u2013764. https:\/\/doi.org\/10.1016\/j.procs.2022.12.193\n"},{"Title":"UTAUT2 Model to Explain the Adoption of Augmented Reality Technology in Education: An Empirical Study in Morocco","DOI":"10.1007\/978-3-031-20429-6_45","Publication year":2023,"Abstract":"The use of Augmented Reality (AR) is recognized as the latest advances in technology that can serve as an educational device capable of improving learning outcomes. Previous research has found that learning through AR technology will help students understand knowledge more creatively and reach a high level of commitment to the learning system. In contrast, the acceptance behavior of AR in an educational setting has been examined by a limited amount of research. Therefore, it is necessary to understand the vitality of AR adoption to motivate learners to use this creative technology in education. In this regard, this study determined the factors of AR acceptance using the Unified Theory of Acceptance and Use of Technology 2 (UTAUT 2). A total of 100 surveys were conducted. The results indicate that effort expectancy (EE), performance expectancy (PE), and social influence (SI) impact the behavioural intention (BI) to use AR-learning.","LowLevel":"computer aided instruction;human factors;technology acceptance model","MidLevel":"training;human-computer interaction;human factors","HighLevel":"end users and user experience;use cases","Venue":"Proceedings of the 2nd International Conference on Emerging Technologies and Intelligent Systems: ICETIS 2022. Lecture Notes in Networks and Systems (573)","Top20AbsAndTags":[62,163,359,87,46,114,184,306,102,196,185,39,103,124,88,136,216,410,244,171],"Top20Abs":[62,359,163,87,46,184,88,102,306,216,103,196,114,410,56,171,39,185,124,136],"Top20Tags":[62,154,244,87,33,204,196,359,147,114,124,405,195,145,82,355,320,410,146,185],"Citation":"Benrahal, M., Bourhim, E. M., Dahane, A., Labti, O., & Akhiate, A. (2022). UTAUT2 Model to Explain the Adoption of Augmented Reality Technology in Education: An Empirical Study in Morocco. Proceedings of the 2nd International Conference on Emerging Technologies and Intelligent Systems, 491\u2013500. https:\/\/doi.org\/10.1007\/978-3-031-20429-6_45\n"},{"Title":"A human-centered conceptual model for integrating Augmented Reality and Dynamic Digital Models to reduce occupational risks in industrial contexts","DOI":"10.1016\/j.procs.2022.12.273","Publication year":2023,"Abstract":"The article proposes a human-centered conceptual model to integrate Augmented Reality (AR) and Dynamic Digital Models (DDM) to improve training and reduce occupational risks in industrial contexts. A general model integrating DDM and AR to support immersive training and customized and real-time risk management is missing in the literature. The proposed conceptual model was preliminarily validated with company experts in a laboratory environment. According to the expert's feedback, the system can improve the efficacy of training by means of an immersive environment where users can better perceive hazards and safety-critical situations. Considering the capability of the conceptual model to support real-time and customized risk management, the participating experts argue that the technology seems not to be ready yet, even if it is very interesting and it would be very useful in practice to mitigate occupational risks. Future research activities will consist of the development of a prototypical system based on the presented conceptual model by considering specific user requirements and experts' feedback. All rights reserved Elsevier.","LowLevel":"computer based training;decision making;occupational safety;risk management","MidLevel":"training;farming and natural science;human factors;inspection, safety and quality","HighLevel":"industries;use cases;end users and user experience","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[436,252,45,179,84,303,0,486,112,118,439,104,463,34,205,340,397,233,65,311],"Top20Abs":[45,252,84,303,179,0,436,35,118,234,65,340,486,104,228,397,205,440,467,112],"Top20Tags":[233,436,94,273,84,206,380,140,439,113,33,204,301,419,383,408,17,179,196,154],"Citation":"Gualtieri, L., Revolti, A., & Dallasega, P. (2023). A human-centered conceptual model for integrating Augmented Reality and Dynamic Digital Models to reduce occupational risks in industrial contexts. Procedia Computer Science, 217, 765\u2013773. https:\/\/doi.org\/10.1016\/j.procs.2022.12.273\n"},{"Title":"Designing interactive augmented reality application for student's directed learning of continuous distillation process","DOI":"10.1016\/j.compchemeng.2022.108086","Publication year":2023,"Abstract":"Continuous distillation is an important separation process in chemical engineering curriculum. Conventionally, it is often taught as black box diagrams that lack user visibility, thus preventing students from developing a deeper understanding and comprehension. This paper presents the technical development of the mobile AR application and its implementation in the curriculum of Chemical Engineering Unit Operations at Nanyang Technological University Singapore. The mobile AR application is to provide interactive visualization and real-time numerical simulation to promote students' active learning in the classroom. The results of the pre-test and post-test indicated that the AR application helped students gain a better understanding of the principle and fundamental concepts of the distillation process. The evaluation survey with 6-point likert scale questions shows that students had a positive experience using the mobile AR application and it has enhanced their learning experience, as suggested by a mean evaluation score of 5.18 out of 6.0. All rights reserved Elsevier.","LowLevel":"chemical engineering;computer aided instruction;data visualization;distillation;educational courses;educational institutions;engineering education","MidLevel":"education;data;other;training","HighLevel":"industries;technology;use cases;other","Venue":"Comput. Chem. Eng. (Netherlands)","Top20AbsAndTags":[262,320,82,163,88,161,124,246,306,136,272,369,114,342,17,103,87,207,51,171],"Top20Abs":[262,82,163,88,320,124,434,161,207,87,306,17,272,103,342,114,369,246,115,51],"Top20Tags":[154,136,171,241,103,400,262,376,405,124,145,410,342,185,196,272,56,9,88,320],"Citation":"Gao, S., Lu, Y., Ooi, C. H., Cai, Y., & Gunawan, P. (2023). Designing interactive augmented reality application for student\u2019s directed learning of continuous distillation process. Computers &amp; Chemical Engineering, 169, 108086. https:\/\/doi.org\/10.1016\/j.compchemeng.2022.108086\n"},{"Title":"Location-Aware Adaptation of Augmented Reality Narratives","DOI":"10.1145\/3544548.3580978","Publication year":2023,"Abstract":"The recent popularity of augmented reality (AR) devices has enabled players to participate in interactive narratives through virtual events and characters populated in a real-world environment, where different actions may lead to different story branches. In this paper, we propose a novel approach to adapt narratives to real spaces for AR experiences. Our optimization-based approach automatically assigns contextually compatible locations to story events, synthesizing a navigation graph to guide players through different story branches while considering their walking experiences. We validated the effectiveness of our approach for adapting AR narratives to different scenes through experiments and user studies.","LowLevel":"computer games;location based services","MidLevel":"geospatial;liberal arts","HighLevel":"industries;technology","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[292,6,229,16,197,79,195,53,26,440,120,139,24,70,410,87,267,279,49,280],"Top20Abs":[6,292,229,16,26,195,53,440,87,79,70,49,120,139,208,410,163,179,388,142],"Top20Tags":[197,29,147,164,204,30,37,410,97,111,254,51,3,388,59,351,10,72,79,346],"Citation":"Li, W., Li, C., Kim, M., Huang, H., & Yu, L.-F. (2023). Location-Aware Adaptation of Augmented Reality Narratives. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580978\n"},{"Title":"Spatial location-based outdoor mobile augmented reality 3D registration technology","DOI":"10.1117\/12.2667317","Publication year":2023,"Abstract":"Human activities are closely related to geographic location. It is proposed to combine spatial location and mobile terminal pose sensor data with meeting the characteristics of real-time accuracy and flexibility in outdoor mobile augmented reality and to realize the virtual-real superposition through the transformation relationship between 3D model coordinate system, world coordinate system, camera coordinate system, image coordinate system, and pixel coordinate system. For the limitations of the vision-based registration method in outdoor scenes, this paper derives the transformation from spatial location data to screen coordinates in detail. It gives the solution and optimization of the transformation matrix and parameters. &copy; 2023 SPIE.","LowLevel":"3d modeling;linear transformations;location","MidLevel":"geospatial;manufacturing;medical;data","HighLevel":"industries;technology","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[321,316,467,486,20,197,76,267,30,157,379,14,220,120,264,139,287,98,270,488],"Top20Abs":[316,20,486,76,321,267,220,467,287,292,98,139,379,352,120,264,311,270,157,401],"Top20Tags":[30,467,457,475,207,183,197,20,157,307,438,147,36,446,29,283,138,14,278,189],"Citation":"Zhou, Q., Wang, Q., Zhong, Q., & Han, M. (2023). Spatial location-based outdoor mobile augmented reality 3D registration technology. Fifth International Conference on Computer Information Science and Artificial Intelligence (CISAI 2022). https:\/\/doi.org\/10.1117\/12.2667317\n"},{"Title":"The Reflective Make-AR In-Action: Using Augmented Reality for Reflection-based Learning of Makerskills","DOI":"10.1145\/3544549.3585850","Publication year":2023,"Abstract":"Recent work on reflective learning supports self-paced learning of skills like breadboarding and using power tools in makerspaces through a reflection exercise toolkit. This toolkit monitors the learners' performances in real-time and prompts them to reflect both in-action and on-action i.e., during and after their maker activities. In this paper, we build on this prior work and use an augmented reality system to monitor, prompt, and record in-action reflections, i.e., while the maker activity is in progress. In particular, we propose a framework to design multi-modal reflective prompts for self-learning exercises using augmented reality with three specific goals - (1) adding real-world contextualization, (2) overlaying personalized multimodal contextual information for supporting in-action reflections, and (3) maintaining an immersive experience during the reflection exercises. We conclude with a discussion of three application case studies for reflective AR maker exercises.","LowLevel":"computer aided instruction;sports","MidLevel":"cultural heritage;training","HighLevel":"industries;use cases","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[311,387,179,163,353,444,236,172,410,215,114,22,65,125,400,364,293,330,454,118],"Top20Abs":[353,311,163,387,444,179,410,330,364,172,22,65,400,454,116,88,236,125,244,215],"Top20Tags":[114,9,204,103,215,119,33,236,93,77,405,3,274,82,34,145,109,17,376,232],"Citation":"Turakhia, D. G., Jiang, P., & Mueller, S. (2023). The Reflective Make-AR In-Action: Using Augmented Reality for Reflection-based Learning of Makerskills. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585850\n"},{"Title":"CO2LLAB: Creating an Eco-Conscious Community through Habit Tracking and Augmented Reality Visualisation","DOI":"10.1145\/3544549.3583834","Publication year":2023,"Abstract":"CO2e emissions remain a substantial problem for the environment despite efforts from numerous initiatives over the years to curb their output. The role of younger generations is critical in this problem space. From user research, we found that young adults rarely volunteer but do engage frequently in individual actions geared towards the environment. They are also motivated when they see others' contributions. We decided to leverage this willingness for individual environmental actions and turn it into a collective effort by creating CO2LLAB, a platform that reimagines how young adults can reduce CO2e emissions with their community. It incorporates a habit-tracking app for eco-friendly actions and augmented reality technology to visualise their impact. Our prototype evaluation demonstrated that CO2LLAB not only educated them on the topic, but also motivated them to consider their impact on their community more.","LowLevel":"data visualization;human factors;mobile computing","MidLevel":"human factors;data;telecommunication","HighLevel":"end users and user experience;technology;industries","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[147,387,67,23,172,42,410,282,359,164,123,311,189,102,440,279,53,57,435,326],"Top20Abs":[147,387,67,282,172,440,23,164,115,53,311,123,102,410,189,226,359,279,292,12],"Top20Tags":[477,410,13,42,82,376,147,195,339,105,142,248,57,327,359,121,70,93,254,288],"Citation":"Chun, H. Y. R., Gao, Y., Nursalamah, R. K., O Keeffe, C. M., & Shin, H. (2023). CO2LLAB: Creating an Eco-Conscious Community through Habit Tracking and Augmented Reality Visualisation. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583834\n"},{"Title":"May I Still Define Myself? Exploring How Dissonance in Displaying Personal Information Through Head-Mounted Augmented Reality Can Affect Personal Information Sovereignty","DOI":"10.1145\/3544549.3585821","Publication year":2023,"Abstract":"Head-mounted Augmented Reality enables individuals to overlay digital information onto the physical world, consequently influencing how they assess and react to augmented social situations. While prior work has shown that augmenting social situations with faithful personal information can benefit a conversation, honest mistakes or an attempt to deceive might lead to a dissonance between augmentation and verbally disclosed information. In this work, we take the first steps towards understanding the happenings in case of information dissonance by conducting a preliminary within-subject online video study (N=30), investigating how it affects users, perception of the interlocutor, and if augmentation or interlocutor would act as the more trusted instance. We found that only 26.7% trusted the interlocutor's verbally uttered information, while a majority believed the AR device (46.7%) or were undecided (26.7%). We discuss this split in trust and argue for the importance of and factors for a follow-up study on this topic.","LowLevel":"helmet mounted displays;human factors","MidLevel":"wearables;human factors;display technology","HighLevel":"displays;end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[244,365,259,121,276,440,102,444,238,129,180,481,165,229,237,248,42,219,69,371],"Top20Abs":[244,129,121,102,365,229,481,276,259,42,412,5,440,110,239,17,181,402,226,248],"Top20Tags":[206,279,238,195,217,364,180,75,259,80,288,165,301,371,440,69,191,424,243,337],"Citation":"Rixen, J. O., Funk, C., Rukzio, E., & Gugenheimer, J. (2023). May I Still Define Myself? Exploring How Dissonance in Displaying Personal Information Through Head-Mounted Augmented Reality Can Affect Personal Information Sovereignty. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585821\n"},{"Title":"Improvement and Application of Image Segmentation Algorithm for Outdoor Augmented Reality","DOI":"10.1007\/978-981-99-0416-7_37","Publication year":2023,"Abstract":"The traditional image recognition process mostly occurs in indoor scenes with relatively stable environment, and the current development of augmented reality tends to apply to outdoor scenes, such as the identification of buildings, statues and other objects in the tourism industry. Aimed to cope with the problem that the augmented reality effect of the ARToolKit system is difficult to be achieved in the outdoor natural scenes because of the wrong segmentation due to the influence of real-time changing illumination, occlusion and other factors, an improved GrabCut algorithm based on histogram equalization is proposed. Histogram equalization uses non-linear stretching of the image to improve image contrast. The boundary function of GrabCut algorithm is averaged to reduce the error segmentation of edge pixels, improve the edge integrity of the target image, and achieve accurate image segmentation. The experimental results show that the proposed method has high real-time performance and stability in complex real-world scenarios, and improves the processing performance of ARToolKit augmented reality system in outdoor scenes. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"equalizers;graphic methods;image enhancement;image recognition","MidLevel":"graphics;computer vision;human factors;other","HighLevel":"end users and user experience;technology;other","Venue":"Lect. Notes Electr. Eng.","Top20AbsAndTags":[98,76,438,290,194,461,453,178,456,345,24,379,132,473,139,292,430,155,487,435],"Top20Abs":[98,76,453,178,438,24,290,345,194,292,461,473,379,430,435,456,139,132,155,301],"Top20Tags":[487,456,438,24,155,446,156,461,194,49,187,470,215,162,323,413,283,183,2,469],"Citation":"Liang, H., Wang, Z., & Li, Y. (2023). Improvement and Application of Image Segmentation Algorithm for Outdoor Augmented Reality. Proceedings of the International Conference on Internet of Things, Communication and Intelligent Technology, 366\u2013376. https:\/\/doi.org\/10.1007\/978-981-99-0416-7_37\n"},{"Title":"Explainable Human-Robot Training and Cooperation with Augmented Reality","DOI":"10.1145\/3544549.3583889","Publication year":2023,"Abstract":"The current spread of social and assistive robotics applications is increasingly highlighting the need for robots that can be easily taught and interacted with, even by users with no technical background. Still, it is often difficult to grasp what such robots know or to assess if a correct representation of the task is being formed. Augmented Reality (AR) has the potential to bridge this gap. We demonstrate three use cases where AR design elements enhance the explainability and efficiency of human-robot interaction: 1) a human teaching a robot some simple kitchen tasks by demonstration, 2) the robot showing its plan for solving novel tasks in AR to a human for validation, and 3) a robot communicating its intentions via AR while assisting people with limited mobility during daily activities.","LowLevel":"human-robot interaction;mobile robots","MidLevel":"robotics","HighLevel":"technology","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[174,205,255,269,352,371,312,159,322,70,253,195,487,122,399,142,45,214,425,407],"Top20Abs":[174,371,269,312,205,352,322,195,487,159,142,70,214,122,307,255,45,425,399,93],"Top20Tags":[255,205,407,70,174,159,352,435,60,399,322,122,396,17,379,269,120,381,401,253],"Citation":"Wang, C., Belardinelli, A., Hasler, S., Stouraitis, T., Tanneberg, D., & Gienger, M. (2023). Explainable Human-Robot Training and Cooperation with Augmented Reality. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583889\n"},{"Title":"An Augmented Reality based Intelligent Precision Agriculture using Cascade Advancement Technique","DOI":"10.1109\/ICOEI56765.2023.10125750","Publication year":2023,"Abstract":"Agriculture is the backbone of developing countries and it is very important to the country's economy. Therefore, automation must be used in agriculture to solve the issues. The proposed system is based on augmented reality intelligent precision agriculture using the smart sensor. It consists of the hardware ESP32 microcontroller, DHT11 sensor, and Load, and the software unity hub in Augmented Reality technology and Blynk application is used. The proposed system uses a deep learning algorithm characterized by 4 process categories 1) sensor interface, 2) wireless transmission, 3) data processing and 4) data monitoring and controlling. Agriculture characteristics are mostly maintained in the IoT platform, and the AR controls the water outlet during irrigation. To improve system performance and reach maximum efficiency, it is helpful for farmers to maintain irrigation properly. This leads to alternatives that may be both effective and practical.","LowLevel":"agriculture;deep learning (artificial intelligence);intelligent sensors;irrigation;microcontrollers","MidLevel":"artificial intelligence;farming and natural science;medical;liberal arts;sensors;input;human-computer interaction;other","HighLevel":"industries;technology;other;end users and user experience","Venue":"2023 7th International Conference on Trends in Electronics and Informatics (ICOEI)","Top20AbsAndTags":[287,129,220,160,361,445,376,182,412,175,465,178,381,345,101,163,401,266,157,120],"Top20Abs":[287,129,220,361,376,178,465,120,182,157,445,412,395,345,266,163,87,401,175,38],"Top20Tags":[308,415,160,428,58,95,255,387,425,271,115,379,339,254,81,79,381,325,273,251],"Citation":"Ulagammai, M., & Moorthy, R. N. (2023). An Augmented Reality based Intelligent Precision Agriculture using Cascade Advancement Technique. 2023 7th International Conference on Trends in Electronics and Informatics (ICOEI). https:\/\/doi.org\/10.1109\/icoei56765.2023.10125750\n"},{"Title":"An Efficient Class Room Teaching Learning Method Using Augmented Reality","DOI":"10.1109\/ICACCS57279.2023.10113096","Publication year":2023,"Abstract":"Augmented Reality (AR), a unique method of integrating the virtual world into the real world, has the potential to increase academic attainment in the classroom. This research work focuses on developing and evaluating a strategy for enhancing student education with AR in the classroom. AR enables unique human-computer interactions in real time between the physical and digital worlds. The effectiveness of AR in the classroom will depend on its development, deployment, and integration into both standard and nontraditional teaching environments. Throughout the creation and implementation of an AR classroom, collaborative learning practices and other methodologies were taken into account. Collaboration occurs when two or more individuals work together, share information, and gain insights from one another. This research offers a succinct summary of the promise and challenges of adopting AR to transform the classroom.","LowLevel":"computer aided instruction;groupware;teaching","MidLevel":"education;collaboration;training","HighLevel":"industries;use cases","Venue":"2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)","Top20AbsAndTags":[163,439,134,366,136,103,363,342,267,246,414,440,87,320,93,51,110,82,62,57],"Top20Abs":[163,366,134,439,103,440,110,342,136,267,93,27,180,87,246,49,363,139,62,82],"Top20Tags":[33,363,125,204,164,29,51,303,372,93,114,222,3,439,304,246,239,82,279,87],"Citation":"Valluru, D., Mustafa, M. A., Jasim, H. Y., Srikanth, K., RajaRao, M. V. L. N., & Sreedhar, P. S. S. (2023). An Efficient Class Room Teaching Learning Method Using Augmented Reality. 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS). https:\/\/doi.org\/10.1109\/icaccs57279.2023.10113096\n"},{"Title":"Be our guest: intercultural heritage exchange through augmented reality (AR)","DOI":"10.1145\/3544548.3581005","Publication year":2023,"Abstract":"This paper explores how interactive applications can help mitigate the adversity of facing cultural differences between migrants and the host community, and between migrants of diverse backgrounds to foster intercultural exchange. Based on literature about situated cognition, immersive theater, and affordance, we designed and built Be Our Guest: an augmented reality application where a user is invited to the houses of people from different cultures and is asked to help with one of their cultural rituals around simple everyday objects. We detail the various phases we took to collect the cultural stories and construct the application. We then report the results of a user study with the developed application. Our findings show that participants were easily immersed in the augmented space due to the app's narrative, visuals, and interactive nature. Moreover, they enjoyed exploring cultural rituals, including their own, and felt more confident connecting with people from other cultures.","LowLevel":"history","MidLevel":"liberal arts","HighLevel":"industries","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[6,105,302,14,145,236,31,109,267,96,219,275,218,30,400,153,27,292,207,28],"Top20Abs":[6,14,105,302,31,145,267,96,109,30,400,207,153,275,219,236,28,73,292,218],"Top20Tags":[302,6,197,218,236,145,105,109,10,405,319,349,27,229,112,3,77,127,275,392],"Citation":"Sabie, D., Sheta, H., Ferdous, H. S., Kopalakrishnan, V., & Ahmed, S. I. (2023). Be Our Guest: Intercultural Heritage Exchange through Augmented Reality (AR). Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581005\n"},{"Title":"User-Driven Constraints for Layout Optimisation in Augmented Reality","DOI":"10.1145\/3544548.3580873","Publication year":2023,"Abstract":"Automatic layout optimisation allows users to arrange augmented reality content in the real-world environment without the need for tedious manual interactions. This optimisation is often based on modelling the intended content placement as constraints, defined as cost functions. Then, applying a cost minimization algorithm leads to a desirable placement. However, such an approach is limited by the lack of user control over the optimisation results. In this paper we explore the concept of user-driven constraints for augmented reality layout optimisation. With our approach users can define and set up their own constraints directly within the real-world environment. We first present a design space composed of three dimensions: the constraints, the regions of interest and the constraint parameters. Then we explore which input gestures can be employed to define the user-driven constraints of our design space through a user elicitation study. Using the results of the study, we propose a holistic system design and implementation demonstrating our user-driven constraints, which we evaluate in a final user study where participants had to create several constraints at the same time to arrange a set of virtual contents.","LowLevel":"minimisation","MidLevel":"device energy management","HighLevel":"displays","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[365,440,208,382,243,53,419,79,195,118,271,189,153,104,224,267,0,166,410,304],"Top20Abs":[365,208,440,419,382,243,271,153,79,53,118,104,224,189,304,195,0,136,410,166],"Top20Tags":[350,28,331,266,382,104,267,121,36,377,243,254,346,282,401,264,452,16,195,9],"Citation":"Niyazov, A., Ens, B., Satriadi, K. A., Mellado, N., Barthe, L., Dwyer, T., & Serrano, M. (2023). User-Driven Constraints for Layout Optimisation in Augmented Reality. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580873\n"},{"Title":"How mobile augmented reality digitally transforms the retail sector: examining trust in augmented reality apps and online\/offline store patronage intention","DOI":"10.1108\/JFMM-12-2020-0273","Publication year":2023,"Abstract":"&lt;b&gt;Purpose &lt;\/b&gt;The purpose of the study was to identify (1) whether aspects of expectancy-value judgments (EVJ) of uses and gratifications, such as novelty, fashion\/status, sociability and relaxation, influenced trust in augmented reality (AR) apps; (2) whether trust in AR apps influenced usage intention toward AR apps and online\/offline store patronage intention and (3) the moderating effect of consumer self-determination. &lt;b&gt;Design\/methodology\/approach &lt;\/b&gt;Mobile users (&lt;i&gt;n&lt;\/i&gt; = 630) were drawn from a USA market research company. The proposed model was tested by structural equation modeling with maximum likelihood estimation. &lt;b&gt;Findings &lt;\/b&gt;The study found that trust in AR apps was a determinant of usage intention toward AR apps and online\/offline store patronage intention. Novelty and fashion\/status for EVJs of uses and gratifications affected trust in AR apps. Sociability for EVJs of uses and gratifications negatively affected trust in AR apps. Users' self-determination moderated the influence of users' trust in AR apps on usage intention toward AR apps and online\/offline store patronage intention. &lt;b&gt;Originality\/value &lt;\/b&gt;First, the study elaborates on the impacts of the underlying aspects of an EVJ model of uses and gratifications regarding AR apps on trust in AR apps and EVJ model's influence on usage intention toward AR apps and online\/offline store patronage intention. Second, the results of the study suggest useful strategies involved in the development of consumer-driven AR apps that satisfy users' needs and desires.","LowLevel":"consumer behaviour;customer satisfaction;human factors;market research;maximum likelihood estimation;mobile computing;statistical analysis","MidLevel":"education;human factors;telecommunication;medical;data;other;sales and marketing","HighLevel":"end users and user experience;business;industries;technology;other","Venue":"J. Fash. Mark. Manag., Int. J. (UK)","Top20AbsAndTags":[40,33,390,378,449,146,37,102,151,42,156,91,62,57,142,359,237,30,410,321],"Top20Abs":[40,33,390,378,449,146,37,151,102,156,42,91,57,62,142,237,30,321,259,359],"Top20Tags":[40,102,142,333,368,160,36,147,410,195,22,185,339,8,164,327,93,139,359,320],"Citation":"Kang, J.-Y. M., Kim, J.-E., Lee, J. Y., & Lin, S. H. (2022). How mobile augmented reality digitally transforms the retail sector: examining trust in augmented reality apps and online\/offline store patronage intention. Journal of Fashion Marketing and Management: An International Journal, 27(1), 161\u2013181. https:\/\/doi.org\/10.1108\/jfmm-12-2020-0273\n"},{"Title":"MOFA: Exploring Asymmetric Mixed Reality Design Strategy for Co-located Multiplayer Between Handheld and Head-mounted Augmented Reality","DOI":"10.1145\/3544549.3583935","Publication year":2023,"Abstract":"For co-located multiplayer asymmetric Mixed Reality (MR) scenarios involving both Handheld Augmented Reality (HAR) and head-mounted Stereoscopic Augmented Reality (StAR) devices, we propose a design strategy to distinguish the roles of different players based on the affordances, limitations, and capabilities of these two kinds of AR devices. Specifically, the roles include: a HAR player as a third-person passive participant (termed spectator), a third-person active participant (termed puppeteer), a first-person participant, and a first-person partial-information participant; and a StAR player as a first-person participant and a first-person partial-information participant. In order to explore this concept, we designed four multiplayer MR game prototypes: The Duck, The Ghost, The Dragon, and The Duel to demonstrate strategies for creating interesting, engaging, and strategic cooperative experiences between players.","LowLevel":"computer games;human computer interaction;stereo image processing","MidLevel":"liberal arts;computer vision;human-computer interaction;data","HighLevel":"end users and user experience;technology;industries","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[338,27,79,283,59,14,125,397,237,164,439,170,211,329,351,119,424,418,111,310],"Top20Abs":[338,27,283,14,170,111,59,79,397,164,439,237,329,211,418,125,395,246,424,292],"Top20Tags":[72,59,204,351,302,183,414,79,80,180,249,339,254,410,314,97,415,53,279,164],"Citation":"Hu, B., Zhang, Y., Hao, S., & Tao, Y. (2023). MOFA: Exploring Asymmetric Mixed Reality Design Strategy for Co-located Multiplayer Between Handheld and Head-mounted Augmented Reality. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583935\n"},{"Title":"Lessons learned from human pose interaction in an industrial spatial augmented reality application","DOI":"10.1016\/j.procs.2022.12.288","Publication year":2023,"Abstract":"This paper is a technical description of a novel Augmented Reality application in the industrial domain of furniture production. In the presented case, workers suffered from high cognitive load in doing end-of-line quality inspection and individual handling of a high variety of products. The proposed solution consists of a Spatial Augmented Reality system, where a projector directly displays information on the product to assist the worker. At the same time results of an in-line quality inspections are shown which can be interactively modified through human gestures. The main contribution of this work is two-fold: (1) in contrast to other industrial augmented reality applications the described one technically builds upon a deep neural net based pose estimation. This allows a seamless interaction with the system and tracking of human actions rather than deriving them from the current state of the work piece, (2) the paper recapitulates experiences and results of the approach with a focus on lessons learned for using human pose estimation in smart production. All rights reserved Elsevier.","LowLevel":"furniture;gesture recognition;pose estimation;production engineering computing","MidLevel":"human factors;engineering;graphics;construction;input;manufacturing","HighLevel":"industries;technology;end users and user experience","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[311,345,387,84,45,370,174,360,194,205,411,122,381,255,283,160,425,388,48,68],"Top20Abs":[345,311,387,370,205,255,174,411,45,360,295,84,322,122,194,425,429,160,388,131],"Top20Tags":[370,388,407,68,387,419,122,48,205,360,381,411,45,77,84,159,198,181,179,449],"Citation":"St\u00fcbl, G., Heindl, C., Ebenhofer, G., Bauer, H., & Pichler, A. (2023). Lessons Learned from Human Pose Interaction in an Industrial Spatial Augmented Reality Application. Procedia Computer Science, 217, 912\u2013917. https:\/\/doi.org\/10.1016\/j.procs.2022.12.288\n"},{"Title":"Exploring Augmented Reality Waste Data Representations for Eco Feedback.","DOI":"10.1145\/3544549.3583905","Publication year":2023,"Abstract":"In this demo, we show how Augmented Reality can be used to visualize waste accumulation data in an engaging and visceral way. The negative impact humans have on the environment is partly caused by thoughtless consumption leading to unnecessary waste. A likely contributing factor is the relative invisibility of waste: waste produced by individuals is either out of their sight or quickly taken away. Nevertheless, waste disposal systems sometimes break down, creating natural information displays of waste production that can have educational value. We take inspiration from such natural displays and introduce a class of situated visualizations we call augmented-reality waste accumulation visualizations or ARwavs, which are literal representations of waste data embedded in users' familiar environment. We implemented examples of ARwavs and will present them at the venue.","LowLevel":"data visualization;environmental science computing;waste disposal","MidLevel":"data;other;engineering","HighLevel":"technology;other","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[85,342,4,178,80,440,432,148,7,424,248,157,56,234,411,89,184,166,22,105],"Top20Abs":[85,342,4,178,440,80,7,432,424,56,248,148,184,12,129,166,361,336,89,276],"Top20Tags":[67,85,4,148,134,42,136,376,80,196,101,410,309,411,234,22,392,121,418,182],"Citation":"Assor, A., Prouzeau, A., Dragicevic, P., & Hachet, M. (2023). Exploring Augmented Reality Waste Data Representations for Eco Feedback. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3583905\n"},{"Title":"ARephotography: Revisiting Historical Photographs using Augmented Reality","DOI":"10.1145\/3544549.3585646","Publication year":2023,"Abstract":"Augmented Reality (AR) opens up new possibilities for interactive experiences which can be used in a variety of circumstances. Rephotography is a photo technique commonly presented on dedicated internet pages that align a past view with a current photo, allowing you to have a comparative view of the past with the present. This project aims to combine these two concepts to create AR experiences where you can view buildings and street views from historical photography seamlessly embedded in the present environment. We report on our automated pipeline that can take a historical photograph of a building and produces a textured 3D model that can be placed in AR over the current view of the building using techniques from machine learning while also reporting on first feedback from a preliminary user study.","LowLevel":"history;image representation;internet;photography;solid modelling","MidLevel":"manufacturing;liberal arts;computer vision;networks","HighLevel":"industries;technology","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[243,130,110,340,448,414,45,207,146,87,82,163,159,184,53,50,109,303,189,0],"Top20Abs":[243,130,45,87,110,207,414,82,184,146,41,340,303,0,63,189,53,50,411,389],"Top20Tags":[16,275,330,163,251,14,181,159,172,110,376,218,268,302,209,220,451,327,244,236],"Citation":"Hasselman, T., Lo, W. H., Langlotz, T., & Zollmann, S. (2023). ARephotography: Revisiting Historical Photographs using Augmented Reality. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585646\n"},{"Title":"The validity of markerless augmented reality-based learning media on the concept of cell organelle","DOI":"10.1063\/5.0105748","Publication year":2023,"Abstract":"The use of technology today is an integral part of education, thus changing the way students learn. This study aims to analyze the feasibility of markerless augmented reality-based learning media on the concept of cell organelles based on media analysis. The validity of the learning media by material experts, media experts, biology teachers, and readability tests by students. The validation and readability tests aim to get input, recommendations for improvement, and an assessment of the developed media. The results showed that the validation test results of markerless augmented reality-based learning media on the concept of cell organelles had valid qualifications with an average of 0.89 in.the high category. The student's readability test showed that it was feasible, with a high interpretation of 88.19%. Based on this, the markerless augmented reality-based learning media on the concept of cell organelles is valid and feasible as a learning media. [The copyright for the referenced work is owned by Author(s). Copies of full-text articles should only be made or obtained from the publisher or authorized sources.]","LowLevel":"biology computing;computer aided instruction","MidLevel":"training;medical","HighLevel":"industries;use cases","Venue":"AIP Conf. Proc. (USA)","Top20AbsAndTags":[77,114,262,39,163,33,320,145,369,52,306,134,82,185,244,150,374,112,331,136],"Top20Abs":[77,114,39,262,163,33,369,306,52,145,320,82,150,185,331,134,374,244,136,188],"Top20Tags":[77,204,376,93,33,262,47,34,145,405,367,117,3,82,320,17,253,140,23,222],"Citation":"Ihsan, M., Sa\u2019adah, S., & Maspupah, M. (2023). The validity of markerless augmented reality-based learning media on the concept of cell organelle. THE 3RD INTERNATIONAL CONFERENCE ON SCIENCE, MATHEMATICS, ENVIRONMENT, AND EDUCATION: Flexibility in Research and Innovation on Science, Mathematics, Environment, and Education for Sustainable Development. https:\/\/doi.org\/10.1063\/5.0105748\n"},{"Title":"Mobile Augmented Reality Shopping System","DOI":"10.1109\/SoutheastCon51012.2023.10115069","Publication year":2023,"Abstract":"This document serves as an extended abstract to detail the architecture for an augmented reality shopping application under current development for a thesis. The system consists of a progressive web app client that presents the augmented view to the user and performs object tracking, and a cloud server that performs object detection, localization, and the necessary product information retrieval and analysis.","LowLevel":"cloud computing;information retrieval;mobile computing;object detection;object tracking","MidLevel":"telecommunication;computer vision;data;networks","HighLevel":"industries;technology","Venue":"SoutheastCon 2023","Top20AbsAndTags":[5,116,309,327,96,435,79,289,139,194,123,346,267,429,105,10,370,178,89,159],"Top20Abs":[5,116,309,139,194,435,96,327,429,316,415,79,41,12,123,400,10,487,370,105],"Top20Tags":[5,111,151,327,13,359,280,346,271,226,376,289,164,220,116,159,298,129,93,115],"Citation":"Roche, C., & Hamam, A. (2023). Mobile Augmented Reality Shopping System. SoutheastCon 2023. https:\/\/doi.org\/10.1109\/southeastcon51012.2023.10115069\n"},{"Title":"Pearl: Physical Environment based Augmented Reality Lenses for In-Situ Human Movement Analysis","DOI":"10.1145\/3544548.3580715","Publication year":2023,"Abstract":"This paper presents Pearl, a mixed-reality approach for the analysis of human movement data in situ. As the physical environment shapes human motion and behavior, the analysis of such motion can benefit from the direct inclusion of the environment in the analytical process. We present methods for exploring movement data in relation to surrounding regions of interest, such as objects, furniture, and architectural elements. We introduce concepts for selecting and filtering data through direct interaction with the environment, and a suite of visualizations for revealing aggregated and emergent spatial and temporal relations. More sophisticated analysis is supported through complex queries comprising multiple regions of interest. To illustrate the potential of Pearl, we developed an Augmented Reality-based prototype and conducted expert review sessions and scenario walkthroughs in a simulated exhibition. Our contribution lays the foundation for leveraging the physical environment in the in-situ analysis of movement data.","LowLevel":"other","MidLevel":"other","HighLevel":"other","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[148,440,178,80,412,221,107,439,224,303,142,68,479,411,374,50,458,198,4,424],"Top20Abs":[148,440,178,412,479,80,4,221,224,396,107,142,303,486,439,123,375,424,432,205],"Top20Tags":[400,329,458,365,407,226,440,69,325,198,187,239,462,200,439,461,276,73,409,27],"Citation":"Luo, W., Yu, Z., Rzayev, R., Satkowski, M., Gumhold, S., McGinity, M., & Dachselt, R. (2023). Pearl: Physical Environment based Augmented Reality Lenses for In-Situ Human Movement Analysis. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580715\n"},{"Title":"How Space is Told: Linking Trajectory, Narrative, and Intent in Augmented Reality Storytelling for Cultural Heritage Sites","DOI":"10.1145\/3544548.3581414","Publication year":2023,"Abstract":"We report on a qualitative study in which 22 participants created Augmented Reality (AR) stories for outdoor cultural heritage sites. As storytelling is a crucial strategy for AR content aimed at providing meaningful experiences, the emphasis has been on what storytelling does, rather than how it is done, the end user's needs prioritized over the author's. To address this imbalance, we identify how recurring patterns in the spatial trajectories and narrative compositions of AR stories for cultural heritage sites are linked to the author's intent and creative process: While authors tend to bind story arcs tightly to confined trajectories for narrative delivery, the need for spatial exploration results in thematic content mapped loosely onto encompassing trajectories. Based on our analysis, we present design recommendations for site-specific AR storytelling tools that can support authors in delivering their intent while leveraging the placeness of cultural heritage sites as a creative resource.","LowLevel":"history;multimedia computing","MidLevel":"education;liberal arts","HighLevel":"industries","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[16,14,302,31,229,218,292,109,197,188,105,236,418,191,155,329,392,102,135,349],"Top20Abs":[14,16,31,302,218,229,292,197,418,109,188,105,191,155,329,392,349,236,102,135],"Top20Tags":[16,229,302,287,109,218,197,39,284,236,405,105,10,89,262,3,319,349,27,77],"Citation":"Shin, J.-E., & Woo, W. (2023). How Space is Told. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581414\n"},{"Title":"Augmented Reality Shopping System and Experience: Overview of the Literature","DOI":"10.1109\/SoutheastCon51012.2023.10115104","Publication year":2023,"Abstract":"This paper reviews literature related to the architecture for a proposed augmented reality shopping application under current development. The system consists of a progressive web app client that renders augmentations for the user and performs tracking, and a cloud server that performs object detection, localization, and product information retrieval and analysis. This review provides an overview of past cloud-based augmented reality applications and identifies design factors for the proposed system.","LowLevel":"cloud computing;information retrieval;mobile computing;object detection","MidLevel":"telecommunication;computer vision;data;networks","HighLevel":"industries;technology","Venue":"SoutheastCon 2023","Top20AbsAndTags":[8,298,116,451,427,98,350,159,92,79,376,289,340,49,321,93,345,327,178,175],"Top20Abs":[8,49,116,41,451,376,92,12,93,175,139,159,345,350,289,298,316,327,98,340],"Top20Tags":[8,298,98,427,143,79,48,116,358,111,274,220,379,438,226,81,159,370,327,172],"Citation":"Roche, C., & Hamam, A. (2023). Augmented Reality Shopping System and Experience: Overview of the Literature. SoutheastCon 2023. https:\/\/doi.org\/10.1109\/southeastcon51012.2023.10115104\n"},{"Title":"Impacts of augmented reality on foreign language teaching: a case study of Persian language","DOI":"10.1007\/s11042-022-13370-5","Publication year":2023,"Abstract":"The use of information technology in the field of foreign language teaching as an auxiliary tool is very important. In a foreign language classroom, place is just an abstract concept; where the language is separated from the community, culture and places in which it is used. Augmented reality is a technology in which virtual components are simultaneously combined with the real environment. Our aim in this study is to investigate the effects of location-based augmented reality in teaching Persian as a foreign language. In this study, after consulting with professors in the field of Persian language teaching and reviewing similar researches, we came to the conclusion that nothing has been done to teach Persian language using augmented reality. Therefore, a Persian game based on augmented reality was designed and implemented and then evaluated. For evaluation, two methods have been used; the user and the heuristic evaluation. Experts in the field of Persian language teaching, human-computer interaction and a number of language learners participated in the evaluation. Their feedback shows that the use of augmented reality increases satisfaction, enthusiasm and interaction with the environment and people, and also makes the process of learning and memorizing concepts more efficient.","LowLevel":"computer aided instruction;teaching","MidLevel":"education;training","HighLevel":"industries;use cases","Venue":"Multimed. Tools Appl. (Germany)","Top20AbsAndTags":[394,87,414,114,372,93,17,457,136,400,164,207,246,112,342,52,363,320,434,185],"Top20Abs":[394,207,93,400,457,87,114,414,45,17,372,434,353,342,203,312,218,366,246,79],"Top20Tags":[197,164,147,82,33,56,320,405,93,89,222,363,51,17,39,390,87,9,232,204],"Citation":"Mozaffari, S., & Hamidi, H. R. (2022). Impacts of augmented reality on foreign language teaching: a case study of Persian language. Multimedia Tools and Applications, 82(3), 4735\u20134748. https:\/\/doi.org\/10.1007\/s11042-022-13370-5\n"},{"Title":"Introduction to the application of Augmented Reality technology in the digital manufacturing process","DOI":"10.1117\/12.2664386","Publication year":2023,"Abstract":"Augmented Reality (AR) technology can skillfully blend virtual information with the real world, so it can greatly help the development of the manufacturing industry. The AR technology in this paper is mainly focused on 3D laser projection technology. This paper first describes the application background of AR technology. And then the basic principles of AR are analyzed and the key technologies such as virtual-real mapping and so on are systematically sorted out. On this basis, the basic composition of the AR system is described. And then, the application area is set out in the aviation manufacturing industry, aerospace manufacturing industry, shipbuilding, and so on. At last, we design an experiment to verify the laser spot error distribution of the calibration result point. &copy; 2023 SPIE.","LowLevel":"other","MidLevel":"other","HighLevel":"other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[270,139,179,159,433,436,51,194,273,437,486,49,440,226,252,41,136,132,44,57],"Top20Abs":[159,444,179,139,51,433,49,270,273,41,429,110,315,437,137,281,87,44,252,135],"Top20Tags":[270,440,449,194,191,486,471,187,132,212,458,18,179,444,312,137,7,462,120,1],"Citation":"Zhao, Z., Jia, Z., & Li, Y. (2023). Introduction to the application of augmented reality technology in the digital manufacturing process. Ninth Symposium on Novel Photoelectronic Detection Technology and Applications. https:\/\/doi.org\/10.1117\/12.2664386\n"},{"Title":"Augmented Reality System as a 5.0 Marketing Strategy in Restaurants: A Case Study in Ambato Ecuador","DOI":"10.1007\/978-3-031-30592-4_10","Publication year":2023,"Abstract":"The Covid-19 pandemic affected several productive sectors. However, the tourism sector was and continues to be one of the most damaged. Companies were forced to evolve technologically in order to cope with this condition. More than 90% of businesses lost their investment, as well as their human resources. After more than two and a half years after this crisis&rsquo;s beginning, establishments are still looking for a solution to their financial debacle. This article presents the development and implementation of an augmented reality system as a 5.0 marketing strategy focused on a restaurant in the city of Ambato, Ecuador. The results showed encouraging data. The evaluation of user experience through the System Usability Scale showed a value above 70. The comparison between before and after the implementation of this system showed a percentage increase in sales of 47.45%. Finally, the T-student test for independent samples was used to verify the existence of a statistically significant difference. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"commerce;strategic planning;tourism","MidLevel":"cultural heritage;business planning and management;sales and marketing","HighLevel":"industries;business","Venue":"Lect. Notes Networks Syst.","Top20AbsAndTags":[155,361,1,391,171,315,390,90,336,368,363,454,320,188,333,30,289,161,414,312],"Top20Abs":[361,315,1,155,306,368,454,363,289,90,320,414,171,443,188,30,281,391,194,70],"Top20Tags":[1,184,155,156,187,333,472,471,361,315,161,336,373,135,391,171,229,137,40,355],"Citation":"Paredes, P.-R., & Ballesteros-Lopez, L.-G. (2023). Augmented Reality System as a 5.0 Marketing Strategy in Restaurants: A Case Study in Ambato Ecuador. Lecture Notes in Networks and Systems, 127\u2013137. https:\/\/doi.org\/10.1007\/978-3-031-30592-4_10\n"},{"Title":"Supporting Piggybacked Co-Located Leisure Activities via Augmented Reality","DOI":"10.1145\/3544548.3580833","Publication year":2023,"Abstract":"Technology, especially the smartphone, is villainized for taking meaning and time away from in-person interactions and secluding people into \"digital bubbles\". We believe this is not an intrinsic property of digital gadgets, but evidence of a lack of imagination in technology design. Leveraging augmented reality (AR) toward this end allows us to create experiences for multiple people, their pets, and their environments. In this work, we explore the design of AR technology that \"piggybacks\" on everyday leisure to foster co-located interactions among close ties (with other people and pets). We designed, developed, and deployed three such AR applications, and evaluated them through a 41-participant and 19-pet user study. We gained key insights about the ability of AR to spur and enrich interaction in new channels, the importance of customization, and the challenges of designing for the physical aspects of AR devices (e.g., holding smartphones). These insights guide design implications for the novel research space of co-located AR.","LowLevel":"humanities;smartphones","MidLevel":"telecommunication;liberal arts","HighLevel":"industries","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[93,79,100,219,216,419,224,200,336,26,125,45,121,248,348,424,304,53,161,17],"Top20Abs":[79,100,224,93,200,45,336,26,219,304,125,87,419,17,53,161,104,135,121,58],"Top20Tags":[93,129,339,348,221,82,325,77,319,216,105,424,220,103,309,16,387,182,140,240],"Citation":"Reig, S., Principe Cruz, E., Powers, M. M., He, J., Chong, T., Tham, Y. J., Kratz, S., Robinson, A., Smith, B. A., Vaish, R., & Monroy-Hern\u00e1ndez, A. (2023). Supporting Piggybacked Co-Located Leisure Activities via Augmented Reality. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580833\n"},{"Title":"Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching","DOI":"10.1145\/3544548.3581449","Publication year":2023,"Abstract":"This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.","LowLevel":"computer vision;gesture recognition;human computer interaction;user interfaces","MidLevel":"computer vision;human-computer interaction;human factors;input","HighLevel":"end users and user experience;technology","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[314,135,224,55,415,125,421,410,49,60,110,234,142,155,179,104,116,45,42,419],"Top20Abs":[314,135,55,224,415,49,421,410,234,60,155,104,42,125,45,27,116,41,110,200],"Top20Tags":[366,242,415,66,180,72,418,419,329,421,382,365,198,280,110,118,36,311,250,338],"Citation":"Monteiro, K., Vatsal, R., Chulpongsatorn, N., Parnami, A., & Suzuki, R. (2023). Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581449\n"},{"Title":"Meta-analysis of augmented reality marketing","DOI":"10.1108\/MIP-06-2022-0221","Publication year":2023,"Abstract":"&lt;b&gt;Purpose &lt;\/b&gt;Amidst the ambiguity about the impact of augmented reality (AR) attributes on hedonic or utilitarian values, the present study aims to understand what AR attributes create hedonic and utilitarian values and how their interaction determines consumers' behavioral intention. &lt;b&gt;Design\/methodology\/approach &lt;\/b&gt;The study synthesizes the results of 19 quantitative studies on AR marketing by using the meta-analysis technique. &lt;b&gt;Findings &lt;\/b&gt;The findings reveal that interactivity and augmentation are salient AR attributes that offer users both hedonic and utilitarian values. They are instrumental in fostering users' behavioral intention. However, interactivity does not have any direct influence on the behavioral intentions. &lt;b&gt;Originality\/value &lt;\/b&gt;Being one of the first meta-analyses on AR marketing; theoretically, it synthesizes the statistical data of the state of art literature on AR marketing. The results of the study would allow AR practitioners to decide on their AR marketing related activities in a better way.","LowLevel":"consumer behaviour;organisational aspects;statistical analysis","MidLevel":"business planning and management;data;sales and marketing","HighLevel":"technology;business","Venue":"Mark. Intell. Plan. (UK)","Top20AbsAndTags":[33,13,390,378,449,146,37,151,368,102,91,237,62,229,321,259,30,45,1,57],"Top20Abs":[33,13,390,449,378,146,37,151,91,368,102,237,62,259,229,321,30,45,49,180],"Top20Tags":[333,13,160,368,44,102,168,185,355,78,378,449,25,346,1,471,229,257,156,418],"Citation":"Kumar, H., Gupta, P., & Chauhan, S. (2022). Meta-analysis of augmented reality marketing. Marketing Intelligence &amp; Planning, 41(1), 110\u2013123. https:\/\/doi.org\/10.1108\/mip-06-2022-0221\n"},{"Title":"A Qualitative Approach to the Electromagnetic Induction Fostered by Augmented Reality","DOI":"10.1119\/5.0062131","Publication year":2023,"Abstract":"In physics education, the topic of electromagnetic induction is an important but also challenging topic for many students. The early introduction of formulae, e.g., Faraday's law of induction, seems to hinder rather than to foster the understanding of the topic's underlying principles. In this paper, we present the basic idea for a teaching concept that can be helpful for a qualitative understanding of electromagnetic induction. To support this teaching concept further, we have developed an application for a tablet computer following the augmented reality approach. The tablet measures the magnetic field strength of a Helmholtz coil and superimposes the corresponding number of virtual field lines on the induction coil. [The copyright for the referenced work is owned by Author(s). Copies of full-text articles should only be made or obtained from the publisher or authorized sources.]","LowLevel":"computer aided instruction;electromagnetic induction;physics education;teaching","MidLevel":"education;training;other;engineering","HighLevel":"industries;technology;use cases;other","Venue":"Phys. Teach. (USA)","Top20AbsAndTags":[414,9,306,245,51,372,77,56,342,136,3,82,134,457,218,112,114,89,363,47],"Top20Abs":[245,9,77,218,414,51,372,56,457,252,316,82,360,3,134,114,306,47,88,196],"Top20Tags":[306,89,342,128,33,136,3,363,82,246,17,87,9,232,204,164,56,372,320,114],"Citation":"Berger, R., & Lensing, P. (2023). A Qualitative Approach to the Electromagnetic Induction Fostered by Augmented Reality. The Physics Teacher, 61(1), 34\u201335. https:\/\/doi.org\/10.1119\/5.0062131\n"},{"Title":"Learning Chemistry with Interactive Simulations: Augmented Reality as Teaching Aid","DOI":"10.1007\/978-3-031-20429-6_48","Publication year":2023,"Abstract":"Augmented Reality (AR) has been identified by educational scientists as a technology with significant potential to improve emotional and cognitive learning outcomes. However, very few papers highlighted the technical process of creating AR applications reserved for education. The following paper proposes a method and framework for how to set up an AR application to teach primary school children the basic forms and shapes of atoms, molecules, and DNA. This framework uses the Unity 3D game engine (GE) with Vuforia SDK (Software Development Kit) packages combined with phone devices or tablets to create an interactive App for AR environments, to enhance the student's vision and understanding of basic chemistry models. We also point out some difficulties in practice. As for those difficulties mentioned, a series of solutions plus further development orientation are put forth.","LowLevel":"cognition;computer aided instruction;computer games;educational institutions;mobile computing;teaching","MidLevel":"education;training;human factors;telecommunication;liberal arts","HighLevel":"industries;use cases;end users and user experience","Venue":"Proceedings of the 2nd International Conference on Emerging Technologies and Intelligent Systems: ICETIS 2022. Lecture Notes in Networks and Systems (573)","Top20AbsAndTags":[392,115,56,83,163,87,103,414,82,136,134,320,188,185,306,207,2,267,184,114],"Top20Abs":[392,115,163,188,207,2,83,87,103,184,267,134,56,161,306,136,185,180,82,62],"Top20Tags":[82,204,164,410,93,147,145,56,196,241,320,222,33,87,17,103,89,414,115,339],"Citation":"Benrahal, M., Bourhim, E. M., Dahane, A., Labti, O., & Akhiate, A. (2022). Learning Chemistry with Interactive Simulations: Augmented Reality as Teaching Aid. Proceedings of the 2nd International Conference on Emerging Technologies and Intelligent Systems, 526\u2013535. https:\/\/doi.org\/10.1007\/978-3-031-20429-6_48\n"},{"Title":"FlowAR: How Different Augmented Reality Visualizations of Online Fitness Videos Support Flow for At-Home Yoga Exercises","DOI":"10.1145\/3544548.3580897","Publication year":2023,"Abstract":"Online fitness video tutorials are an increasingly popular way to stay fit at home without a personal trainer. However, to keep the screen playing the video in view, users typically disrupt their balance and break the motion flow - two main pillars for the correct execution of yoga poses. While past research partially addressed this problem, these approaches supported only a limited view of the instructor and simple movements. To enable the fluid execution of complex full-body yoga exercises, we propose FlowAR, an augmented reality system for home workouts that shows training video tutorials as always-present virtual static and dynamic overlays around the user. We tested different overlay layouts in a study with 16 participants, using motion capture equipment for baseline performance. Then, we iterated the prototype and tested it in a furnished lab simulating home settings with 12 users. Our results highlight the advantages of different visualizations and the system's general applicability.","LowLevel":"pose estimation;video signal processing","MidLevel":"graphics;data;sensors;semiconductors","HighLevel":"technology","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[68,274,189,130,42,243,151,272,366,38,358,10,7,263,4,91,213,232,387,166],"Top20Abs":[68,189,151,272,38,243,42,274,130,366,10,232,109,7,91,0,263,358,15,153],"Top20Tags":[130,300,274,358,48,12,425,387,396,327,381,255,148,76,23,81,428,98,72,403],"Citation":"Jo, H.-Y., Seidel, L., Pahud, M., Sinclair, M., & Bianchi, A. (2023). FlowAR: How Different Augmented Reality Visualizations of Online Fitness Videos Support Flow for At-Home Yoga Exercises. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580897\n"},{"Title":"Improving Learning-based Camera Pose Estimation for Image-based Augmented Reality Applications","DOI":"10.1145\/3544549.3585756","Publication year":2023,"Abstract":"Camera tracking is essential for many augmented reality (AR) applications, such as rendering virtual content on top of a display. While marker-based camera tracking methods can accurately estimate the camera pose, they require pre-placed markers and occupy valuable screen space, potentially impacting the user experience. In contrast, markerless camera tracking methods do not have these limitations but tend to be less stable. In this work, we propose an improved approach for camera tracking that utilizes salient visual features commonly found on websites. We develop robust algorithms for detecting these features and design efficient methods for estimating the camera pose from them. Our approach outperforms state-of-the-art methods in terms of robustness, as demonstrated by our experiments. This work paves the way for a wide range of important AR applications, such as online shopping and interactive AR games.","LowLevel":"cameras;pose estimation","MidLevel":"graphics;input","HighLevel":"technology","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[425,97,41,435,116,119,105,98,81,73,208,318,173,155,316,277,142,243,387,12],"Top20Abs":[425,435,73,97,208,119,41,116,98,316,173,318,81,155,105,243,277,326,142,387],"Top20Tags":[97,98,396,255,425,81,387,356,5,72,41,195,50,12,300,381,73,422,71,148],"Citation":"Cai, E., Rossi, R., & Xiao, C. (2023). Improving Learning-based Camera Pose Estimation for Image-based Augmented Reality Applications. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585756\n"},{"Title":"Virtual and Augmented Reality for Digital Medicine - Design and Implementation of a Hybrid, Interdisciplinary Course for Engineering and Medical Students","DOI":"10.1109\/EDUCON54358.2023.10125163","Publication year":2023,"Abstract":"In the winter semester 2021\/22, the interdisciplinary course \"Virtual and Augmented Reality for Digital Medicine\" was offered. Here, students of medicine, medical technology, psychology as well as computer science were lectured together to merge the expertise from these fields and to promote mutual understanding. After a virtual theory phase, a practical phase took place on site. During the following hands-on phase, students collaborated in two different groups to develop an Augmented Reality (AR) and a Virtual Reality (VR) application with a medical context. After a presentation of the applications in front of an expert audience, the acquired knowledge was assessed by a written digital exam. The course was evaluated very positively by students from all fields and the good exam grades also indicate that the project was a success.","LowLevel":"biomedical education;computer aided instruction;educational courses;medical computing;psychology","MidLevel":"education;medical;training","HighLevel":"industries;use cases","Venue":"2023 IEEE Global Engineering Education Conference (EDUCON)","Top20AbsAndTags":[272,112,372,436,431,414,88,136,82,241,134,320,124,163,204,357,439,252,171,306],"Top20Abs":[112,272,431,372,88,82,436,414,124,234,241,252,163,320,207,136,439,30,134,340],"Top20Tags":[117,262,140,9,439,376,390,154,204,408,33,112,34,93,64,222,134,17,103,88],"Citation":"Eiler, T. J., Schm\u00fccker, V., Gie\u00dfer, C., & Br\u00fcck, R. (2023). Virtual and Augmented Reality for Digital Medicine - Design and Implementation of a Hybrid, Interdisciplinary Course for Engineering and Medical Students. 2023 IEEE Global Engineering Education Conference (EDUCON). https:\/\/doi.org\/10.1109\/educon54358.2023.10125163\n"},{"Title":"Representing Augmented Reality Applications in Systems Modeling Language","DOI":"10.1109\/SysCon53073.2023.10131060","Publication year":2023,"Abstract":"Augmented Reality (AR) devices offer novel capabilities that can be exploited in AR systems to positively impact human-machine interactions in a variety of future-work and education contexts. This paper presents a systems model for a no-code AR systems framework that can be used to create AR applications that present just-in-time informatics to assist and guide users in the completion of complex task sequences while ensuring operator and environment safety. The salient structural and behavioral aspects of the system, and key use cases are modeled using the Systems Modeling Language (SysML). Representative examples of the model are presented using use case, block definition, internal block, activity, and state-machine diagrams. These models offer new insights into how AR capabilities can be integrated with a variety of engineered systems. In the future such SysML models can steer the design of new tools and an ontology to strengthen connections to domain knowledge.","LowLevel":"human computer interaction;ontology;production engineering computing;sysml;unified modeling language","MidLevel":"engineering;web services;manufacturing;human-computer interaction;other","HighLevel":"end users and user experience;technology;industries;other","Venue":"2023 IEEE International Systems Conference (SysCon)","Top20AbsAndTags":[174,179,468,0,12,410,122,175,54,118,62,205,159,135,137,27,311,84,53,229],"Top20Abs":[0,179,174,410,175,54,62,118,27,12,178,135,229,10,468,82,87,137,139,53],"Top20Tags":[388,370,122,205,84,407,468,179,174,411,419,242,37,159,12,360,439,449,77,412],"Citation":"Aoki, E., Tran, B., Tran, V., Soth, K., Thompson, C., Tripathy, S., Chandra, K., & Sastry, S. (2023). Representing Augmented Reality Applications in Systems Modeling Language. 2023 IEEE International Systems Conference (SysCon). https:\/\/doi.org\/10.1109\/syscon53073.2023.10131060\n"},{"Title":"The Effect of Virtual Reality and Augmented Reality on Managing Projects","DOI":"10.1109\/ICBATS57792.2023.10111112","Publication year":2023,"Abstract":"This report highlights how digital disruption and transformation have affected project management. Amazon was selected as the organization, and project management was chosen as the industry. Primary research-based open-end interviews with thirty project managers and secondary research based on a review of past literature were conducted to perform this study. It was found that digital disruption has occurred in the project management industry, which has modified the behaviors, needs, and expectations of people in society, the market, and the industry. It was found that virtual reality (VR) and augmented reality (AR) has disrupted the project management industry. The AR caused a shift in the consumer (sponsor\/client) wants in the project management industry. Customer derives the demand want from. Therefore the businesses and their competitors' activities are modified to adapt the AR and VR-based technologies. AR and VR reduced the project managers' efforts, time, and resources. Moreover, the metaverse is another technology that will cause significant digital disruption and digital transformation in the project management industry.","LowLevel":"business data processing;organisational aspects;project management","MidLevel":"business planning and management;business performance metrics;data","HighLevel":"technology;business","Venue":"2023 International Conference on Business Analytics for Technology and Security (ICBATS)","Top20AbsAndTags":[124,197,184,210,328,377,363,204,454,193,35,414,200,2,355,103,179,215,273,249],"Top20Abs":[197,124,184,454,35,429,414,2,179,1,210,273,377,204,352,467,428,200,103,102],"Top20Tags":[328,363,193,40,64,210,289,146,78,378,256,163,86,355,372,252,233,257,181,216],"Citation":"Khatib, Dr. M. E., Al Falasi, F., Anani, H. A., & Shurrab, W. (2023). The Effect of Virtual Reality and Augmented Reality on Managing Projects. 2023 International Conference on Business Analytics for Technology and Security (ICBATS). https:\/\/doi.org\/10.1109\/icbats57792.2023.10111112\n"},{"Title":"HoloLens augmented reality system for transperineal free-hand prostate procedures","DOI":"10.1117\/1.JMI.10.2.025001","Publication year":2023,"Abstract":"Purpose: An augmented reality (AR) system was developed to facilitate free-hand real-time needle guidance for transperineal prostate (TP) procedures and to overcome the limitations of a traditional guidance grid. Approach: The HoloLens AR system enables the superimposition of annotated anatomy derived from preprocedural volumetric images onto a patient and addresses the most challenging part of free-hand TP procedures by providing real-time needle tip localization and needle depth visualization during insertion. The AR system accuracy, or the image overlay accuracy (n = 56), and needle targeting accuracy (n = 24) were evaluated within a 3D-printed phantom. Three operators each used a planned-path guidance method (n = 4) and free-hand guidance (n = 4) to guide needles into targets in a gel phantom. Placement error was recorded. The feasibility of the system was further evaluated by delivering soft tissue markers into tumors of an anthropomorphic pelvic phantom via the perineum. Results: The image overlay error was 1.29 &plusmn; 0.57 mm, and needle targeting error was 2.13 &plusmn; 0.52 mm. The planned-path guidance placements showed similar error compared to the free-hand guidance (4.14 &plusmn; 1.08 mm versus 4.20 &plusmn; 1.08 mm, p = 0.90). The markers were successfully implanted either into or in close proximity to the target lesion. Conclusions: The HoloLens AR system can provide accurate needle guidance for TP interventions. AR support for free-hand lesion targeting is feasible and may provide more flexibility than grid-based methods, due to the real-time 3D and immersive experience during free-hand TP procedures. &copy; 2023 Published by SPIE.","LowLevel":"errors;medical imaging;phantoms;urology","MidLevel":"medical;human factors","HighLevel":"end users and user experience;industries","Venue":"J. Med. Imaging","Top20AbsAndTags":[191,432,189,488,323,433,0,133,250,415,366,285,152,408,118,345,401,384,470,137],"Top20Abs":[191,189,488,432,323,250,433,133,415,152,0,384,285,345,151,366,408,186,375,255],"Top20Tags":[0,432,470,488,170,453,323,207,138,433,191,285,464,431,463,212,442,480,74,137],"Citation":"Li, M., Mehralivand, S., Xu, S., Varble, N., Bakhutashvili, I., Gurram, S., Pinto, P. A., Choyke, P. L., Wood, B. J., & Turkbey, B. (2023). HoloLens augmented reality system for transperineal free-hand prostate procedures. Journal of Medical Imaging, 10(02). https:\/\/doi.org\/10.1117\/1.jmi.10.2.025001\n"},{"Title":"\"In Your Face!\": Visualizing Fitness Tracker Data in Augmented Reality","DOI":"10.1145\/3544549.3585912","Publication year":2023,"Abstract":"The benefits of augmented reality (AR) have been demonstrated in both medicine and fitness, while its application in areas where these two fields overlap has been barely explored. We argue that AR opens up new opportunities to interact with, understand and share personal health data. To this end, we developed an app prototype that uses a Snapchat-like face filter to visualize personal health data from a fitness tracker in AR. We tested this prototype in two pilot studies and found that AR does have potential in this type of application. We suggest that AR cannot replace the current interfaces of smartwatches and mobile apps, but it can pick up where current technology falls short in creating intrinsic motivation and personal health awareness. We also provide ideas for future work in this direction.","LowLevel":"data visualization;health care;medical computing;mobile computing","MidLevel":"telecommunication;medical;data","HighLevel":"industries;technology","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[93,241,125,102,140,53,64,13,213,82,155,4,328,50,51,139,111,49,100,252],"Top20Abs":[93,125,241,53,213,49,64,102,328,13,159,155,60,82,140,50,111,252,87,62],"Top20Tags":[376,140,410,93,424,22,390,336,82,178,327,102,234,13,196,262,105,117,309,392],"Citation":"Rigling, S., Yu, X., & Sedlmair, M. (2023). \u201cIn Your Face!\u201d: Visualizing Fitness Tracker Data in Augmented Reality. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585912\n"},{"Title":"A New Architecture of Augmented Reality Engine","DOI":"10.1109\/MEEE57080.2023.10127071","Publication year":2023,"Abstract":"Augmented reality (AR) technology has been a rapidly developing research hotspot in the past ten years. AR combines computer-generated virtual scenes with the real world to achieve sensory enhancements beyond reality. As the middleware between the operating system layer and the application layer, the AR engine is a platform-based underlying technology and the cornerstone of building an augmented reality ecosystem. In this paper, a new AR engine architecture is proposed, and the core modules such as architecture composition, data management, algorithm integration, and multi-platform support interfaces are described in detail. Also, Nanhu AR engine (NHAR) is developed based on the new architecture. And an application is developed using NHAR to test marker-based tracking and marker-less tracking function of the developed engine.","LowLevel":"middleware;operating systems","MidLevel":"education;developers","HighLevel":"industries;technology","Venue":"2023 2nd International Conference on Mechatronics and Electrical Engineering (MEEE)","Top20AbsAndTags":[97,48,150,155,116,139,105,49,110,53,2,184,252,267,144,141,163,421,435,51],"Top20Abs":[155,139,116,110,144,48,184,49,252,2,150,97,141,267,123,421,53,5,51,159],"Top20Tags":[97,48,150,49,413,278,199,133,271,329,297,110,36,249,405,232,242,154,56,55],"Citation":"Xu, Z., Wu, S., & Zhang, L. (2023). A New Architecture of Augmented Reality Engine. 2023 2nd International Conference on Mechatronics and Electrical Engineering (MEEE). https:\/\/doi.org\/10.1109\/meee57080.2023.10127071\n"},{"Title":"Ethnomatematics learning media based on augmented reality for learning geometry: A needs analysis","DOI":"10.1063\/5.0105812","Publication year":2023,"Abstract":"Two-dimensional and three-dimensional geometric shapes are often mathematical objects that are difficult for elementary and junior high school students to understand. Using augmented reality technology to visualize it can make it easier for students to learn geometry material. The purpose of this study was to analyze the need for developing learning media based on ethnomathematics with augmented reality. This research is quantitative research. The research subjects were 40 mathematics teachers in Yogyakarta, Indonesia. Subject determination techniques using cluster random sampling in 5 districts in Yogyakarta Province. Research data were collected by giving questionnaires to respondents. The results of this study indicate (1) most of the teachers found that their students had difficulty in learning geometry material; (2) most mathematics teachers rarely use culture as a learning context; (3) all mathematics teachers have never used ethnomathematics-based Augmented reality-assisted learning media; (4) all mathematics teachers are in dire need of learning multimedia innovations to teach geometry material, such as augmented reality; and (5) the development of ethnomathematical AR multimedia based on geometry material can be a solution to create an innovation in mathematics learning that can facilitate students in learning, so as to improve mathematical abilities, make learning more meaningful, and can increase student self-regulated learning during the pandemic.","LowLevel":"computational geometry;computer aided instruction;educational institutions;mathematics computing;multimedia computing;teaching","MidLevel":"education;artificial intelligence;training;engineering;graphics","HighLevel":"industries;technology;use cases","Venue":"AIP Conf. Proc. (USA)","Top20AbsAndTags":[262,163,320,114,82,9,342,369,77,246,306,57,136,207,161,185,103,124,383,87],"Top20Abs":[262,163,114,320,9,82,77,369,342,246,306,207,103,124,88,161,185,136,383,57],"Top20Tags":[136,246,145,284,89,3,82,287,262,33,6,56,391,320,405,372,52,134,51,363],"Citation":"Richardo, R., Abdullah, A. A., Rochmadi, T., Wijaya, A., & Nurkhamid. (2023). Ethnomatematics learning media based on augmented reality for learning geometry: A needs analysis. THE 3RD INTERNATIONAL CONFERENCE ON SCIENCE, MATHEMATICS, ENVIRONMENT, AND EDUCATION: Flexibility in Research and Innovation on Science, Mathematics, Environment, and Education for Sustainable Development. https:\/\/doi.org\/10.1063\/5.0105812\n"},{"Title":"An Optimal Visualization of Traffic System by using Augmented Reality and Virtual Reality","DOI":"10.1109\/ICSMDI57622.2023.00062","Publication year":2023,"Abstract":"This study explains the concept of deadlock by focusing on prevention and avoidance with the help of one of the emerging technologies, Augmented Reality (AR). With the help of AR, people can easily understand this concept. This can be easily explained with the help of real time example of traffic system on the road. To solve this issue, this study has implemented the Bankers Algorithm, which is a Deadlock Prevention Algorithm. The Bankers Algorithm can use its calculation and prevent the occurrence of Deadlock or incase a deadlock is happening it can help to resolve it.","LowLevel":"concurrency control;flexible manufacturing systems","MidLevel":"education;manufacturing;other","HighLevel":"industries;other","Venue":"2023 3rd International Conference on Smart Data Intelligence (ICSMDI)","Top20AbsAndTags":[466,4,16,2,57,62,226,471,93,128,64,435,257,328,222,171,82,190,438,131],"Top20Abs":[4,466,62,16,57,226,328,435,471,222,128,93,438,257,82,152,64,2,178,464],"Top20Tags":[15,282,174,416,449,346,193,311,275,426,354,200,382,388,254,429,64,210,222,10],"Citation":"Dang, R., Krishna, V., Sharma, R., & Kowsigan, M. (2023). An Optimal Visualization of Traffic System by using Augmented Reality and Virtual Reality. 2023 3rd International Conference on Smart Data Intelligence (ICSMDI). https:\/\/doi.org\/10.1109\/icsmdi57622.2023.00062\n"},{"Title":"Augmented Reality-based Indoor Positioning for Smart Home Automations","DOI":"10.1145\/3544549.3585745","Publication year":2023,"Abstract":"Ambient Assisted Living (AAL) has been discussed for some time; however, many systems are not considered as interoperable or user-friendly. This fact is an even more important issue as every interaction is more costly in terms of time and effort for people with disabilities or senior citizens. Therefore, this paper examines the potential of automations that can substitute typical daily interactions in AAL or Smart Home settings in general based on the users' location. Particularly, we suggest the novel approach of using the indoor positioning capabilities of Augmented Reality (AR) head-mounted displays (HMD) to detect, track, and identify residents for the purpose of automatically controlling various Internet of Things (IoT) devices in Smart Homes. An implementation of this feature on an off-the-shelf AR HMD without additional external trackers is demonstrated and the results of an initial feasibility study are presented.","LowLevel":"assisted living;helmet mounted displays;home automation;indoor navigation;internet of things","MidLevel":"internet of things;smart cities;medical;wearables;navigation;manufacturing;networks;display technology","HighLevel":"displays;technology;industries;use cases","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[217,180,364,80,175,224,69,68,129,110,301,279,93,63,266,139,385,310,220,361],"Top20Abs":[180,80,68,217,364,224,129,175,93,385,69,50,0,361,310,448,110,63,376,447],"Top20Tags":[129,123,258,80,286,110,220,31,175,298,271,266,121,364,385,289,279,378,180,359],"Citation":"Schenkluhn, M., Peukert, C., & Weinhardt, C. (2023). Augmented Reality-based Indoor Positioning for Smart Home Automations. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585745\n"},{"Title":"Movement Time for Pointing Tasks in Real and Augmented Reality Environments","DOI":"10.3390\/app13020788","Publication year":2023,"Abstract":"Human-virtual target interactions are becoming more and more common due to the emergence and application of augmented reality (AR) devices. They are different from interacting with real objects. Quantification of movement time (MT) for human-virtual target interactions is essential for AR-based interface\/environment design. This study aims to investigate the motion time when people interact with virtual targets and to compare the differences in motion time between real and AR environments. An experiment was conducted to measure the MT of pointing tasks on the basis of both a physical and a virtual calculator panel. A total of 30 healthy adults, 15 male and 15 female, joined. Each participant performed pointing tasks on both physical and virtual panels with an inclined angle of the panel, hand movement direction, target key, and handedness conditions. The participants wore an AR head piece (Microsoft Hololens 2) when they pointed on the virtual panel. When pointing on the physical panel, the participants pointed on a panel drawn on board. The results showed that the type of panel, inclined angle, gender, and handedness had significant (&lt;i&gt;p&lt;\/i&gt;&lt; 0.0001) effects on the MT. A new finding of this study was that the MT of the pointing task on the virtual panel was significantly (&lt;i&gt;p&lt;\/i&gt;&lt; 0.0001) higher than that of the physical one. Users using a Hololens 2 AR device had inferior performance in pointing tasks than on a physical panel. A revised Fitts's model was proposed to incorporate both the physical-virtual component and inclined angle of the panel in estimating the MT. This model is novel. The index of difficulty and throughput of the pointing tasks between using the physical and virtual panels were compared and discussed. The information in this paper is beneficial to AR designers in promoting the usability of their designs so as to improve the user experience of their products.","LowLevel":"human computer interaction;user experience;user interfaces","MidLevel":"human-computer interaction;human factors","HighLevel":"end users and user experience","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[33,40,80,390,162,13,378,146,449,91,32,151,142,440,367,195,237,157,118,365],"Top20Abs":[33,40,80,390,162,13,146,378,32,91,151,449,367,440,157,142,237,69,195,118],"Top20Tags":[365,382,142,198,14,29,415,339,248,205,45,66,242,70,122,63,302,180,213,314],"Citation":"Zhao, C., Li, K. W., & Peng, L. (2023). Movement Time for Pointing Tasks in Real and Augmented Reality Environments. Applied Sciences, 13(2), 788. https:\/\/doi.org\/10.3390\/app13020788\n"},{"Title":"Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence","DOI":"10.1145\/3557999","Publication year":2023,"Abstract":"Mobile Augmented Reality (MAR) integrates computer-generated virtual objects with physical environments for mobile devices. MAR systems enable users to interact with MAR devices, such as smartphones and head-worn wearables, and perform seamless transitions from the physical world to a mixed world with digital entities. These MAR systems support user experiences using MAR devices to provide universal access to digital content. Over the past 20 years, several MAR systems have been developed, however, the studies and design of MAR frameworks have not yet been systematically reviewed from the perspective of user-centric design. This article presents the first effort of surveying existing MAR frameworks (count: 37) and further discusses the latest studies on MAR through a top-down approach: (1) MAR applications; (2) MAR visualisation techniques adaptive to user mobility and contexts; (3) systematic evaluation of MAR frameworks, including supported platforms and corresponding features such as tracking, feature extraction, and sensing capabilities; (4) and underlying machine learning approaches supporting intelligent operations within MAR systems. Finally, we summarise the development of emerging research fields and the current state-of-the-art and discuss the important open challenges and possible theoretical and technical directions. This survey aims to benefit both researchers and MAR system developers alike.","LowLevel":"feature extraction;learning algorithms;mobile computing;user interfaces","MidLevel":"artificial intelligence;telecommunication;computer vision;medical;chemical;human-computer interaction","HighLevel":"industries;technology;end users and user experience","Venue":"ACM Comput. Surv. (USA)","Top20AbsAndTags":[264,369,164,147,440,175,76,405,208,118,267,45,424,365,179,419,382,413,103,327],"Top20Abs":[264,369,164,147,440,175,45,76,405,118,224,179,208,267,365,327,153,419,446,371],"Top20Tags":[142,13,221,331,203,195,139,115,264,339,29,387,382,110,175,72,309,24,116,73],"Citation":"Cao, J., Lam, K.-Y., Lee, L.-H., Liu, X., Hui, P., & Su, X. (2023). Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence. ACM Computing Surveys, 55(9), 1\u201336. https:\/\/doi.org\/10.1145\/3557999\n"},{"Title":"Augmented reality to support the maintenance of urban-line infrastructures: a case study","DOI":"10.1016\/j.procs.2022.12.271","Publication year":2023,"Abstract":"Urban-line infrastructure projects encounter the installation of new pipe networks for water, sewage, gas, heating and their resulting maintenance operations. Often such kind of projects are characterized by inaccurate information of the layed pipes in terms of their location, geometry and type. In the literature, only a few Augmented Reality practical applications in construction have been identified. This confirms the fact that guidelines, best cases and standardized implementation models are still missing for a successful roll-out of this technology in construction. In this article, we propose a feasibility study of Augmented Reality to support the maintenance of a heat-district installation project as case study. By using a S.W.O.T. analysis, the strengths and weaknesses as well as the opportunities and threats of Augmented Reality in these contexts were investigated. Future research activities will focus to support the creation of digital models as well as to have a bi-directional information flow between AR and real construction sites. All rights reserved Elsevier.","LowLevel":"construction industry;maintenance engineering;pipes","MidLevel":"manufacturing;construction","HighLevel":"industries","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[60,389,86,146,181,197,475,179,44,395,190,412,124,261,131,54,148,454,273,472],"Top20Abs":[86,146,197,389,44,181,472,124,179,395,454,273,54,131,315,60,184,190,261,45],"Top20Tags":[389,86,181,190,261,146,412,131,144,179,370,106,68,475,295,360,12,463,395,467],"Citation":"Revolti, A., Dallasega, P., Schulze, F., & Walder, A. (2023). Augmented Reality to support the maintenance of urban-line infrastructures: A case study. Procedia Computer Science, 217, 746\u2013755. https:\/\/doi.org\/10.1016\/j.procs.2022.12.271\n"},{"Title":"Design and Prototype Development of Augmented Reality in Reading Learning for Autism","DOI":"10.3390\/computers12030055","Publication year":2023,"Abstract":"(1) Background: Augmented reality is no less popular than virtual reality. This technology has begun to be used in education fields, one of which is special education. Merging the real and virtual worlds is the advantage of augmented reality. However, it needs special attention in making software for children with special needs, such as children with autism. This paper presents an application prototype by paying attention to the characteristics of autistic individuals according to the Autism Guide, that has existed in previous studies. (2) Method: The method used in the development of this prototype is the Linear Sequential Model. Application development is made using Unity3D, Vuforia, and Adobe Illustrator by considering accessibility and other conveniences for developers. (3) Results: The prototype was developed with reference to the Autism Guide, then validated by media experts and autistic experts with the results of the assessment obtaining a score of 87.3\/100 which is in the \"Very Good\" category and is suitable for use. (4) Conclusions: The development of a prototype that refers to the characteristics of children with autism needs to be considered so that what will be conveyed can be easily accepted.","LowLevel":"computer aided instruction;medical disorders","MidLevel":"training;medical","HighLevel":"industries;use cases","Venue":"Computers (Switzerland)","Top20AbsAndTags":[46,115,443,51,136,83,180,436,114,253,222,54,351,140,9,439,414,208,394,53],"Top20Abs":[46,443,115,136,83,180,253,351,51,222,114,436,336,105,9,208,54,439,394,140],"Top20Tags":[222,367,204,9,93,33,363,17,275,292,253,117,0,47,246,163,244,145,61,372],"Citation":"Khoirunnisa, A. N., Munir, & Dewi, L. (2023). Design and Prototype Development of Augmented Reality in Reading Learning for Autism. Computers, 12(3), 55. https:\/\/doi.org\/10.3390\/computers12030055\n"},{"Title":"Media comparison studies dominate comparative research on augmented reality in education","DOI":"10.1016\/j.compedu.2022.104711","Publication year":2023,"Abstract":"Research on the use of augmented reality (AR) in education has received a lot of attention in recent years. Based on many systematic reviews and meta-analyses, it has been concluded that AR is effective. Recently, however, researchers have criticized the fact that the empirical basis for this conclusion is based on results from methodologically problematic media comparison studies. However, an analysis of the literature and quantitative evidence for this claim are lacking. In this research project, this research gap was addressed using the Systematic Review method. A total of 92 primary studies from the top 12&lt;i&gt;Educational Technology&lt;\/i&gt;journals were coded and analyzed. The results show that research on AR in education is based on media comparison studies: 80% of the studies compare AR to another medium or technology. Few studies examine how and when learning with AR is effective. In addition, results show that over the years, since 2009, more media comparison studies have been published than other research types. We summarize why media comparison studies are problematic and discuss directions for future research on AR in education. This research shifts from the question&lt;i&gt;if&lt;\/i&gt;AR can be used in instruction to the more important questions of&lt;i&gt;how&lt;\/i&gt;and&lt;i&gt;when&lt;\/i&gt;learning and teaching with AR works. All rights reserved Elsevier.","LowLevel":"computer aided instruction;educational technology;reviews;teaching","MidLevel":"education;standards;human-computer interaction;training","HighLevel":"industries;standards;use cases;end users and user experience","Venue":"Comput. Educ. (Netherlands)","Top20AbsAndTags":[40,390,13,378,146,449,37,91,114,151,77,9,46,57,49,62,87,88,136,163],"Top20Abs":[40,390,13,378,146,449,37,91,151,77,46,9,114,49,88,62,57,321,87,259],"Top20Tags":[222,204,17,122,405,114,87,363,3,82,372,196,154,51,9,414,145,232,164,62],"Citation":"Buchner, J., & Kerres, M. (2023). Media comparison studies dominate comparative research on augmented reality in education. Computers &amp; Education, 195, 104711. https:\/\/doi.org\/10.1016\/j.compedu.2022.104711\n"},{"Title":"Predicting Gaze-based Target Selection in Augmented Reality Headsets based on Eye and Head Endpoint Distributions","DOI":"10.1145\/3544548.3581042","Publication year":2023,"Abstract":"Target selection is a fundamental task in interactive Augmented Reality (AR) systems. Predicting the intended target of selection in such systems can provide users with a smooth, low-friction interaction experience. Our work aims to predict gaze-based target selection in AR headsets with eye and head endpoint distributions, which describe the probability distribution of eye and head 3D orientation when a user triggers a selection input. We first conducted a user study to collect users' eye and head behavior in a gaze-based pointing selection task with two confirmation mechanisms (air tap and blinking). Based on the study results, we then built two models: a unimodal model using only eye endpoints and a multimodal model using both eye and head endpoints. Results from a second user study showed that the pointing accuracy is improved by approximately 32% after integrating our models into gaze-based selection techniques.","LowLevel":"eye;gaze tracking;human computer interaction;probability","MidLevel":"video;computer vision;human-computer interaction;input","HighLevel":"end users and user experience;technology","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[277,250,259,75,237,202,419,37,72,243,260,276,221,123,279,433,53,283,45,368],"Top20Abs":[250,75,277,202,259,237,419,37,221,283,368,166,45,153,456,123,260,189,215,433],"Top20Tags":[277,250,259,72,422,339,126,419,167,424,276,213,172,421,198,260,55,144,314,112],"Citation":"Wei, Y., Shi, R., Yu, D., Wang, Y., Li, Y., Yu, L., & Liang, H.-N. (2023). Predicting Gaze-based Target Selection in Augmented Reality Headsets based on Eye and Head Endpoint Distributions. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3581042\n"},{"Title":"Are you talking to me? An Audio Augmented Reality conversational guide for cultural heritage","DOI":"10.1016\/j.pmcj.2023.101797","Publication year":2023,"Abstract":"Augmented Reality (AR) technologies are increasingly utilized as a means of stimulating immersive experiences to cultural site visitors, mainly through visual superimposition of interactive digital elements onto the physical world. Recent research has investigated the use of Audio AR (AAR) in heritage sites, wherein visitors listen to spatially registered sound which could be attributed to &lsquo;talking&rsquo; physical artefacts. A parallel trend in the audience engagement programs of cultural institutions involves the employment of AI chatbots which are engaged in dialogues with followers or visitors to provide meaningful responses to a number of user questions. Herein, we present Exhibot, an intelligent audio guide system aiming at enhancing the user experience of cultural site visitors. Exhibot involves the combination of AAR and chatbot technologies to enable natural visitor-exhibit interaction, while also leveraging IoT devices to contextualize the delivered information. The key contribution of the proposed system lies in the interplay of AAR, chatbot and IoT technologies to create immersive learning experiences in the context of an integrated cultural guide system. Exhibot has undergone field trials to validate its usability and utility in realistic operational conditions. As a case study, we have chosen the statue of a prominent politician situated at a central square in Heraklion, Greece. The evaluation results indicated a very positive attitude of users, which is attributed both to the sense of immersion evoked by the AAR-powered storytelling and the natural human-like conversation enabled by the chatbot. &copy; 2023 Elsevier B.V.","LowLevel":"internet of things","MidLevel":"internet of things;networks","HighLevel":"technology","Venue":"Pervasive Mob. Comput.","Top20AbsAndTags":[267,6,302,109,175,129,16,197,440,229,218,123,405,145,447,96,395,236,188,251],"Top20Abs":[6,302,109,267,197,16,440,229,175,218,405,145,129,96,418,45,419,30,236,251],"Top20Tags":[129,123,315,447,472,175,483,38,361,469,188,286,258,298,459,220,289,110,121,378],"Citation":"Tsepapadakis, M., & Gavalas, D. (2023). Are you talking to me? An Audio Augmented Reality conversational guide for cultural heritage. Pervasive and Mobile Computing, 92, 101797. https:\/\/doi.org\/10.1016\/j.pmcj.2023.101797\n"},{"Title":"Location-Based Augmented Reality for Cultural Heritage Communication and Education: The Doltso District Application","DOI":"10.3390\/s23104963","Publication year":2023,"Abstract":"Location-based Augmented Reality applications are increasingly used in many research and commercial fields. Some of the fields that these applications are used are recreational digital games, tourism, education, and marketing. This study aims to present a location-based augmented reality (AR) application for cultural heritage communication and education. The application was created to inform the public, especially K12 students, about a district of their city with cultural heritage value. Furthermore, Google Earth was utilized to create an interactive virtual tour for consolidating the knowledge acquired by the location-based AR application. A scheme for evaluating the AR application was also constructed using factors suitable for location-based applications: challenge, educational usefulness (knowledge), collaboration, and intention to reuse. A sample of 309 students evaluated the application. Descriptive statistical analysis showed that the application scored well in all factors, especially in challenge and knowledge (mean values 4.21 and 4.12). Furthermore, structural equation modeling (SEM) analysis led to a model construction that represents how the factors are causally related. Based on the findings, the perceived challenge significantly influenced the perceived educational usefulness (knowledge) (b = 0.459, sig = 0.000) and interaction levels (b = 0.645, sig = 0.000). Interaction amongst users also had a significant positive impact on users&rsquo; perceived educational usefulness (b = 0.374, sig = 0.000), which in turn influenced users&rsquo; intention to reuse the application (b = 0.624, sig = 0.000). &copy; 2023 by the authors.","LowLevel":"education computing;factor analysis;location","MidLevel":"education;geospatial;other","HighLevel":"industries;technology;other","Venue":"Sensors","Top20AbsAndTags":[410,62,56,197,164,147,188,46,207,16,267,171,252,145,24,155,57,47,236,87],"Top20Abs":[410,62,56,188,46,171,207,396,16,87,252,57,96,145,82,88,103,363,306,102],"Top20Tags":[24,197,147,29,14,454,438,3,164,31,487,479,433,431,116,473,1,192,187,23],"Citation":"Kleftodimos, A., Evagelou, A., Triantafyllidou, A., Grigoriou, M., & Lappas, G. (2023). Location-Based Augmented Reality for Cultural Heritage Communication and Education: The Doltso District Application. Sensors, 23(10), 4963. https:\/\/doi.org\/10.3390\/s23104963\n"},{"Title":"Dream Garden: Exploring Location-Based, Collaboratively-Created Augmented Reality Spaces","DOI":"10.1145\/3544549.3585810","Publication year":2023,"Abstract":"We introduce Dream Garden, an augmented reality application (AR) that lets people place 3D flowers into the physical world to build a collaborative location-based garden. Despite the potential for connecting strangers in the digital realm, current research has not explored location-based augmented reality experiences that enable strangers to connect by building artifacts collaboratively. We explore this by creating an AR digital community garden deployed in a specific location. Anyone with the proper app can access and see the flowers previously planted by strangers, as well as plant their own flowers to grow the garden. We evaluated this app with 10 participants, with 5 visiting the digital garden more than once, to evaluate their sense of connection to the other participants as each participant added one digital flower to the garden. We found that participants were joyful about building a shared space, were excited about the dynamic nature of the garden, felt a connection to the physical location of the digital garden, and expressed a sense of belonging to a community of strangers but not necessarily an emotional connection.","LowLevel":"groupware;human computer interaction;location based services;user experience","MidLevel":"geospatial;human-computer interaction;collaboration;human factors","HighLevel":"end users and user experience;technology;use cases","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[197,338,30,279,267,24,27,125,219,440,26,365,14,16,17,385,147,304,69,22],"Top20Abs":[338,197,27,16,26,440,30,279,267,219,292,232,22,447,385,467,125,309,24,412],"Top20Tags":[197,14,142,279,147,365,339,199,17,104,382,303,30,164,304,239,415,248,213,329],"Citation":"Petrov, E., & Monroy-Hern\u00e1ndez, A. (2023). Dream Garden: Exploring Location-Based, Collaboratively-Created Augmented Reality Spaces. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585810\n"},{"Title":"Research On Virtual Restoration Display Of Chinese Paintings Based On Augmented Reality","DOI":"10.1109\/ICCEA58433.2023.10135180","Publication year":2023,"Abstract":"In response to the current problem that a large number of broken Chinese paintings cannot be exhibited, an AR restoration method is proposed that allows visitors to experience the restoration process and appreciate the restored works when they visit. Firstly, the damaged Chinese paintings are digitally restored; then the ORB(oriented fast and rotated brief) algorithm extracts more uniform feature points in terms of quantity and distribution through linear transformation image enhancement and improved adaptive thresholding to ensure the accurate overlay of AR virtual content through feature point matching; finally, the color of virtual content is corrected through histogram matching before the virtual-real overlay to ensure the consistency of AR virtual-real fusion. Finally, the color of the virtual content is corrected by histogram matching before the virtual-real superposition to ensure the consistency of AR virtual-real fusion. The experiments show that the method can effectively solve the problem that the broken paintings and calligraphy cannot be exhibited directly and can increase the participation and immersion of visitors.","LowLevel":"art;feature extraction;image enhancement;image matching;image restoration","MidLevel":"construction;computer vision;chemical;liberal arts","HighLevel":"industries;technology","Venue":"2023 4th International Conference on Computer Engineering and Application (ICCEA)","Top20AbsAndTags":[318,98,438,99,81,436,20,107,229,2,471,267,365,198,48,95,76,302,233,486],"Top20Abs":[318,438,98,99,20,365,107,229,302,81,267,436,288,486,471,95,123,151,402,2],"Top20Tags":[81,98,318,270,71,387,76,381,370,255,321,379,471,265,99,2,116,107,220,405],"Citation":"Zhong, Q., Wang, Q., Chen, H., Han, M., & Wang, G. (2023). Research On Virtual Restoration Display Of Chinese Paintings Based On Augmented Reality. 2023 4th International Conference on Computer Engineering and Application (ICCEA). https:\/\/doi.org\/10.1109\/iccea58433.2023.10135180\n"},{"Title":"Utilizing Augmented Reality to Enhance Twenty-First Century Skills in Chemistry Education","DOI":"10.1109\/EDUCON54358.2023.10125271","Publication year":2023,"Abstract":"Numerous studies have shown that mobile devices like smartphones play a significant role in education these days, and it's clear that the influence and benefits of these devices regarding the potential for pedagogical perspectives are obvious. Mobile learning allows for flexibility by eliminating the need for learning to happen at a particular time and place. Moreover, mobile devices that can support Augmented Reality (AR) are becoming more powerful, less expensive and quite useful in the education process. In this paper, we conduct a study where mobile AR application alongside project-based learning is utilized in a classroom setting, where interviews are administered to understand the K-12 students' attitude towards the educational model based on mobile AR for enhancing the twenty-first century skills in chemistry education. The interviews are conducted with thirty students and their teacher who teaches them chemistry course. Four skills are investigated and discussed, i.e., innovation and creativity, critical skills and problem solving, communication and collaboration, and information culture. Based on the research findings, some recommendations are provided. Furthermore, in the scope of Agenda 2030 for Palestine, we believe these findings could serve as a basis for developing practical policy interventions attributed to enhancing the educational system with emerging technologies.","LowLevel":"computer aided instruction;educational courses;mobile computing;mobile learning;smartphones;teaching","MidLevel":"education;training;telecommunication;medical;liberal arts","HighLevel":"industries;use cases","Venue":"2023 IEEE Global Engineering Education Conference (EDUCON)","Top20AbsAndTags":[320,115,163,128,56,82,184,171,363,57,51,134,88,46,62,185,17,39,342,124],"Top20Abs":[115,163,128,88,320,363,46,62,188,57,56,171,184,39,306,185,82,134,87,17],"Top20Tags":[115,93,320,82,164,113,51,222,134,56,196,139,89,33,241,171,3,204,376,124],"Citation":"Abualrob, M., Ewais, A., Dalipi, F., & Awaad, T. (2023). Utilizing Augmented Reality to Enhance Twenty-First Century Skills in Chemistry Education. 2023 IEEE Global Engineering Education Conference (EDUCON). https:\/\/doi.org\/10.1109\/educon54358.2023.10125271\n"},{"Title":"Designing an Augmented Reality Authoring Tool to Support Complex Tasks. A Design Science Study Using Cognitive Load Theory","DOI":"10.1007\/978-3-031-32808-4_6","Publication year":2023,"Abstract":"Despite the potential of augmented reality (AR) to support and guide complex industrial tasks, the technology is still not broadly applied. One possible reason for this is that the creation of AR content is highly complex and requires programming skills and deep spatial knowledge. AR authoring tools can help address this complexity by enabling non-developers to create self-sufficient AR content. Therefore, this paper proposes a theory-driven design for AR authoring tools that allows non-developers to create self-sufficient AR-based instructions to support complex tasks. Based on ten interviews with experts working with AR authoring tools and a following focus group with eight participants, we propose three design principles for future AR authoring tools in the engineering context. These design principles are instantiated in two prototypes of different richness and evaluated in an experiment with 23 students. Our study shows that the cognitive load is slightly increased when using the extensive AR authoring tool, but it also shows that significantly better results can be achieved with the extensive AR authoring tool. We contribute by providing design principles for AR authoring tools for creating AR-based instructions, which extend the existing AR authoring research in the industrial context. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"design;industrial research","MidLevel":"education;business performance metrics;human-computer interaction;medical","HighLevel":"industries;business;end users and user experience","Venue":"Lect. Notes Comput. Sci.","Top20AbsAndTags":[53,179,68,136,161,189,49,45,410,434,185,69,87,336,114,2,155,27,93,104],"Top20Abs":[53,179,68,49,189,410,136,45,87,161,88,69,155,104,315,434,110,185,336,93],"Top20Tags":[161,471,114,454,418,442,369,485,184,469,459,373,472,446,315,188,156,210,443,68],"Citation":"H\u00f6nemann, K., Konopka, B., & Wiesche, M. (2023). Designing an Augmented Reality Authoring Tool to Support Complex Tasks. A Design Science Study Using Cognitive Load Theory. Design Science Research for a New Society: Society 5.0, 87\u2013101. https:\/\/doi.org\/10.1007\/978-3-031-32808-4_6\n"},{"Title":"Combining Augmented Reality and Fairy Tales to Teach Science to Primary School Students: Teachers&rsquo; Experience from the Fairy Tale Science Augmented (FAnTASIA) Project","DOI":"10.1007\/978-3-031-29800-4_53","Publication year":2023,"Abstract":"The use of storytelling has long been covering a wide variety of subjects across different levels of education, from preschool and K12 to professional training. Regardless of the content being taught, storytelling methods are assumed to catch students&rsquo; attention and involve both their cognitive and affective skills. Educational literature has also collected evidence for effectiveness of the use of augmented reality (AR) technology in promoting greater academic achievement and engagement among students, com-pared with traditional and other digital media-related lessons. Based on existing research, aim of the Fairy Tale Science Augmented (FAnTASIA) project was to design a multi-lingual educational package to integrate storytelling and AR to support the teaching of science concepts and skills in children aged from 5 to 12&nbsp;years. The present paper first provides a description of the FAnTASIA project and the developed educational package. Following, the paper presents the research design implemented to evaluate the effectiveness of the package in promoting children&rsquo;s knowledge and cognitive skills and users&rsquo; (students and teachers) satisfaction with its use. In this regard, the final section describes teachers&rsquo; experience with the use of the educational kit from the results of a questionnaire specifically administered to this purpose. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"digital storage;personnel training;students","MidLevel":"training;users;human resources;farming and natural science;networks","HighLevel":"end users and user experience;use cases;business;industries;technology","Venue":"Commun. Comput. Info. Sci.","Top20AbsAndTags":[184,128,56,46,459,82,454,51,103,161,443,124,372,414,115,406,6,57,342,136],"Top20Abs":[184,128,56,46,82,454,103,51,88,443,459,414,124,372,115,136,57,30,161,39],"Top20Tags":[161,447,472,459,462,31,469,184,315,478,369,361,135,56,471,373,140,312,154,332],"Citation":"Chiazzese, G., Tosto, C., Seta, L., Chifari, A., Denaro, P., Dhrami, D., La Guardia, D., Arrigo, M., Farella, M., Ioannides, C., Yegorina, D., & Mangina, E. (2023). Combining Augmented Reality and Fairy Tales to Teach Science to Primary School Students: Teachers\u2019 Experience from the Fairy Tale Science Augmented (FAnTASIA) Project. Communications in Computer and Information Science, 706\u2013718. https:\/\/doi.org\/10.1007\/978-3-031-29800-4_53\n"},{"Title":"More than meets the eye: In-store retail experiences with augmented reality smart glasses","DOI":"10.1016\/j.chb.2023.107816","Publication year":2023,"Abstract":"Augmented reality smart glasses (ARSGs) promise to enhance consumer experiences and decision-making when deployed as in-store retail technologies. However, research to date has not studied in-store use cases; instead, it has focused primarily on consumers' potential adoption of these devices for everyday use. Nor have prior studies compared ARSG uses with the now-common use of AR on touchscreen devices. The current research addresses these knowledge gaps by examining whether ARSGs outperform AR on touchscreen devices in the context of in-store retail experiences. Testing with an actual retail application (n = 308) shows that ARSGs are superior to AR on touchscreen devices for evoking consumers&rsquo; perceptions of immersion and mental intangibility. Furthermore, this superiority leads consumers to evaluate their shopping experiences more positively in terms of their decision comfort, satisfaction, and ease of evaluation, with significantly positive effects on their purchase intentions. These results highlight the relevance of implementing ARSGs in-store and provide retailers with recommendations for effective ARSG strategies. &copy; 2023 The Authors","LowLevel":"decision making;glass;purchasing;sales","MidLevel":"chemical;human factors;sales and marketing;logistics","HighLevel":"industries;business;end users and user experience","Venue":"Comput. Hum. Behav.","Top20AbsAndTags":[102,267,13,333,228,376,57,410,468,166,62,129,368,454,188,361,70,179,25,252],"Top20Abs":[102,13,267,333,410,129,57,88,62,468,166,454,70,368,110,376,252,87,188,228],"Top20Tags":[228,333,373,355,315,25,471,472,20,162,468,361,155,102,141,368,135,194,1,446],"Citation":"Pfeifer, P., Hilken, T., Heller, J., Alimamy, S., & Di Palma, R. (2023). More than meets the eye: In-store retail experiences with augmented reality smart glasses. Computers in Human Behavior, 146, 107816. https:\/\/doi.org\/10.1016\/j.chb.2023.107816\n"},{"Title":"Integrated optical phased arrays: augmented reality, LiDAR, and beyond","DOI":"10.1117\/12.2649394","Publication year":2023,"Abstract":"Integrated optical phased arrays (OPAs), fabricated in advanced silicon-photonics platforms, enable manipulation and dynamic control of free-space light in a compact form factor, at low costs, and in a non-mechanical way. In this talk, I will highlight our work on developing OPA-based platforms, devices, and systems that enable chip-based solutions to high-impact problems in areas including augmented-reality displays, LiDAR sensing for autonomous vehicles, optical trapping for biophotonics, 3D printing, and trapped-ion quantum engineering. &copy; 2023 SPIE.","LowLevel":"3d printing;optical radar;photonic devices;space platforms;three-dimensional displays;trapped ions","MidLevel":"optics;developers;geospatial;manufacturing;other;display technology","HighLevel":"displays;technology;industries;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[317,201,141,343,278,444,149,169,451,324,441,120,228,74,413,235,437,290,344,430],"Top20Abs":[317,141,343,278,444,267,434,465,442,451,430,228,344,437,129,424,371,399,463,120],"Top20Tags":[201,317,169,176,223,149,430,235,437,332,441,324,74,361,290,444,228,0,393,141],"Citation":"Notaros, J. (2023). Integrated optical phased arrays: augmented reality, LiDAR, and beyond. AI and Optical Data Sciences IV. https:\/\/doi.org\/10.1117\/12.2649394\n"},{"Title":"Design of an Augmented Reality App for Primary School Students Which Visualizes Length Units to Promote the Conversion of Units","DOI":"10.1007\/978-3-031-32808-4_20","Publication year":2023,"Abstract":"Insight and understanding of the structure of units can be considered one of mathematics&rsquo; most important learning areas, as it is needed in everyday life. It is essential in everyday tasks, such as trading goods, which requires dealing with monetary values and, e.g., lengths, weights, volumes, or area units. In addition, students in higher education after primary school show problems with understanding units by confusing units, especially in advanced STEM subjects, e.g., physics. The paper shows how Augmented Reality (AR) technology can be used to gain insight into understanding units by visualizing units of length using an AR app. The design of the AR app will be presented, which was developed after a theoretical grounding and practical testing with an existing AR measuring tool on the app market. Design Science Research (DSR) will be used to develop a suitable content learning environment using the AR app. The learning environment will then be tested with students again and disseminated in international workshops for teachers. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"commerce;computer aided instruction;design;e-learning;students","MidLevel":"education;training;users;medical;human-computer interaction;sales and marketing","HighLevel":"industries;business;use cases;end users and user experience","Venue":"Lect. Notes Comput. Sci.","Top20AbsAndTags":[124,134,163,320,39,454,135,115,188,369,306,136,114,262,312,51,57,88,184,342],"Top20Abs":[124,134,163,88,39,320,306,136,188,312,135,51,262,115,87,159,89,342,27,82],"Top20Tags":[135,369,459,188,454,471,184,485,333,443,114,413,442,1,446,341,57,157,312,418],"Citation":"Mueller, L. M., & Platz, M. (2023). Design of an Augmented Reality App for Primary School Students Which Visualizes Length Units to Promote the Conversion of Units. Design Science Research for a New Society: Society 5.0, 314\u2013328. https:\/\/doi.org\/10.1007\/978-3-031-32808-4_20\n"},{"Title":"0.37-inch UHD 11,800 PPI Liquid Crystal on Silicon Micro-Display with Embedded 4x Up-Scaler Using Micro-Mirror Space-Interpolation Pixel Circuit for Metaverse Augmented Reality Glasses","DOI":"10.1117\/12.2645045","Publication year":2023,"Abstract":"A 0.37-inch 360 Hz field refresh rate ultra-HD 11,800PPI 2.15 &mu;m pixel pitch liquid crystal on silicon (LCoS) micro-display panel with embedded 4x up-scaler is presented. In AR glasses for metaverse, spatial resolution is very important as field of view (FoV) increases. The lack of spatial resolution interferes with the immersion of the augmented virtual space, such as limited information on image quality degradation and screen door effects. The display is proposed with new ultra-low power resolution enhancement technology of quadruple scaler to solve spatial aliasing caused by limited resolution at augmented reality (AR) glasses. The ultra-fine resolution pixel circuit is designed by new spatial interpolation technique called as micro-mirror space-interpolation (mmSI). The new micro-mirror architecture was proposed to make capacitive circuits network which produce interpolated pixel data and pixel mirror itself. The embedded spatial interpolation is done by pixel circuit itself and there are no additional circuits from video input to pixel driving. In this reason, the power consumption of driving the panel is same to full-HD resolution drivers&rsquo; which is only 100mW despite of quadruple resolution. The micro-display panel for metaverse AR glasses was fabricated using a 0.11-&mu;m CMOS process and was assembled with an LC front plane using VAN LC. The die size, active area and panel size are 11.65 mm x 7.75 mm, 8.25 x 4.64 mm2 and 13.8 x 8.5 mm2, respectively. The output video resolution is 3840 x 2160 with RGB 8-bit gray depth with 1920x1080 video input. The presented panel has 11,800 PPI with UHD video resolution only 0.37-inch diagonal active display area. The angular pixel resolution of the display is achieved at a 49 cycles per degree (cpd) resolution with 90-degree diagonal FoV for ultra-portable handheld AR glasses application. &copy; 2023 SPIE.","LowLevel":"image resolution;interpolation;liquid crystals;mirrors;pixels;silicon;timing circuits","MidLevel":"engineering;data;graphics;chemical;semiconductors;optics","HighLevel":"industries;technology;displays","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[235,141,37,379,424,228,364,358,201,80,433,149,220,265,290,130,345,481,317,319],"Top20Abs":[235,141,37,424,364,80,228,379,201,358,220,433,393,248,130,481,265,345,225,319],"Top20Tags":[149,141,235,228,430,223,215,317,156,169,437,422,74,20,432,438,201,285,290,158],"Citation":"Jeong, M., Lee, J., Lee, K. soo, Kim, J., Park, S., Kim, B.-E., & Lee, K. (2023). 0.37-inch UHD, 11,800 PPI liquid crystal on silicon micro-display with embedded 4x up-scaler using micro-mirror space-interpolation pixel circuit for metaverse augmented reality glasses. Advances in Display Technologies XIII. https:\/\/doi.org\/10.1117\/12.2645045\n"},{"Title":"Comparative Study on Brightness and Leakage Ratio Test Methods of Optical Waveguide Augmented Reality Glasses","DOI":"10.1117\/12.2667900","Publication year":2023,"Abstract":"Because the near eye display (NED) device mainly displays its imaging effect with a unique virtual image, in essence, compared with the traditional display device, the measurement requirements and methods of brightness, light leakage ratio and other related parameters have also changed. By analyzing the imaging characteristics of optical waveguide AR display equipment, a comparative test scheme for brightness and light leakage ratio is proposed. The same type of arrayed optical waveguide NED module is experimentally measured by using equipment with different test principles, and the measurement results are compared and analyzed. The research results show that the measurement results of the ordinary aiming point luminance meter are not the luminance values in the real display. For the measurement of a certain illumination degree of the equipment, the luminance value and light leakage measured by the aiming point luminance meter have lower deviation than that measured by the two-dimensional imaging luminance meter. Therefore, in the process of testing NED, it is necessary to correctly select the measuring instrument according to its imaging characteristics, the size of the area to be tested and the test scene. &copy; 2023 SPIE.","LowLevel":"display devices;optical design;optical waveguides","MidLevel":"optics;human-computer interaction;display technology","HighLevel":"displays;end users and user experience","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[481,332,170,422,461,228,158,343,223,317,141,393,432,441,413,278,444,235,297,344],"Top20Abs":[170,461,481,422,332,343,228,432,297,141,278,225,129,98,393,248,223,158,444,235],"Top20Tags":[223,393,441,201,317,158,437,344,0,74,444,228,332,290,235,430,438,431,445,481],"Citation":"Di, F., Xu, Y., Chen, C., & Wu, J. (2023). Comparative study on brightness and leakage ratio test methods of optical waveguide augmented reality glasses. International Conference on Precision Instruments and Optical Engineering (PIOE 2022). https:\/\/doi.org\/10.1117\/12.2667900\n"},{"Title":"The Effect of Luminance on Depth Perception in Augmented Reality Guided Laparoscopic Surgery","DOI":"10.1117\/12.2654223","Publication year":2023,"Abstract":"Depth perception is a major issue in surgical augmented reality (AR) with limited research conducted in this scientific area. This study establishes a relationship between luminance and depth perception. This can be used to improve visualisation design for AR overlay in laparoscopic surgery, providing surgeons a more accurate perception of the anatomy intraoperatively. Two experiments were conducted to determine this relationship. First, an online study with 59 participants from the general public, and second, an in-person study with 10 surgeons as participants. We developed 2 open-source software tools utilising SciKit-Surgery libraries to enable these studies and any future research. Our findings demonstrate that the higher the relative luminance, the closer a structure is perceived to the operating camera. Furthermore, the higher the luminance contrast between the two structures, the higher the depth distance perceived. The quantitative results from both experiments are in agreement, indicating that online recruitment of the general public can be helpful in similar studies. An observation made by the surgeons from the in-person study was that the light source used in laparoscopic surgery plays a role in depth perception. This is due to its varying positioning and brightness which could affect the perception of the overlaid AR. We found that luminance directly correlates with depth perception for both surgeons and the general public, regardless of other depth cues. Future research may focus on comparing different colours used in surgical AR and using a mock operating room (OR) with varying light sources and positions. &copy; 2023 SPIE.","LowLevel":"depth perception;laparoscopy;light sources;medical imaging;open source software;open systems;visualization","MidLevel":"education;human factors;data;medical;developers;input","HighLevel":"industries;technology;end users and user experience","Venue":"Progr. Biomed. Opt. Imaging Proc. SPIE","Top20AbsAndTags":[234,379,69,169,371,437,213,326,433,481,81,300,376,26,410,152,285,204,432,237],"Top20Abs":[379,69,169,234,437,213,81,326,26,371,300,430,410,237,102,118,288,204,481,152],"Top20Tags":[432,433,43,453,0,207,323,138,488,470,285,464,463,191,431,443,442,445,480,430],"Citation":"Reissis, A., Yoo, S., Clarkson, M. J., & Thompson, S. A. (2023). The effect of luminance on depth perception in augmented reality guided laparoscopic surgery. Medical Imaging 2023: Image-Guided Procedures, Robotic Interventions, and Modeling. https:\/\/doi.org\/10.1117\/12.2654223\n"},{"Title":"Applicability of a digitalization model based on augmented reality for building construction education in architecture","DOI":"10.1108\/CI-07-2021-0136","Publication year":2023,"Abstract":"&lt;b&gt;Purpose &lt;\/b&gt;The purpose of this paper is to introduce a digitalization model (DM) for building construction courses in architectural education as a response to the recent emerging technologies in the era of digital transformations. This DM is developed and applied through augmented reality (AR) technologies to boost perception, understandings and ability to solve building construction details. &lt;b&gt;Design\/methodology\/approach &lt;\/b&gt;Based on a thorough review of recent technologies like AR, virtual reality (VR), building information modelling (BIM) and their applications in architectural education, the methodology involves the generation of a model, its application and evaluation. The model is based on the integration of BIM and AR which is applied into a third year \"Building Construction Project\" course. Each student has designed a residential building and accordingly prepared the construction drawings by adapting the DM. An online survey -based on Technology Acceptance Model - is conducted to evaluate the DM by quantitative data analysis from SPSS and Excel. &lt;b&gt;Findings &lt;\/b&gt;The key findings of the study include the following items: determination of the proper digital tools, the definition of the steps and workflow based on the building project phases to develop construction drawings and define precise details effectively. By the help of this, the DM is generated and applied. According to the survey and results, the DM which involves BIM-based AR is considered as beneficial, highly motivating and providing better perception on construction details. &lt;b&gt;Originality\/value&lt;\/b&gt;I mplication of AR\/VR technologies is frequently seen in design studios, whereas building construction courses state its traditional approach. However, there is a huge potential in the digitalization of building construction education by increasing the perception of students together with the increased level of communication. The study aims to close the gap of digitalization by proposing a DM, which brings a systematic approach considering each phase of building construction project as conceptual, schematic, design development and construction documents by using BIM integrated AR. Moreover, the novel model specifically brings a new approach by generation of QR codes for construction details to embed videos or simulations into the two-dimensional drawing sheets. Furthermore, the DM proposes a new approach to satisfy emerging needs and requirements of the Architecture Engineering Construction industry.","LowLevel":"architecture;building information modelling;building management systems;buildings;construction industry;data analysis;project management;qr codes;structural engineering computing;technology acceptance model","MidLevel":"education;human factors;engineering;data;business planning and management;construction;developers;input;human-computer interaction","HighLevel":"industries;technology;business;end users and user experience","Venue":"Constr. Innov. (UK)","Top20AbsAndTags":[33,40,181,378,389,390,86,449,13,35,412,37,10,472,252,414,47,62,124,184],"Top20Abs":[33,40,181,378,390,449,13,86,389,35,472,37,414,10,151,184,124,273,41,321],"Top20Tags":[86,181,389,412,62,355,359,90,35,57,395,97,44,418,205,210,261,157,190,4],"Citation":"Seyman Guray, T., & Kismet, B. (2021). Applicability of a digitalization model based on augmented reality for building construction education in architecture. Construction Innovation, 23(1), 193\u2013212. https:\/\/doi.org\/10.1108\/ci-07-2021-0136\n"},{"Title":"Exploiting deep learning and augmented reality in fused deposition modeling: a focus on registration","DOI":"10.1007\/s12008-022-01107-5","Publication year":2023,"Abstract":"The current study aimed to propose a Deep Learning (DL) based framework to retrieve in real-time the position and the rotation of an object in need of maintenance from live video frames only. For testing the positioning performances, we focused on intervention on a generic Fused Deposition Modeling (FDM) 3D printer maintenance. Lastly, to demonstrate a possible Augmented Reality (AR) application that can be built on top of this, we discussed a specific case study using a Prusa i3 MKS FDM printer. This method was developed using a You Only Look Once (YOLOv3) network for object detection to locate the position of the FDM 3D printer and a subsequent Rotation Convolutional Neural Network (RotationCNN), trained on a dataset of artificial images, to predict the rotations' parameters for attaching the 3D model. To train YOLOv3 we used an augmented dataset of 1653 real images, while to train the RotationCNN we utilized a dataset of 99.220 synthetic images, showing the FDM 3D Printer with different orientations, and fine-tuned it using 235 real images tagged manually. The YOLOv3 network obtained an AP (Average Precision) of 100% with Intersection Over Unit parameter of 0.5, while the RotationCNN showed a mean Geodesic Distance of 0.250 (&#963; = 0.210) and a mean accuracy to detect the correct rotationrof 0.619 (&#963; = 0.130), considering as acceptable the range [r - 10,r + 10]. We then evaluate the CAD system performances with 10 non-expert users: the average speed improved from 9.61 (&#963; = 1.53) to 5.30 (&#963; = 1.30) and the average number of actions to complete the task from 12.60 (&#963; = 2.15) to 11.00 (&#963; = 0.89). This work is a further step through the adoption of DL and AR in the assistance domain. In future works, we will overcome the limitations of this approach and develop a complete mobile CAD system that could be extended to any object that presents a 3D counterpart model.","LowLevel":"cad;convolutional neural nets;deep learning (artificial intelligence);feature extraction;medical image processing;object detection;printers;production engineering computing;rapid prototyping;three-dimensional printing","MidLevel":"artificial intelligence;engineering;computer vision;data;medical;graphics;liberal arts;chemical;developers;manufacturing;networks;other;display technology","HighLevel":"industries;technology;displays;other","Venue":"Int. J. Interact. Des. Manuf. (Germany)","Top20AbsAndTags":[144,116,255,451,415,76,106,379,425,160,152,411,358,409,45,341,178,453,387,95],"Top20Abs":[144,116,415,106,255,152,451,76,409,453,189,379,437,20,178,425,341,435,487,45],"Top20Tags":[255,379,370,98,71,81,387,358,5,326,95,160,381,415,425,388,79,116,451,411],"Citation":"Tanzi, L., Piazzolla, P., Moos, S., & Vezzetti, E. (2022). Exploiting deep learning and augmented reality in fused deposition modeling: a focus on registration. International Journal on Interactive Design and Manufacturing (IJIDeM), 17(1), 103\u2013114. https:\/\/doi.org\/10.1007\/s12008-022-01107-5\n"},{"Title":"Augmented reality assisted astronaut operations in space to upgrade the cold atom lab instrument","DOI":"10.1117\/12.2650750","Publication year":2023,"Abstract":"The Cold Atom Lab (CAL) is a first of its kind quantum physics science instrument that utilizes the microgravity environment of the International Space Station (ISS) for ultra-cold atom fundamental physics experiments. CAL was installed into the US Destiny Lab of ISS by astronauts in May 2018. The CAL instrument was designed for a 3-year mission life and has limited capability to be serviced or upgraded on orbit. Due to its great success the CAL team was requested to upgrade a specific electronic circuit card that was never intended to be replaced on orbit. As such, the instrument was not designed to accommodate easy access to the circuit cards to enable replacement. Therefore, the CAL team at Jet Propulsion Lab (JPL) formed a collaborative team with experts from Marshall Space Flight Center (MSFC) and Johnson Space Center (JSC) to create a new capability for Augmented Reality (AR) to be utilized on ISS that enabled real time astronaut on orbit guidance for critical repair activities. For the first time ever during an Intra-Vehicular Activity (IVA) a payload developer on the ground (CAL team) was able to see a real-time astronaut perspective video stream and simultaneously direct the astronaut with virtual visual annotations in the astronaut's field of view in addition to voice commands. This AR capability enabled the complex process of accessing and replacing the circuit card and restoring the full functionality of the CAL instrument. &copy; 2023 SPIE.","LowLevel":"atoms;bose-einstein condensation;laboratories;manned space flight;microgravity processing;orbits;space platforms;space stations;statistical mechanics","MidLevel":"training;data;aviation and aerospace;chemical;input;other","HighLevel":"industries;technology;use cases;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[101,118,64,286,127,212,470,331,425,177,158,162,157,0,443,342,65,339,54,243],"Top20Abs":[101,64,118,286,127,470,331,157,162,425,339,0,342,443,65,463,212,190,45,232],"Top20Tags":[158,177,484,474,212,438,30,487,473,479,431,433,470,43,2,187,191,440,287,283],"Citation":"Kellogg, J., Andrea-Liner, K., Jennings, J., Timms, S., Keramidas, R. C., Berry, J., DeLaCruz, T., Bryant, K., Crawford, J., Roth, K., Li, I., Kwan, S., Nuernberger, B., Croonquist, A., McArthur, M., Mohageg, M., & Oudrhiri, K. (2023). Augmented reality assisted astronaut operations in space to upgrade the cold atom lab instrument. Quantum Sensing, Imaging, and Precision Metrology. https:\/\/doi.org\/10.1117\/12.2650750\n"},{"Title":"An interactive method with gestures oriented toward the augmented reality electronic sand table","DOI":"10.1117\/12.2668367","Publication year":2023,"Abstract":"Traditional interaction techniques such as mouse and keyboard or gestures for a touch screen are utilized for a 2D electronic sand table, which don't have enough degrees of freedom (DoFs) for operations on a 3D graph. Gestures interaction method has received increasing attention for 3D electronic sand tables with solid terrain construction and virtual-real fusion display techniques. This paper researches an interactive feedback method with gestures based on contextual information, gestures controlling virtual targets and a gesture interaction modal are designed. Users can control virtual targets with intuitive and feasible interaction methods through the real calculation of a gesture recognition engine and a gesture understanding engine, and the displaying status of the virtual targets can be updated in real time. &copy; 2023 SPIE.","LowLevel":"degrees of freedom;engines;mammals;touch screens;user interfaces","MidLevel":"power and energy;human-computer interaction;robotics;input","HighLevel":"end users and user experience;technology;industries","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[172,242,250,348,66,142,419,445,340,80,208,270,118,279,456,432,412,189,132,467],"Top20Abs":[172,250,66,142,419,348,445,242,208,456,80,340,270,118,279,41,432,412,467,153],"Top20Tags":[176,0,469,169,332,74,158,409,344,393,141,487,465,481,470,235,228,223,72,201],"Citation":"Li, H., Huang, Z., & Guo, C. (2023). An interactive method with gestures oriented toward the augmented reality electronic sand table. International Conference on Electronic Information Engineering and Computer Science (EIECS 2022). https:\/\/doi.org\/10.1117\/12.2668367\n"},{"Title":"Gamified Augmented Reality Mobile Application for Tourism in Kuching","DOI":"10.1007\/978-981-19-8406-8_27","Publication year":2023,"Abstract":"Augmented Reality (AR) for tourism utilizing mobile application mostly features scanning prefixed image marker by tourist in order to retrieve additional multimedia content for learning. This paper proposes the use of similar AR application for tourism that extends beyond learning to include gamification for the enhanced tourist&rsquo;s experience. Majority of tourism AR uses marker-based method like scanning QR code to load virtual objects on the screen. The current study will explore marker-less method using spatial anchor. Furthermore, functionality for site operator, an aspect often neglected in literature, like the ability to dynamically add content without relying on developer, will be categorically examined. Platforms, frameworks and services used include Unity, AR Core or AR Foundation and Azure cloud services. This would culminate in implementation and evaluation of an AR mobile application prototype that benefits both tourists and site operators towards proliferation of gamified AR tourism in Kuching city of Sarawak state in Malaysia. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"mobile computing;tourism","MidLevel":"cultural heritage;telecommunication","HighLevel":"industries","Venue":"Lect. Notes Electr. Eng.","Top20AbsAndTags":[90,166,25,163,41,97,64,48,49,116,53,20,105,87,135,42,194,134,111,179],"Top20Abs":[166,90,163,41,49,97,64,116,48,53,87,135,134,25,179,392,105,20,6,111],"Top20Tags":[157,25,194,222,20,192,156,233,56,73,1,215,279,77,30,167,105,395,87,302],"Citation":"Tay, C. A., Dominic, B. B., Ho, H. I., Annuar, N., & Saferinor, N. E. M. (2023). Gamified Augmented Reality Mobile Application for Tourism in Kuching. Proceedings of the 9th International Conference on Computational Science and Technology, 355\u2013366. https:\/\/doi.org\/10.1007\/978-981-19-8406-8_27\n"},{"Title":"Prevalence of oculomotor changes following the near work in stereoscopic augmented reality","DOI":"10.1117\/12.2668376","Publication year":2023,"Abstract":"Extended reality human factors studies commonly utilize an approach in which group behavior is reported. This possibly masks the true prevalence of oculomotor changes that appear in response to stereoscopic augmented reality. Thus, the study aimed to elucidate the prevalence, direction, and magnitude of oculomotor changes after the near work in stereoscopic augmented reality. The task of fifty-three subjects (18-28 years old, normal visual acuity, no vision complaints) was to type the text displayed at 60 cm as accurately and quickly as possible. Each subject participated in two sessions &ndash; the text was displayed in stereoscopic augmented reality and on the computer screen. Clinical assessments of visual parameters were performed before and immediately after 30 minutes of the task. As a result, individual variations were found in the magnitude and direction of oculomotor changes after the near work. After the use of stereoscopic augmented reality, adverse changes in vergence and accommodation were observed in about 40% of the group. Despite the prevalence of adverse oculomotor changes being similar in the case of text displayed on the computer screen, only less than 20% of the group showed a decline of visual parameters in both viewing conditions. The exploratory study highlights the necessity to consider individual variations in visual responses and identify groups that might benefit or be disadvantaged in using stereoscopic augmented reality technologies. &copy; 2023 SPIE.","LowLevel":"stereo image processing","MidLevel":"computer vision;data","HighLevel":"technology","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[211,353,75,248,197,164,455,383,133,171,320,185,245,488,278,105,154,410,281,341],"Top20Abs":[211,75,248,197,353,164,455,171,383,245,133,313,185,320,234,154,410,77,21,105],"Top20Tags":[438,283,488,157,307,24,138,457,433,281,467,446,207,290,480,464,479,74,475,315],"Citation":"Zizlane, K., Alksnis, R., & Pladere, T. (2023). Prevalence of oculomotor changes following the near work in stereoscopic augmented reality. Advances in Display Technologies XIII. https:\/\/doi.org\/10.1117\/12.2668376\n"},{"Title":"The ARETE Ecosystem for&nbsp;the&nbsp;Creation and&nbsp;Delivery of&nbsp;Open Augmented Reality Educational Resources: The PBIS Case Study","DOI":"10.1007\/978-3-031-29800-4_57","Publication year":2023,"Abstract":"Augmented reality (AR) is rapidly emerging as an increasingly useful technology in educational settings. In the ARETE (Augmented Reality Interactive Educational System) H2020 project, consortium members designed and implemented an ecosystem aimed at supporting teachers in building a collaborative learning environment through the use of AR in order to improve educational experiences. In particular, one of the pilot projects aims to introduce AR into school behavior lessons for the first time, leveraging the Positive Behaviour Intervention and Support (PBIS) methodology. Specifically, in this paper we will discuss the proposed architecture within the ARETE project that incorporates AR technology into the learning process of behavior lessons to support the teaching, practice and reinforcement phases of expected behaviors. Through the combination of different technologies and systems, it is possible to create an example of a technological and innovative ecosystem designed for creating behavioral lessons in AR. &copy; 2023, The Author(s).","LowLevel":"computer aided instruction;ecosystems;engineering education;learning systems","MidLevel":"education;farming and natural science;medical;training","HighLevel":"industries;use cases","Venue":"Commun. Comput. Info. Sci.","Top20AbsAndTags":[62,454,188,57,196,103,163,46,124,56,44,87,197,363,136,241,82,171,51,312],"Top20Abs":[62,188,57,197,163,46,44,196,103,124,56,87,454,82,41,241,136,51,312,222],"Top20Tags":[369,454,161,477,485,467,434,25,188,446,478,373,135,442,207,483,171,307,469,443],"Citation":"Farella, M., Arrigo, M., Tosto, C., Seta, L., Chifari, A., Mangina, E., Psyrra, G., Dom\u00ednguez, A., Pacho, G., Wild, F., Bowers, L., Hillman, R., Goei, S. L., Denaro, P., Dhrami, D., & Chiazzese, G. (2023). The ARETE Ecosystem for\u00a0the\u00a0Creation and\u00a0Delivery of\u00a0Open Augmented Reality Educational Resources: The PBIS Case Study. Communications in Computer and Information Science, 760\u2013775. https:\/\/doi.org\/10.1007\/978-3-031-29800-4_57\n"},{"Title":"Perception of Virtual Agents as Communicators in Virtual vs. Augmented Reality by a Male Sample","DOI":"10.1007\/978-3-031-30933-5_3","Publication year":2023,"Abstract":"Virtual agents are often employed in persuasive applications, and different studies in the literature have shown that the gender of the agent may have an impact on how users perceive the agent as a communicator. This paper adds a new variable to this line of research, considering the possible effects of presenting the agent in Virtual Reality (VR) vs. Augmented Reality (AR). We measured attentional allocation, perceived affective understanding, speaker credibility and speaker strength. While attentional allocation was the same in all conditions, an interesting pattern emerged for the other variables. The transition from VR to AR apparently changed the perception of some communicator aspects to the advantage of the female virtual agent. We also found associations between participants&rsquo; personality traits (in particular, extraversion) and perception of the agent. The paper describes and discusses these findings. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"other","MidLevel":"other","HighLevel":"other","Venue":"Lect. Notes Comput. Sci.","Top20AbsAndTags":[175,454,200,195,237,60,188,25,170,471,440,257,189,155,252,125,2,123,410,436],"Top20Abs":[175,454,200,237,195,60,188,170,257,440,155,189,410,252,213,161,339,436,160,316],"Top20Tags":[344,2,487,25,458,7,462,1,285,471,20,200,469,485,461,459,30,138,466,194],"Citation":"Serafini, M., & Chittaro, L. (2023). Perception of Virtual Agents as Communicators in Virtual vs. Augmented Reality by a Male Sample. Lecture Notes in Computer Science, 36\u201349. https:\/\/doi.org\/10.1007\/978-3-031-30933-5_3\n"},{"Title":"Compact digital micromirror device (DMD) based optical architecture for augmented reality (AR) glasses","DOI":"10.1117\/12.2655671","Publication year":2023,"Abstract":"Size and efficacy are two of the main challenges of an AR display. A compact form factor and low power consumption are highly desirable in AR glasses. Self-emitting display technologies such as OLED and MicroLED do not need illumination thus enabling simpler and smaller optics, but their efficacy is quite low, especially for pixel sizes of 5um or below. Spatial Light Modulator (SLM) based technologies, such as DMD, LCoS (Liquid Crystal on Silicon), and LBS (Laser Beam Scanning), take advantage of high efficacy light sources and can achieve a high brightness AR display combining low power consumption and high pixel density, which is highly preferable in many applications. The DMD, together with the efficient and ever-improving LED light source, has been highly favored in the battery-powered portable projection display market, due to its high efficacy in a compact form factor as well as its insensitivity to polarization. Here we present a compact optical engine architecture for AR glasses, taking into consideration both size and efficacy. The design simplifies the traditional DMD illumination to a compact size targeting a thin profile similar to a backlit LCD. This compact size is achieved with a small compromise to engine efficacy. Multiplying the pixel density with a glass plate actuator offers a performance improvement that easily justifies the slight increase in size. This architecture offers an excellent optical engine option for AR glasses which is both compact and high performance. &copy; 2023 SPIE.","LowLevel":"approximation theory;architecture;electric power utilization;engines;helmet mounted displays;laser beams;light modulation;light modulators;liquid crystals;network architecture;organic light emitting diodes;pixels","MidLevel":"artificial intelligence;computer vision;power and energy;graphics;construction;chemical;wearables;input;optics;networks;semiconductors;display technology","HighLevel":"industries;technology;displays","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[293,158,235,162,393,278,248,332,169,422,228,201,413,41,317,324,364,343,444,277],"Top20Abs":[293,158,248,235,162,278,422,393,41,201,332,343,169,324,228,277,364,444,376,111],"Top20Tags":[235,430,162,201,228,223,169,413,74,332,149,158,324,486,431,438,290,444,379,317],"Citation":"Sheng, Z., Zhou, X., & Shaw, S. A. (2023). Compact digital micromirror device (DMD)-based optical architecture for augmented reality (AR) glasses. Emerging Digital Micromirror Device Based Systems and Applications XV. https:\/\/doi.org\/10.1117\/12.2655671\n"},{"Title":"Non-overlayed Guidance in&nbsp;Augmented Reality: User Study in&nbsp;Radio-Pharmacy","DOI":"10.1007\/978-3-031-27199-1_52","Publication year":2023,"Abstract":"In many traditional industries, production instructions are usually provided on paper. Past research has shown the effectiveness of Augmented Reality (AR) for virtual user guidance in various cases. Usually, the main focus lies on 3D overlays and spatially anchored tokens to guide the user. Unfortunately, tracking small and moving objects is not always feasible in highly dynamic or complex environments. Additionally, the setup of anchors, 3D models and guidance procedures is often time-consuming and problem-specific. This study addresses such scenarios and provides empirical results of AR user guidance without employing overlays or object tracking. Therefore, we developed two AR concepts that guide the user using either 2D illustrations or 3D models. For evaluation, we designed a user study in the field of radio-pharmaceuticals, assessing quantitative measurements, usability and cognitive load. The conducted user study indicates that AR can also improve the effectiveness of user guidance in scenarios where direct 3D overlays or object tracking approaches are not feasible. The presented 2D and 3D AR concepts performed similarly, while both lead to fewer errors, faster execution time, and lower cognitive load than the paper instructions. Therefore, to reduce the effort required to create 3D instructions, the use of 2D illustrations could often be the more efficient choice. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"tracking;wearable technology","MidLevel":"computer vision;medical;inspection, safety and quality;wearables;human-computer interaction","HighLevel":"end users and user experience;displays;use cases;industries;technology","Venue":"Lect. Notes Comput. Sci.","Top20AbsAndTags":[43,435,135,467,151,340,79,137,173,87,152,419,50,191,179,97,82,130,488,53],"Top20Abs":[43,151,135,87,419,79,50,137,82,194,152,130,432,191,173,435,415,97,340,179],"Top20Tags":[467,323,475,247,443,192,435,137,439,206,24,301,68,126,345,440,135,445,101,288],"Citation":"Simmen, Y., Eggler, T., Legath, A., Agotai, D., & Cords, H. (2023). Non-overlayed Guidance in\u00a0Augmented Reality: User Study in\u00a0Radio-Pharmacy. Lecture Notes in Computer Science, 516\u2013526. https:\/\/doi.org\/10.1007\/978-3-031-27199-1_52\n"},{"Title":"Detection and&nbsp;Pose Estimation of&nbsp;Flat, Texture-Less Industry Objects on&nbsp;HoloLens Using Synthetic Training","DOI":"10.1007\/978-3-031-31438-4_37","Publication year":2023,"Abstract":"Current state-of-the-art 6d pose estimation is too compute intensive to be deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both used for an increasing number of augmented reality applications. The quality of AR is greatly dependent on its capabilities to detect and overlay geometry within the scene. We propose a synthetically trained client-server-based augmented reality application, demonstrating state-of-the-art object pose estimation of metallic and texture-less industry objects on edge devices. Synthetic data enables training without real photographs, i.e. for yet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted sorting task, and quantitative evaluation on both renderings, as well as real-world data recorded on HoloLens 2, sheds light on its real-world applicability. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"petroleum reservoir evaluation;windows operating system","MidLevel":"education;power and energy;construction;farming and natural science;other","HighLevel":"industries;other","Venue":"Lect. Notes Comput. Sci.","Top20AbsAndTags":[425,283,387,440,20,12,2,460,280,139,151,120,486,454,116,301,405,267,189,155],"Top20Abs":[425,151,20,387,280,440,139,189,415,12,454,267,116,405,335,486,123,111,8,460],"Top20Tags":[445,283,440,2,155,20,469,90,212,295,486,1,192,156,30,187,74,459,239,177],"Citation":"P\u00f6llabauer, T., R\u00fccker, F., Franek, A., & Gorschl\u00fcter, F. (2023). Detection and\u00a0Pose Estimation of\u00a0Flat, Texture-Less Industry Objects on\u00a0HoloLens Using Synthetic Training. Lecture Notes in Computer Science, 569\u2013585. https:\/\/doi.org\/10.1007\/978-3-031-31438-4_37\n"},{"Title":"Evaluating the Extension of Wall Displays with AR for Collaborative Work","DOI":"10.1145\/3544548.3580752","Publication year":2023,"Abstract":"Wall displays are well suited for collaborative work and are often placed in rooms with ample space in front of them that remains largely unused. Augmented Reality (AR) headsets can seamlessly extend the collaboration space around the Wall. Nevertheless, it is unclear if extending Walls with AR is effective and how it may affect collaboration. We first present a prototype combining a Wall and AR headsets to extend the Wall workspace. We then use this prototype to study how users utilize the virtual space created in AR. In an experiment with 24 participants, we compare how pairs solve collaborative tasks with the Wall alone and with Wall+AR. Our qualitative and quantitative results highlight that with Wall+AR, participants use the physical space in front and around the Wall extensively, and while this creates interaction overhead, it does not impact performance and improves the user experience.","LowLevel":"display devices;groupware","MidLevel":"collaboration;display technology","HighLevel":"displays;use cases","Venue":"CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[304,64,303,69,17,235,104,151,53,279,102,219,267,211,197,440,100,87,203,248],"Top20Abs":[304,64,151,235,303,104,197,102,248,53,440,73,211,87,118,59,27,17,135,49],"Top20Tags":[69,29,329,240,303,279,17,304,259,203,131,239,337,415,104,267,263,125,100,142],"Citation":"James, R., Bezerianos, A., & Chapuis, O. (2023). Evaluating the Extension of Wall Displays with AR for Collaborative Work. Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544548.3580752\n"},{"Title":"Assessing Enterprise Level, Augmented Reality Solutions for Electronics Manufacturing","DOI":"10.4271\/2023-01-0098","Publication year":2023,"Abstract":"With the growth of Industry 4.0 in recent years, Augmented Reality (AR) technologies are changing the way operators work by increasing their efficiency and operational performance. A common use of AR is providing operators helpful work instructions for assembly by presenting relevant digital information in the context of the physical environment. These AR experiences can be viewed via several devices such as mobile, wearable, and stationary devices, each being useful for different applications. While in the experience, instructions are provided by means of 3D animation, text, images, and interactive buttons, all of which are directly overlaid onto the physical product or equipment being worked on. This work presents a closed-loop, enterprise connected, AR system for post end Printed Circuit Board (PCB) assembly work instructions. The system is designed to work with a stationary device, allows for varying types of PCB\"s, provides overlaid instruction, and logs important information to an enterprise system, such as overall cycle time, step cycle time, number of errors, type of error, and who performed the assembly. A comparison was made for single cell manual assembly PCB work instructions using both an Industry 4.0 driven system and a more traditional manufacturing system which used packets, tracers, Manufacturing Execution System (MES), and PDF instructions. Discovered benefits of an enterprise connected AR system included increased throughput and utilization, improved communication between operators and support, reduced overtime costs, reduced defects, reduced non-value-added secondary inspections, nearly 50% increase in ergonomics, reduced labor due to rework, and improved corrective actions due to granularity between steps compared to total operation. This AR driven solution transformed the single cell manual assembly to be more informed, make better decisions, and help operator productivity. &copy; 2023 SAE International. All rights reserved.","LowLevel":"assembly;ergonomics;industry 4.0;printed circuit boards","MidLevel":"internet of things;manufacturing;human factors;semiconductors","HighLevel":"end users and user experience;technology;industries","Venue":"SAE Techni. Paper.","Top20AbsAndTags":[370,179,84,174,81,189,45,159,118,2,429,190,69,131,172,135,261,86,129,311],"Top20Abs":[370,179,174,189,81,118,159,2,84,190,45,69,86,429,135,261,131,129,172,49],"Top20Tags":[315,447,361,227,289,189,215,31,129,159,332,43,38,429,475,344,470,66,2,0],"Citation":"Becerra, E. J., Hovanski, Y., Tenny, J., & Peterson, R. (2023). Assessing Enterprise Level, Augmented Reality Solutions for Electronics Manufacturing. SAE Technical Paper Series. https:\/\/doi.org\/10.4271\/2023-01-0098\n"},{"Title":"Gaze Depth Estimation for In-vehicle AR Displays","DOI":"10.1145\/3582700.3583707","Publication year":2023,"Abstract":"In our previous study, we proposed a method for judging whether the user is gazing at a semi-transparent virtual object or real objects behind it in augmented reality environments. This paper shows that the accuracy of our method can be improved by selecting the optimal thresholds for the fixation detection. Fourteen participants experienced a virtual reality environment in which there were a transparent subway map and buildings behind it in the distance of 2 m and 15 m away from each participant, respectively. As a result, the accuracy of our method has achieved 88.3 % and improved by 13.8 percentage points from the previous 74.5 %.","LowLevel":"gaze tracking;human factors","MidLevel":"computer vision;human factors;input","HighLevel":"end users and user experience;technology","Venue":"AHs23: Proceedings of the Augmented Humans Conference 2023","Top20AbsAndTags":[208,409,79,98,435,221,142,116,132,274,340,81,400,345,415,7,321,237,80,242],"Top20Abs":[208,409,435,221,98,79,142,116,237,440,20,400,80,345,415,340,274,318,438,109],"Top20Tags":[72,195,259,206,37,79,250,415,370,213,21,144,260,98,220,276,32,123,12,66],"Citation":"Uramune, R., Sawamura, K., Ikeda, S., Ishizuka, H., & Oshiro, O. (2023). Gaze Depth Estimation for In-vehicle AR Displays. Augmented Humans Conference. https:\/\/doi.org\/10.1145\/3582700.3583707\n"},{"Title":"Intraocular augmented reality display with retinal prosthesis","DOI":"10.1117\/12.2648010","Publication year":2023,"Abstract":"We present an intraocular augmented reality display featuring retinal prostheses in association with bionic vision processing. Unlike conventional retinal prostheses, whose electrodes are spaced equidistantly, our solution is to rearrange the electrodes to match the distribution of ganglion cells. To naturally imitate the human vision, a scheme of bionic vision processing is developed. On top of a three-dimensional eye model, our bionic vision processing is able to visualize the monocular image, binocular image fusion, and parallax-induced depth map. &copy; 2023 SPIE.","LowLevel":"bionics;electrodes;image fusion;ophthalmology;prosthetics","MidLevel":"computer vision;medical;other","HighLevel":"industries;technology;other","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[453,71,290,247,74,317,397,81,0,76,201,432,220,223,158,425,437,413,323,278],"Top20Abs":[71,247,81,397,453,76,290,220,74,110,99,421,291,190,379,437,223,300,201,119],"Top20Tags":[488,0,207,464,74,453,432,433,285,438,43,170,463,290,317,480,323,169,431,201],"Citation":"Zou, S. P., Xi, N., Ye, J., Chen, C. P., Chu, Q., & Hu, H. (2023). Intraocular augmented reality display with retinal prosthesis. Advances in Display Technologies XIII. https:\/\/doi.org\/10.1117\/12.2648010\n"},{"Title":"Location-based AR for Social Justice: Case Studies, Lessons, and Open Challenges","DOI":"10.1145\/3544549.3573855","Publication year":2023,"Abstract":"Dear Visitor and Charleston Reconstructed were location-based augmented reality (AR) experiences created between 2018 and 2020 dealing with two controversial monument sites in the US. The projects were motivated by the ability of AR to 1) link layers of context to physical sites in ways that are otherwise difficult or impossible and 2) to visualize changes to physical spaces, potentially inspiring changes to the spaces themselves. We discuss the projects' motivations, designs, and deployments. We reflect on how physical changes to the projects' respective sites radically altered their outcomes, and we describe lessons for future work in location-based AR, particularly for projects in contested spaces.","LowLevel":"history;location based services","MidLevel":"geospatial;liberal arts","HighLevel":"industries;technology","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[44,267,184,35,329,124,304,30,6,64,29,14,31,418,82,24,229,27,454,147],"Top20Abs":[44,184,35,304,64,329,124,418,31,6,82,267,454,440,374,183,229,155,91,178],"Top20Tags":[29,3,14,147,30,16,97,164,24,302,6,105,49,218,236,27,127,129,93,77],"Citation":"Schroeder, H., Tokanel, R., Qian, K., & Le, K. (2023). Location-based AR for Social Justice: Case Studies, Lessons, and Open Challenges. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3573855\n"},{"Title":"Safety in the Laboratory-An Exit Game Lab Rally in Chemistry Education","DOI":"10.3390\/computers12030067","Publication year":2023,"Abstract":"The topic of safety in chemistry laboratories in schools is crucial, as severe accidents in labs occur worldwide, primarily due to poorly trained individuals and improper behavior. One reason for this could be that the topic is often dry and boring for students. One solution to this problem is engaging students more actively in the lesson using a game format. In this publication, we present an augmented-reality-supported exit game in the form of a laboratory rally and the results of a pilot study that examined the use of the rally in terms of technology acceptance and intrinsic motivation. The study involved 22 students from a general high school. The study results show a high level of technology acceptance for the augmented reality used, as well as good results in terms of the intrinsic motivation triggered by the lesson.","LowLevel":"computer aided instruction;educational institutions;human factors","MidLevel":"education;human factors;training","HighLevel":"industries;use cases;end users and user experience","Venue":"Computers (Switzerland)","Top20AbsAndTags":[320,342,62,184,57,312,185,136,82,83,414,124,210,204,306,272,87,145,163,369],"Top20Abs":[320,342,312,62,184,57,163,369,88,185,82,136,124,272,87,414,83,359,306,56],"Top20Tags":[204,145,51,154,147,320,33,405,114,195,410,103,62,171,241,87,57,306,88,117],"Citation":"Krug, M., & Huwer, J. (2023). Safety in the Laboratory\u2014An Exit Game Lab Rally in Chemistry Education. Computers, 12(3), 67. https:\/\/doi.org\/10.3390\/computers12030067\n"},{"Title":"Impact of User Mobility on Attentional Tunneling in Handheld AR","DOI":"10.1145\/3544549.3585692","Publication year":2023,"Abstract":"Attentional tunneling in augmented reality (AR) refers to a phenomenon where users pay disproportionately high attention to virtual content while ignoring events in the real world. Although many handheld AR applications are deployed in public places where users are expected to be attentive to their surroundings while walking freely, the effect of user mobility on attentional tunneling is not yet ascertained. To investigate the effects of user mobility, we designed a 2x3 study comparing two postures (stationary and walking) with three AR tasks (a non-cognitive baseline task, a working memory task, and a perceptual monitoring task). The results suggest that walking significantly magnifies the tunneling effect, provided the user is engaged with a task that is associated with the virtual content being viewed. We demonstrate the significance of considering user mobility when evaluating AR application usage and provide guidelines for designers to evaluate the application in such scenarios.","LowLevel":"human factors","MidLevel":"human factors","HighLevel":"end users and user experience","Venue":"CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems","Top20AbsAndTags":[19,365,70,305,203,410,60,269,14,189,267,242,187,15,213,87,79,163,135,37],"Top20Abs":[19,70,305,269,14,410,189,60,151,203,365,242,419,135,267,187,107,15,248,87],"Top20Tags":[365,198,21,196,154,114,213,339,13,62,36,147,66,57,383,87,163,48,309,219],"Citation":"Parmar, M., & Silpasuwanchai, C. (2023). Impact of User Mobility on Attentional Tunneling in Handheld AR. Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. https:\/\/doi.org\/10.1145\/3544549.3585692\n"},{"Title":"Panoramic ultra-high-definition augmented reality 360&deg; color holograms as inclusive tools in transportation","DOI":"10.1117\/12.2657447","Publication year":2023,"Abstract":"Modern touch control infotainment systems in vehicles present distractions to drivers and endanger road safety. Current industrial head-up displays (HUDs) require the driver to shift the gaze from the road towards a region on the 2D windscreen. Panoramic augmented reality holographic color projections in could prevent driver distraction. This is an inclusive tool to incorporate all members of society into the transportation sector. A 4k color augmented reality holographic automotive head-up display was developed to project road obstacles in 360&deg; in the driver's field of view. This technology could be useful for drivers, including elderly and disabled populations. &copy; 2023 SPIE.","LowLevel":"color;holograms;holographic displays;liquid crystals;roads and streets;silicon;three-dimensional displays","MidLevel":"transportation;graphics;chemical;semiconductors;display technology","HighLevel":"industries;technology;displays","Venue":"Proc SPIE Int Soc Opt Eng","Top20AbsAndTags":[288,276,444,344,278,481,413,158,235,225,332,228,317,99,95,265,441,290,162,351],"Top20Abs":[288,276,444,481,344,225,265,99,235,206,278,95,228,72,351,332,242,270,0,317],"Top20Tags":[278,413,290,162,470,235,223,332,481,450,228,74,344,158,169,441,201,438,141,431],"Citation":"Skirnewskaja, J., Montelongo, Y., & Wilkinson, T. D. (2023). Panoramic ultra-high-definition augmented reality 360\u00b0 color holograms as inclusive tools in transportation. Practical Holography XXXVII: Displays, Materials, and Applications. https:\/\/doi.org\/10.1117\/12.2657447\n"},{"Title":"Application of Augmented Reality for the Monitoring of Parameters of Industrial Instruments","DOI":"10.1007\/978-981-19-8493-8_51","Publication year":2023,"Abstract":"The development of this augmented reality application on mobile devices with Android operating system is oriented for the identification and visualization of industrial equipment and instruments, the interaction through the characteristics and labels of its parts, exploded views and assembly that make up a process of a level control panel. The generation of this mobile application was done with the use of recognition of each element through the generation of a QR code, to be subsequently focused on 3D models using CAD software linked through a multiplatform. Additionally, the simulation and visualization of data from industrial sensors in real time of the process are based on the operation of the instruments, and these generated data contribute to the development of skills for the manipulation and control of industrial processes. &copy; 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","LowLevel":"application programs;computer aided design;data visualization;mobile computing;process control;three dimensional computer graphics;visualization","MidLevel":"telecommunication;data;graphics;industrial equipment;developers;human-computer interaction","HighLevel":"end users and user experience;technology;industries","Venue":"Lect. Notes Electr. Eng.","Top20AbsAndTags":[178,307,80,457,463,411,370,4,424,467,443,148,134,275,207,127,486,311,464,216],"Top20Abs":[178,463,370,411,80,127,424,467,4,134,486,207,311,275,307,20,443,131,315,174],"Top20Tags":[457,155,307,443,183,464,178,488,467,438,446,433,290,74,399,283,24,278,369,410],"Citation":"Navas, J. L., Toapanta, J. L., & Freire, L. O. (2023). Application of Augmented Reality for the Monitoring of Parameters of Industrial Instruments. Computational Intelligence for Engineering and Management Applications, 689\u2013701. https:\/\/doi.org\/10.1007\/978-981-19-8493-8_51\n"},{"Title":"Augmented reality guidance platform for epicardial access: a phantom study","DOI":"10.1117\/12.2653902","Publication year":2023,"Abstract":"Percutaneous epicardial access for epicardial ablation and mapping of cardiac arrhythmias is being performed more and more often. Unfortunately, complications such as injury to surrounding structures have been reported. Despite the current imaging techniques, it is still difficult to guarantee sufficient ablation accuracy. Head-mounted-display (HMD) augmented reality (AR) overlay and guidance has the potential to reduce the risk of complications. The objective of this study was to evaluate the accuracy and performance of an AR-guided epicardial puncture for catheter ablation of ventricular tachycardia. An AR software tool was designed to render real-time needle trajectories and 3D patient-specific organs. Registration of preoperative data is realized by attaching four AR patterns to the skin of the patient. Needle tracking is realized by attaching one AR pattern to the end of the needle's base. The ideal trajectory through the pericardial space and patient-specific organs was planned and segmented on preoperative CT. The application's accuracy was evaluated in a phantom study. Seven operators performed needle puncture with and without the use of the AR system. Placements errors were measured on postprocedural CT. With the use of the proposed AR-based guidance, postprocedure CT revealed an error at the puncture site of 3.67&plusmn;2.78 mm. At the epicardial interface, the error increased to 7.78&plusmn;2.36 mm. The angle of the actual trajectory deviated on average 4.82&plusmn;1.48&#9702; from the planned trajectory. The execution time was on average 34.0 &plusmn; 25.1 s, hence introducing no significant delay at an overall superior performance level compared to without AR-guided puncturing. The proposed AR platform has the potential to facilitate percutaneous epicardial access for epicardial ablation and mapping of cardiac arrhythmias by improving needle insertion accuracy. &copy; 2023 SPIE.","LowLevel":"ablation;computerized tomography;diseases;errors;helmet mounted displays;mapping;needles;phantoms;trajectories","MidLevel":"human factors;medical;wearables;navigation;display technology","HighLevel":"displays;industries;use cases;end users and user experience","Venue":"Progr. Biomed. Opt. Imaging Proc. SPIE","Top20AbsAndTags":[43,488,186,189,152,432,6,2,69,323,294,364,232,180,49,217,87,135,356,151],"Top20Abs":[43,488,189,6,152,432,186,151,323,232,69,87,49,135,294,0,178,180,128,102],"Top20Tags":[270,43,443,344,2,488,440,21,450,170,470,283,206,278,38,341,180,486,307,212],"Citation":"Bamps, K., Bertels, J., Minten, L., Puvrez, A., Coudyzer, W., De Buck, S., & Ector, J. (2023). Augmented reality guidance platform for epicardial access: a phantom study. Medical Imaging 2023: Image-Guided Procedures, Robotic Interventions, and Modeling. https:\/\/doi.org\/10.1117\/12.2653902\n"},{"Title":"ReadAR, Playful Book Finding Through Peer Book Reviews for&nbsp;Multi-faceted Characters in&nbsp;AR","DOI":"10.1007\/978-3-031-30933-5_4","Publication year":2023,"Abstract":"One important element to provide reading enjoyment and to persuade children to read (more) is providing children with books that fit their interests. We structure filtering and recommendation of books in a playful way via animated 3D characters. These characters have an unusual mix of characteristics that can be related to categories of books, while at the same time aiming for overcoming a filter bubble effect. In our Augmented Reality application the characters playfully &lsquo;structure&rsquo; the process of book review and searching. We tested the prototype during two within-subject sessions, testing reflecting on the book as well as searching for books, with respectively 18 and 15 participants. When comparing to a regular &lsquo;writing a book report&rsquo;-approach, children indicated they would more likely want to use the app again for providing feedback about the book to peers as well as for finding books. Although they wanted to look again for the books and watch the accompanying localised video reviews from their peers, almost half did not want to record videos themselves again which points out a clear challenge for future improvements. &copy; 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","LowLevel":"well testing","MidLevel":"utilities;inspection, safety and quality","HighLevel":"industries;use cases","Venue":"Lect. Notes Comput. Sci.","Top20AbsAndTags":[284,275,115,105,443,180,188,34,83,161,140,454,25,316,228,292,87,1,455,136],"Top20Abs":[284,275,115,443,180,105,188,83,34,161,454,140,292,25,136,458,457,448,361,228],"Top20Tags":[482,235,443,345,189,1,155,466,290,68,322,307,475,30,323,256,206,312,481,194],"Citation":"Wintermans, L., van Delden, R., & Reidsma, D. (2023). ReadAR, Playful Book Finding Through Peer Book Reviews for\u00a0Multi-faceted Characters in\u00a0AR. Lecture Notes in Computer Science, 50\u201364. https:\/\/doi.org\/10.1007\/978-3-031-30933-5_4\n"},{"Title":"Indoor Place Prediction on Smart Phones","DOI":"10.1145\/3560905.3568062","Publication year":2022,"Abstract":"High-accuracy and low-latency indoor place prediction for mobile users is crucial to enable applications for assisted living, emergency services, smart homes, and augmented reality. Previous studies on indoor place prediction use complex infrastructure with multiple visual\/wireless anchors or multiple wireless access points. These localization techniques are difficult to deploy, may negatively impact user privacy through location tracking, and their data collection is not suitable for personalized place prediction. To solve these challenges, this paper proposes GoPlaces, a novel app that fuses inertial sensor data with WiFi-RTT estimated distances to predict the future indoor places visited by a user. GoPlaces does not require any infrastructure, except for one cheap off-the-shelf WiFi access point that supports ranging with RTT. In addition, it enables personalized place naming and prediction through its on-the-phone data collection and protects users' location privacy because user's data never leaves the phone. GoPlaces uses an attention-based bidirectional long short-term memory model to detect user's current trajectory, which is then used together with historical information stored in a prediction tree to infer user's future places. We implemented GoPlaces in Android and evaluated it in several indoor spaces. The experimental results demonstrate prediction accuracy as high as 92%, low latency, and low resource consumption on the phones.","LowLevel":"data privacy;emergency services;indoor navigation;indoor radio;mobile computing;mobile handsets;recurrent neural nets;smartphones;wireless lan","MidLevel":"artificial intelligence;emergency response;policy;telecommunication;liberal arts;navigation;input;networks","HighLevel":"industries;technology;business;use cases","Venue":"SenSys '22: Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems","Top20AbsAndTags":[259,358,484,166,38,151,268,63,409,453,68,81,77,248,267,396,195,208,419,410],"Top20Abs":[259,358,484,166,453,38,409,63,268,68,81,151,396,208,4,267,77,404,195,447],"Top20Tags":[129,38,147,347,339,93,377,100,325,209,82,111,104,27,416,271,105,410,335,216],"Citation":"Sen, P., Jiang, X., Wu, Q., Talasila, M., Hsu, W.-L., & Borcea, C. (2022). Indoor Place Prediction on Smart Phones. Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems. https:\/\/doi.org\/10.1145\/3560905.3568062\n"},{"Title":"Interactive application for training orthopedic residents to perform knee arthroscopy procedure","DOI":"10.1145\/3565494.3565528","Publication year":2022,"Abstract":"A knee arthroscopy exploration is a minimally invasive surgical procedure; but training residents to perform it is complex and expensive. In addition, years of training are required to perform the procedure correctly and safely. An interactive Mixed Reality application was developed with the purpose of providing an alternative for the procedures training, which seeks to create a practice environment for training surgeons while promoting safe learning. Hololens 2 was used to project holograms of the procedure scenario, and a set of optitrack tracking cameras were placed to track the instruments and simulate the operation. In the design process, a CTA (cognitive task analysis) was carried out, which functions as a fundamental guide to establishing a step-by-step procedure and finding the most significant difficulties when executing it. In the interaction with the application, the user must complete several tasks and simulate an arthroscopic knee exploration procedure within a virtual scenario. Tests were conducted with medical students, general practitioners, and orthopedic experts to test the application's performance as an educational tool. The results showed that the application is a valuable tool to improve the learning of this type of procedure,providing a suitable practice environment for skill improvement.","LowLevel":"biomedical education;cognition;computer based training;medical computing;orthopedics;surgery;task analysis","MidLevel":"education;training;human factors;medical;farming and natural science","HighLevel":"industries;use cases;end users and user experience","Venue":"MexIHC '22: 9th Mexican International Conference on Human-Computer Interaction","Top20AbsAndTags":[432,168,286,112,357,306,326,294,51,233,436,320,414,140,101,374,163,189,114,82],"Top20Abs":[432,168,286,357,112,294,326,374,306,414,189,207,163,51,233,101,188,91,403,303],"Top20Tags":[140,117,390,112,376,233,47,145,9,54,168,436,301,113,262,87,152,306,51,294],"Citation":"Nunez De Villavicencio Castineyra, J. A., Rodriguez Chaparro, S., Bautista Rojas, L., & Bautista Rozo, L. (2022). Interactive application for training orthopedic residents to perform knee arthroscopy procedure. 9th Mexican International Conference on Human-Computer Interaction. https:\/\/doi.org\/10.1145\/3565494.3565528\n"},{"Title":"Interactive Parametric Design and Robotic Fabrication within Mixed Reality Environment","DOI":"10.3390\/app122412797","Publication year":2022,"Abstract":"In this study, a method, in which parametric design and robotic fabrication are combined into one unified framework, and integrated within a mixed reality environment, where designers can interact with design and fabrication alternatives, and manage this process in collaboration with other designers, is proposed. To achieve this goal, the digital twin of both design and robotic fabrication steps was created within a mixed-reality environment. The proposed method was tested on a design product, which was defined with the shape-grammar method using parametric-modeling tools. In this framework, designers can interact with both design and robotic-fabrication parameters, and subsequent steps are generated instantly. Robotic fabrication can continue uninterrupted with human-robot collaboration. This study contributes to improving design and fabrication possibilities such as mass-customization, and shortens the process from design to production. The user experience and augmented spatial feedback provided by mixed reality are richer than the interaction with the computer screen. Since the whole process from parametric design to robotic fabrication can be controlled by parameters with hand gestures, the perception of reality is richer. The digital twin of parametric design and robotic fabrication is superimposed as holographic content by adding it on top of real-world images. Designers can interact with both design and fabrication processes both physically and virtually and can collaborate with other designers.","LowLevel":"digital twins;gesture recognition;grammars;human-robot interaction;production engineering computing","MidLevel":"smart cities;human factors;engineering;robotics;aviation and aerospace;input;manufacturing;standards;other","HighLevel":"end users and user experience;use cases;industries;technology;standards;other","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[418,295,224,205,122,365,331,412,444,58,114,247,371,19,388,429,227,440,401,174],"Top20Abs":[295,418,224,365,331,58,444,371,412,122,114,429,401,336,388,227,291,205,440,344],"Top20Tags":[205,19,159,255,174,122,70,411,388,370,419,352,12,412,440,362,45,179,84,429],"Citation":"Buyruk, Y., & \u00c7a\u011fda\u015f, G. (2022). Interactive Parametric Design and Robotic Fabrication within Mixed Reality Environment. Applied Sciences, 12(24), 12797. https:\/\/doi.org\/10.3390\/app122412797\n"},{"Title":"Integrating Stand-Alone New Media Technologies Such as Games and Virtual and Augmented Reality Software into Learning Management Systems: Integrating Stand-Alone Media Software into LMSs","DOI":"10.1145\/3578837.3578839","Publication year":2022,"Abstract":"Both learning management systems (LMSs) and new media technologies (NMTs, e.g., Augmented\/Virtual Reality software or Serious Games) can enhance teaching and learning, however, the integration of both technologies within an E-Learning system is challenging. NMTs are often developed as bulky stand-alone applications, for example, using game engines, which make them difficult to integrate into the LMS courses. Thus, connecting middleware technologies are needed, however, various technologies exist. In this paper, we introduce three techniques (SCORM-Data, xAPI-Injection, and JSON-REST) that integrate stand-alone NMT learning software into LMSs based on existing technologies and standards from the E-Learning domain. We discuss the techniques and their underlying processes based on a prototype implementation and the results of an expert interview and point out the advantages and disadvantages of each technique to give practitioners differentiated advice on which technique to use. Although SCORM-Data has fewer technical requirements, we conclude that our JSON-REST technique was favored through applicability, simplicity, ease of integration, and efficiency.","LowLevel":"educational courses;learning management systems;middleware;serious games;teaching","MidLevel":"education;medical;developers;simulation","HighLevel":"industries;technology;use cases","Venue":"ICEEL '22: Proceedings of the 2022 6th International Conference on Education and E-Learning","Top20AbsAndTags":[163,41,414,87,114,236,185,39,400,439,244,207,77,57,51,262,136,296,363,320],"Top20Abs":[163,77,87,114,39,41,207,414,244,296,116,9,262,442,185,400,115,381,259,51],"Top20Tags":[145,113,236,400,136,363,41,33,88,164,87,390,232,372,103,56,3,414,82,272],"Citation":"Horst, R., Naraghi-Taghi-Off, R., & D\u00f6rner, R. (2022). Integrating Stand-Alone New Media Technologies Such as Games and Virtual and Augmented Reality Software into Learning Management Systems. Proceedings of the 2022 6th International Conference on Education and E-Learning. https:\/\/doi.org\/10.1145\/3578837.3578839\n"},{"Title":"System and Package-Level EMI Shielding Effectiveness Analysis for AR\/VR Devices","DOI":"10.1109\/EMCSI39492.2022.10050244","Publication year":2022,"Abstract":"Conformal metal sputtering on package mold compound provides an alternative to meet package and system electromagnetic interference (EMI) and RF interference (RFI) requirements without sacrificing product form factors. However available coating thickness and material options on the outsourced semiconductor assembly and test (OSTA) market is limited and inadequate. An EM-circuit co-simulation method was developed for conformal shielding effectiveness (SE) analysis for augmented and virtual reality (AR\/VR) applications. SE results of various coating thickness and materials are presented to show the custom needs for thicker Cu coating and high permeability coating materials.","LowLevel":"circuit analysis computing;copper;electromagnetic interference;electromagnetic shielding;outsourcing;semiconductor devices;sputtered coatings","MidLevel":"education;other;human resources;engineering","HighLevel":"industries;technology;business;other","Venue":"2022 IEEE International Symposium on Electromagnetic Compatibility &amp; Signal\/Power Integrity (EMCSI)","Top20AbsAndTags":[188,148,370,342,51,414,200,227,423,114,383,49,39,374,262,82,137,125,163,287],"Top20Abs":[370,188,148,342,423,414,39,114,383,262,227,200,49,82,374,216,51,137,152,163],"Top20Tags":[272,140,360,185,136,122,289,163,52,58,232,227,84,380,117,64,210,205,94,78],"Citation":"Zhang, H., Sarmast, S., Basu, S., & Codd, P. (2022). System and Package-Level EMI Shielding Effectiveness Analysis for AR\/VR Devices. 2022 IEEE International Symposium on Electromagnetic Compatibility &amp; Signal\/Power Integrity (EMCSI). https:\/\/doi.org\/10.1109\/emcsi39492.2022.10050244\n"},{"Title":"Ethical Design Approaches for Workplace Augmented Reality","DOI":"10.1145\/3531210.3531212","Publication year":2022,"Abstract":"Augmented reality (AR) technologies are increasingly being implemented in various workplace contexts; however, they pose a number of ethical design challenges. To discern the ethical implications of workplace AR, this article conducts an analysis of the promotional discourses surrounding a workplace AR system. This analysis demonstrates a tendency to frame AR technologies in terms of a transhumanist evolution in worker agency and organizational efficiency. Such discourses elide applications of workplace AR for purposes of worker surveillance and exploitation. The article concludes by outlining speculative ethical design guidelines that communication designers can take up in their work on workplace AR systems.","LowLevel":"ethical aspects;organisational aspects;personnel","MidLevel":"business planning and management;policy;human resources","HighLevel":"business","Venue":"Commun. Des. Q. (USA)","Top20AbsAndTags":[94,49,125,102,117,96,139,27,60,135,62,148,219,418,58,114,360,81,136,2],"Top20Abs":[49,94,125,102,62,27,110,139,60,81,429,58,135,45,418,2,96,148,136,159],"Top20Tags":[305,40,125,44,359,140,360,380,378,117,94,310,273,67,472,289,406,410,335,354],"Citation":"Greene, J. (2022). Ethical Design Approaches for Workplace Augmented Reality. Communication Design Quarterly, 10(4), 16\u201326. https:\/\/doi.org\/10.1145\/3531210.3531212\n"},{"Title":"Evaluating the design of an art student framework supporting xr exhibitions: evaluating an art student framework","DOI":"10.1145\/3575879.3576003","Publication year":2022,"Abstract":"A successful system design may lead to a successfully implemented system. This paper demonstrates the followed evaluation procedure for the creation of a system dedicated to art schools. Specifically, we focus on meeting the needs of art teachers, art students, and visitors who are the stakeholders of such a system. Art teachers need to be able to initiate art exhibitions for their students and assess them, art students to create virtual exhibitions, and art exhibition visitors to have immersive and interactive experiences. This evaluation is a part of an EU-funded research project, namely CREAMS which aims to design, develop end evaluate a framework and open-source tools based on virtual, augmented, and mixed reality for the creation of virtual exhibitions for art students. Here, we present the followed user-centered design methodology that we followed to reach the first system design. The methodology begins with the literature review and continues with the filtering of review findings by experts, the creation of the corresponding mockups, the evaluation of the mockups by the end users, the specification of quantitative and qualitative metrics on employing XR technologies in Higher Education Institutions and concludes with the identification of a set of specifications that drives the development of a specific conceptual system design.","LowLevel":"art;computer aided instruction;educational institutions;further education;user centered design","MidLevel":"education;training;liberal arts;developers;human-computer interaction","HighLevel":"industries;technology;use cases;end users and user experience","Venue":"PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics","Top20AbsAndTags":[229,471,145,99,82,114,205,204,306,392,136,194,39,46,246,443,134,414,418,275],"Top20Abs":[229,471,145,99,82,205,306,194,204,46,136,39,443,114,88,454,418,188,246,31],"Top20Tags":[33,204,171,145,391,125,383,196,302,56,3,88,229,241,154,236,124,9,414,320],"Citation":"Koukopoulos, D., Dafiotis, P., Sylaiou, S., Koukoulis, K., & Fidas, C. (2022). Evaluating the Design of an Art Student Framework Supporting XR Exhibitions. Proceedings of the 26th Pan-Hellenic Conference on Informatics. https:\/\/doi.org\/10.1145\/3575879.3576003\n"},{"Title":"Breaking Edge Shackles: Infrastructure-Free Collaborative Mobile Augmented Reality","DOI":"10.1145\/3560905.3568546","Publication year":2022,"Abstract":"Collaborative AR applications are gaining popularity, but have heavy computing requirements for identifying and tracking AR devices and objects in the ecosystem. Prior AR frameworks typically rely on edge infrastructure to offload AR's compute-heavy tasks. However, such infrastructure may not always be available, and continuously running AR computations on user devices can rapidly drain battery and impact application longevity. In this work, we enable infrastructure-free mobile AR with a low energy footprint, by using&lt;i&gt;collaborative time slicing&lt;\/i&gt;to distribute compute-heavy AR tasks across user devices. Realizing this idea is challenging because distributed execution can result in inconsistent synchronization of the AR virtual overlays. Our framework, FreeAR, tackles this with novel lightweight techniques for tightly synchronized virtual overlay placements across user views, and low latency recovery upon disruptions. We prototype FreeAR on Android and show that it can improve the virtual overlay positioning accuracy (with respect to the IOU metric) by up to 78%, relative to state-of-the-art collaborative AR systems, while also reducing power by up to 60% relative to a direct application of those prior solutions.","LowLevel":"android;mobile computing;object tracking","MidLevel":"telecommunication;computer vision;developers","HighLevel":"industries;technology","Venue":"SenSys '22: Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems","Top20AbsAndTags":[40,33,77,189,216,111,13,194,107,267,446,229,37,390,139,385,110,87,50,123],"Top20Abs":[40,33,189,194,111,13,229,37,390,107,87,49,139,110,50,259,45,123,267,146],"Top20Tags":[216,77,56,8,254,280,116,93,425,173,410,115,435,139,82,220,42,376,164,13],"Citation":"Apicharttrisorn, K., Chen, J., Sekar, V., Rowe, A., & Krishnamurthy, S. V. (2022). Breaking Edge Shackles. Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems. https:\/\/doi.org\/10.1145\/3560905.3568546\n"},{"Title":"Graphic Design and Evaluation of an Augmented Reality for Advergame","DOI":"10.1145\/3574131.3574462","Publication year":2022,"Abstract":"This letter attempts to introduce an Augmented Reality Advergame (ARA), which is designed to eliminate the pain-points present of current advergames. i.e., cumbersome interface design, excessive entertainment elements lead to user distraction, and low participation among middle-aged and elderly users. For this purpose, our ARA improves color scheme, optimizes graphic proportions, and reduces typographical complexity of interface design. Furthermore, we introduce an augmented reality way in interaction. Eye-tracking studies demonstrate that ARA is better at directing user attention to key information than the current advergames. Perceptive studies confirm that ARA user engagement is 25% higher than the control advergame.","LowLevel":"computer games;computer graphics;gaze tracking;user interfaces","MidLevel":"computer vision;graphics;liberal arts;input;human-computer interaction","HighLevel":"end users and user experience;technology;industries","Venue":"VRCAI'22: Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry","Top20AbsAndTags":[277,32,419,259,79,53,288,214,108,409,147,95,382,243,208,83,15,248,149,189],"Top20Abs":[419,288,214,149,79,53,147,108,248,409,465,15,95,32,208,189,224,385,259,382],"Top20Tags":[422,198,259,32,79,277,204,351,415,71,414,243,59,421,410,331,260,148,250,98],"Citation":"Hu, B., Wang, W., Chan, K., Chen, Z., Tang, C., & Li, P. (2022). Graphic Design and Evaluation of an Augmented Reality for Advergame. Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry. https:\/\/doi.org\/10.1145\/3574131.3574462\n"},{"Title":"An Augmented Reality Design Tool to Guide Furniture Arrangements at Home","DOI":"10.1145\/3572921.3576216","Publication year":2022,"Abstract":"Home is a place to live and an environment to support and enhance our psychological well-being. Factors such as furniture selection and arrangements can contribute to more harmonized and balanced home environments. Augmented reality (AR) tools have recently gained attention in home interior design due to their competencies and potential in improving and envisioning the experience of living at home. Following Feng Shui principles and iterative prototyping processes, we designed a high-fidelity AR prototype to guide users in making informed decisions about furniture arrangements at home. We then recruited nine participants to evaluate the tool's usability and usefulness. We discuss our preliminary findings on the benefit of the Feng Shui-supported AR design tool for users. We hope this research inspires future development of AR tools with embedded design recommendations for guiding users in advancing their home environments.","LowLevel":"furniture;psychology;software performance evaluation;software tools","MidLevel":"medical;inspection, safety and quality;construction;business performance metrics;developers;manufacturing","HighLevel":"industries;technology;business;use cases","Venue":"OzCHI '22: Proceedings of the 34th Australian Conference on Human-Computer Interaction","Top20AbsAndTags":[50,135,38,179,114,336,12,86,53,136,240,418,7,385,219,93,92,27,443,410],"Top20Abs":[50,38,135,336,179,114,136,53,418,86,385,27,224,92,410,118,93,419,70,219],"Top20Tags":[84,328,12,345,443,380,374,288,322,64,370,90,47,192,323,383,206,439,318,126],"Citation":"Qu, C., & Aflatoony, L. (2022). An Augmented Reality Design Tool to Guide Furniture Arrangements at Home. Proceedings of the 34th Australian Conference on Human-Computer Interaction. https:\/\/doi.org\/10.1145\/3572921.3576216\n"},{"Title":"Aerial Manipulation using a Human-Embodied Drone Interface","DOI":"10.1109\/ARSO54254.2022.9802972","Publication year":2022,"Abstract":"This paper presents a human-embodied drone interface for aerial manipulation. With the immersive frame-work constructed by Unity and Virtual and Augment Reality (VR\/AR), the system allows the operator to leverage their intelligence to evaluate the tasks and collaboratively perform the desired tasks beyond the operator's line-of-sight with a mobile manipulating drone. Flight trials demonstrate the results from three horizontal tasks including drilling, peg-in-hole, and key manipulation to illustrate the efficacy of this system. This paper concludes that the successful human-embodied drone interface can contribute to society by the following: a quality dataset for autonomous dexterous aerial manipulation; hardware-free training system; opportunities for various people who live outside of normal life boundaries.","LowLevel":"control engineering computing;dexterous manipulators;manipulators;mobile robots;path planning","MidLevel":"engineering;robotics;business planning and management;industrial equipment;manufacturing","HighLevel":"industries;technology;business","Venue":"2022 IEEE International Conference on Advanced Robotics and Its Social Impacts (ARSO)","Top20AbsAndTags":[398,174,84,205,19,45,120,179,269,210,165,303,424,60,338,371,101,374,419,345],"Top20Abs":[398,174,165,84,45,424,303,345,205,202,12,179,419,338,374,101,120,19,210,80],"Top20Tags":[401,420,398,322,60,174,371,19,120,157,347,269,396,205,379,407,86,465,122,190],"Citation":"Kim, D., & Oh, P. Y. (2022). Aerial Manipulation using a Human-Embodied Drone Interface. 2022 IEEE International Conference on Advanced Robotics and Its Social Impacts (ARSO). https:\/\/doi.org\/10.1109\/arso54254.2022.9802972\n"},{"Title":"HARIN: HoloLens Augmented Reality Indoor Navigation","DOI":"10.1145\/3543895.3543938","Publication year":2022,"Abstract":"This paper describes a proposed augmented reality software called HARIN for indoor navigation. Navigating buildings poses a significant problem to a variety of populations, including disabled individuals and maintenance workers. The general population also faces difficulties navigating larger, or unfamiliar, buildings; the potential benefits of a solution are significant in scope. Initial solution attempts have been produced successfully in other research, however there is much room for improvement, particularly in user interface design and localization accuracy. To pursue these improvements, this paper proposes an AR indoor navigation using the HoloLens as a platform for new types of navigation markers to improve user experiences and decrease location estimate error. A proof-of-concept software prototype for a part of the overall HARIN system is shown, and future work is discussed.","LowLevel":"handicapped aids;indoor navigation;user experience;user interfaces","MidLevel":"medical;human-computer interaction;human factors;navigation","HighLevel":"end users and user experience;industries;use cases","Venue":"ACIT '22: Proceedings of the 9th International Conference on Applied Computing &amp; Information Technology","Top20AbsAndTags":[139,70,49,326,101,432,38,79,12,267,385,186,360,51,351,248,389,41,443,261],"Top20Abs":[139,70,49,326,267,389,101,38,12,186,51,360,385,79,86,261,10,41,432,248],"Top20Tags":[93,365,382,394,351,214,142,70,415,38,37,248,205,122,29,79,36,242,347,180],"Citation":"Lynam, H., Folmer, E., & Dascalu, S. (2022). HARIN: HoloLens Augmented Reality Indoor Navigation. Proceedings of the 9th International Conference on Applied Computing &amp; Information Technology. https:\/\/doi.org\/10.1145\/3543895.3543938\n"},{"Title":"Immersive AR Merged with MI-BCI Hand Function Rehabilitation Training System for Stroke Patients","DOI":"10.1145\/3581807.3581851","Publication year":2022,"Abstract":"Strokes can cause neurological damage to the patient, which leads to hand dysfunction. Traditional methods of hand function rehabilitation, such as electrical stimulation and therapist-dependent movement therapy, are ineffective due to the brain's lack of direct involvement in the motor nervous system. To improve the rehabilitation efficacy, we design a rehabilitation system based on motor imagery brain-computer interface (MI-BCI) and augmented reality (AR) for hand function rehabilitation of stroke patients. It includes two-class motor imagery tasks: left-hand fist and right-hand fist based on AR. Motor imagery electroencephalogram (MI-EEG) is acquired from 10 subjects and decoded by using an algorithm module encapsulated in the master system. It reaches an average accuracy of 76.4% and is eventually fed back to patients through rehabilitation peripherals. In addition, the master system provides an interactive interface with features to design treatment tasks, manage patient information and monitor patient status. The system realizes an immersive rehabilitation experience that promotes the reconstruction of the central nervous system and provides a new approach for stroke patients to recover.","LowLevel":"brain;brain-computer interfaces;electroencephalography;feature extraction;medical computing;medical signal processing;neurophysiology;patient monitoring;patient rehabilitation;patient treatment;signal classification","MidLevel":"human factors;computer vision;data;medical;inspection, safety and quality;chemical;sensors;farming and natural science","HighLevel":"industries;technology;use cases;end users and user experience","Venue":"ICCPR '22: Proceedings of the 2022 11th International Conference on Computing and Pattern Recognition","Top20AbsAndTags":[417,465,455,330,91,175,165,294,108,250,415,366,210,464,118,43,153,326,375,287],"Top20Abs":[417,465,330,91,455,175,165,43,415,366,153,108,294,0,118,287,375,401,210,367],"Top20Tags":[455,165,108,417,250,91,330,230,253,326,152,465,294,356,133,210,386,168,71,358],"Citation":"Qin, Y., Yang, B., & Li, D. (2022). Immersive AR Merged with MI-BCI Hand Function Rehabilitation Training System for Stroke Patients. Proceedings of the 2022 11th International Conference on Computing and Pattern Recognition. https:\/\/doi.org\/10.1145\/3581807.3581851\n"},{"Title":"Posture Guided Human Action Recognition for Fitness Applications","DOI":"10.1145\/3571600.3571612","Publication year":2022,"Abstract":"Human action recognition has attracted a lot of attention in the recent past due to newer applications in computer vision such as fitness tracking, augmented reality and virtual reality. Most of the existing deep learning based methods first deploy a deep neural network to estimate the human pose from a sequence of images followed by a second network to classify the human actions using all the estimated human poses. However, the pose estimation used in these methods typically fail to generalize for non-upright actions such as push-ups, plank, etc since the keypoints are closer to each other than observed in upright postures such as jump, dead-lift, etc. Hence, the accuracy of these methods gets impacted for non-upright actions, typically seen in fitness applications. In this paper, we propose a novel multi-stage deep learning based method for action recognition to predict upright as well as non-upright actions with high accuracy. We use a Light Weight Boundary Refinement Module (LWBRM) during pose estimation to distinguish closer keypoints more effectively. Further, we also introduce an intermediate frame-by-frame posture classification stage after pose estimation. We observed that this intermediate stage enables us to improve the human action recognition accuracy by while improving computational efficiency by ~ 2 &#215; compared to state-of-the-art methods. Our method can process at 104 frames per second on an android smartphone, and hence can readily be deployed for consumer oriented fitness applications.","LowLevel":"computer vision;deep learning (artificial intelligence);feature extraction;image classification;image motion analysis;pose estimation;smartphones","MidLevel":"artificial intelligence;telecommunication;computer vision;medical;graphics;liberal arts;chemical;input;other","HighLevel":"industries;technology;other","Venue":"ICVGIP '22: Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing","Top20AbsAndTags":[81,381,283,255,12,425,194,172,311,160,23,48,22,98,453,163,116,144,220,143],"Top20Abs":[12,255,311,381,81,172,283,425,194,23,160,22,48,453,480,163,483,77,205,352],"Top20Tags":[255,381,81,425,172,415,379,115,160,95,98,220,18,451,339,71,143,76,12,48],"Citation":"S R, V., Akula, J., Prasad, B. H. P., & Rosh, G. (2022). Posture Guided Human Action Recognition for Fitness Applications. Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing. https:\/\/doi.org\/10.1145\/3571600.3571612\n"},{"Title":"HoloSet - A Dataset for Visual-Inertial Pose Estimation in Extended Reality: Dataset","DOI":"10.1145\/3560905.3567763","Publication year":2022,"Abstract":"There is a lack of datasets for visual-inertial odometry applications in Extended Reality (XR). To the best of our knowledge, there is no dataset available that is captured from an XR headset with a human as a carrier. To bridge this gap, we present a novel pose estimation dataset --- called HoloSet --- collected using Microsoft Hololens 2, which is a state-of-the-art head mounted device for XR. Potential applications for HoloSet include visual-inertial odometry, simultaneous localization and mapping (SLAM), and additional applications in XR that leverage visual-inertial data.HoloSet captures both macro and micro movements. For macro movements, the dataset consists of more than 66,000 samples of visual, inertial, and depth camera data in a variety of environments (indoor, outdoor) and scene setups (trails, suburbs, downtown) under multiple user action scenarios (walk, jog). For micro movements, the dataset consists of more than 12,000 samples of additional articulated hand depth camera images while a user plays games that exercise fine motor skills and hand-eye coordination. We present basic visualizations and high-level statistics of the data and outline the potential research use cases for HoloSet.","LowLevel":"cameras;distance measurement;mobile robots;pose estimation;robot vision;slam robotics","MidLevel":"computer vision;robotics;inspection, safety and quality;graphics;navigation;input","HighLevel":"technology;use cases","Venue":"SenSys '22: Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems","Top20AbsAndTags":[425,327,308,379,306,314,257,48,67,4,30,316,375,182,255,381,12,385,194,7],"Top20Abs":[425,327,306,257,314,308,379,67,30,4,375,7,194,255,48,385,182,316,305,12],"Top20Tags":[347,381,425,322,220,48,98,379,387,398,345,402,81,60,255,399,71,173,12,72],"Citation":"Chandio, Y., Bashir, N., & Anwar, F. M. (2022). HoloSet - A Dataset for Visual-Inertial Pose Estimation in Extended Reality. Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems. https:\/\/doi.org\/10.1145\/3560905.3567763\n"},{"Title":"Exploring Text Selection in Augmented Reality Systems","DOI":"10.1145\/3574131.3574459","Publication year":2022,"Abstract":"Text selection is a common and essential activity during text interaction in all interactive systems. As Augmented Reality (AR) head-mounted displays (HMDs) become more widespread, they will need to provide effective interaction techniques for text selection that ensure users can complete a range of text manipulation tasks (e.g., to highlight, copy, and paste text, send instant messages, and browse the web). As a relatively new platform, text selection in AR is largely unexplored and the suitability of interaction techniques supported by current AR HMDs for text selection tasks is unclear. This research aims to fill this gap and reports on an experiment with 12 participants, which compares the performance and usability (user experience and workload) of four possible techniques (Hand+Pinch, Hand+Dwell, Head+Pinch, and Head+Dwell). Our results suggest that Head+Dwell should be the default selection technique, as it is relatively fast, has the lowest error rate and workload, and has the highest-rated user experience and social acceptance.","LowLevel":"helmet mounted displays;human computer interaction;interactive systems;text analysis","MidLevel":"education;wearables;input;human-computer interaction;display technology","HighLevel":"displays;technology;industries;end users and user experience","Venue":"VRCAI'22: Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry","Top20AbsAndTags":[353,250,32,313,279,424,364,80,237,419,180,348,202,153,248,118,165,415,219,283],"Top20Abs":[32,250,353,313,419,279,424,202,153,237,221,183,118,248,219,364,109,80,348,165],"Top20Tags":[279,80,180,21,364,206,58,210,313,224,415,259,424,238,38,217,55,165,248,69],"Citation":"Liu, X., Meng, X., Spittle, B., Xu, W., Gao, B., & Liang, H.-N. (2022). Exploring Text Selection in Augmented Reality Systems. Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry. https:\/\/doi.org\/10.1145\/3574131.3574459\n"},{"Title":"A Novel MAC Scheduling Approach for Mobility based 5G Millimeter Wave Networks","DOI":"10.1109\/ICECTE57896.2022.10114524","Publication year":2022,"Abstract":"Recent days have seen an increase in network traffic caused by bandwidth-hungry apps like augmented reality (AR), virtual reality (VR), and vehicle-to-everything (V2X), which 4G standards and technologies are unable to handle. The millimeter-wave (mmWave) can be seen as a gift for 5G systems meeting extremely high traffic demands by guaranteeing high throughput and low latency. More performance metrics such as SINR, throughput, and RTT inspection are required while increasing the number of UEs in different challenging regions. In UMa situations, MAC scheduling, as well as admission control, are also confusing TCP performance issues. This article uses ns3 to demonstrate a mobility based novel Proportional-Fair (PF) MAC scheduler for 5G networks integrated with mmWave. In this paper, we investigated resource allocation and access to the nearest data center in a practical mobility model based on 5G heterogeneous networks (HetNets). The result spreads higher-yielding in simultaneously growing quantities in the future complex structure of 5G networks.","LowLevel":"4g mobile communication;5g mobile communication;millimetre wave communication;mobility management;resource allocation;telecommunication congestion control;telecommunication scheduling;telecommunication traffic;transport protocols","MidLevel":"cultural heritage;telecommunication;business planning and management;developers;input;geospatial;standards;networks","HighLevel":"industries;technology;standards;business","Venue":"2022 4th International Conference on Electrical, Computer &amp; Telecommunication Engineering (ICECTE)","Top20AbsAndTags":[257,268,256,266,447,209,427,423,231,426,458,466,428,335,271,258,416,307,448,195],"Top20Abs":[256,268,266,257,447,427,426,458,231,466,423,335,209,428,411,416,307,448,385,325],"Top20Tags":[257,423,209,427,264,428,346,271,268,266,426,335,256,222,326,33,231,416,168,105],"Citation":"Hassan, T., & Mowla, M. (2022). A Novel MAC Scheduling Approach for Mobility based 5G Millimeter Wave Networks. 2022 4th International Conference on Electrical, Computer &amp; Telecommunication Engineering (ICECTE). https:\/\/doi.org\/10.1109\/icecte57896.2022.10114524\n"},{"Title":"Analyzing the impact and application of Augmented Reality in Education: The case of students with special educational needs","DOI":"10.1145\/3575879.3575999","Publication year":2022,"Abstract":"This study presents a systematic review of the literature which analyzes the impact and application of Augmented Reality (AR) technology in the tutoring of students with special educational needs. In recent years, Information and Communication Technology (ICT) promotes learning in a pluralistic and multisensory environment that favors students with special educational needs. Augmented Reality is an emerging technology which has been widely used in the field of education, but only few studies are related to special education. This paper investigates the research in AR educational systems addressed to students with special educational needs. In total, 26 studies between 2014 and 2022 were selected and analyzed. Specifically, this systematic review examines the advantages and limitations of AR use in Special Education, the AR platforms and tools used in learning scenarios and the different types of students with special educational needs.","LowLevel":"computer aided instruction","MidLevel":"training","HighLevel":"use cases","Venue":"PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics","Top20AbsAndTags":[56,34,57,163,87,62,88,306,188,184,103,33,171,128,82,320,222,124,369,414],"Top20Abs":[56,34,88,62,306,163,57,87,171,188,103,184,33,128,49,82,369,124,136,204],"Top20Tags":[222,77,114,82,9,204,87,88,57,33,93,405,124,3,39,34,145,443,17,23],"Citation":"Kapetanaki, A., Krouska, A., Troussas, C., & Sgouropoulou, C. (2022). Analyzing the impact and application of Augmented Reality in Education: The case of students with special educational needs. Proceedings of the 26th Pan-Hellenic Conference on Informatics. https:\/\/doi.org\/10.1145\/3575879.3575999\n"},{"Title":"CAFI-AR: Contact-aware Freehand Interaction with AR Objects","DOI":"10.1145\/3569499","Publication year":2022,"Abstract":"Freehand interaction enhances user experience, allowing one to use bare hands to manipulate virtual objects in AR. Yet, it remains challenging to accurately and efficiently detect contacts between real hand and virtual object, due to the imprecise captured\/estimated hand geometry. This paper presents CAFI-AR, a new approach for Contact-Aware Freehand Interaction with virtual AR objects, enabling us to automatically detect hand-object contacts in real-time with low latency. Specifically, we formulate a compact deep architecture to efficiently learn to predict hand action and contact moment from sequences of captured RGB images relative to the 3D virtual object. To train the architecture for detecting contacts on AR objects, we build a new dataset with 4,008 frame sequences, each with annotated hand-object interaction information. Further, we integrate CAFI-AR into our prototyping AR system and develop various interactive scenarios, demonstrating fine-grained contact-aware interactions on a rich variety of virtual AR objects, which cannot be achieved by existing AR interaction approaches. Lastly, we also evaluate CAFI-AR, quantitatively and qualitatively, through two user studies to demonstrate its effectiveness in terms of accurately detecting the hand-object contacts and promoting fluid freehand interactions","LowLevel":"deep learning (artificial intelligence);gesture recognition;human computer interaction;image capture;image colour analysis;user experience","MidLevel":"artificial intelligence;human factors;computer vision;medical;graphics;liberal arts;input;human-computer interaction;other","HighLevel":"end users and user experience;technology;industries;other","Venue":"Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. (USA)","Top20AbsAndTags":[142,409,224,53,79,366,400,487,255,87,118,189,139,167,143,194,331,45,123,81],"Top20Abs":[142,224,409,53,487,79,87,400,366,194,167,189,255,139,123,151,45,49,118,104],"Top20Tags":[79,95,255,18,379,425,387,142,365,160,382,339,58,381,81,259,115,308,72,251],"Citation":"Tang, X., Li, R., & Fu, C.-W. (2022). CAFI-AR. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(4), 1\u201323. https:\/\/doi.org\/10.1145\/3569499\n"},{"Title":"Implementation of the XR rehabilitation simulation system for the utilization of rehabilitation with robotic prosthetic leg","DOI":"10.3390\/app122412659","Publication year":2022,"Abstract":"With the recent development of a digital rehabilitation system, research on the rehabilitation of amputees is accelerating. However, research on rehabilitation systems for patients with amputation of the lower extremities is insufficient. For the rehabilitation of amputees, it is important to maintain muscle mass through the improvement of muscle movement memory, continuous rehabilitation learning, and motivation to improve efficiency. The rehabilitation system in a virtual environment is convenient in that there is no restriction on time and space because rehabilitation training of amputees is possible without removing\/attaching general prosthetic legs and robot prosthetic legs. In this paper, we propose an XR rehabilitation system for patients with lower extremity amputation to improve the motivational aspect of rehabilitation training. The proposed method is a system that allows patients and clinical experts to perform rehabilitation in the same environment using two XR equipment called HoloLens 2. The content was provided in the form of a game in which the number of movements of amputees was allocated as scores to enhance rehabilitation convenience and motivation aspects. The virtual 3D model prosthetic leg used in-game content worked through the acquisition and processing of the patient's actual muscle EMG (ElectroMyoGraphy) signal. In order to improve reactivity, there was a time limit for completing the operation. The classified action should be completed by the amputee within the time limit, although the number of times set as the target. To complete the operation, the amputee must force the amputation area to exceed an arbitrarily set threshold. The evaluation results were evaluated through an independent sample&lt;i&gt;t&lt;\/i&gt;-test. we contribute to the development of digital rehabilitation simulation systems. XR rehabilitation training techniques, operated with EMG signals obtained from actual amputation sites, contribute to the promotion of rehabilitation content in patients with amputation of the lower extremities. It is expected that this paper will improve the convenience and rehabilitation of rehabilitation training in the future.","LowLevel":"biomechanics;computer games;electromyography;human factors;medical robotics;medical signal processing;muscle;patient rehabilitation;prosthetics","MidLevel":"human factors;medical;data;robotics;liberal arts;sensors","HighLevel":"end users and user experience;technology;industries","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[384,465,91,330,294,390,306,33,54,40,356,257,408,0,436,365,320,305,314,414],"Top20Abs":[384,465,91,330,294,390,306,33,54,408,257,0,40,436,314,305,365,37,374,389],"Top20Tags":[91,294,384,165,455,108,253,133,250,326,397,465,425,186,356,410,204,230,147,254],"Citation":"Shim, W., Kim, H., Lim, G., Lee, S., Kim, H., Hwang, J., Lee, E., Cho, J., Jeong, H., Pak, C., Suh, H., Hong, J., & Kwon, S. (2022). Implementation of the XR Rehabilitation Simulation System for the Utilization of Rehabilitation with Robotic Prosthetic Leg. Applied Sciences, 12(24), 12659. https:\/\/doi.org\/10.3390\/app122412659\n"},{"Title":"Improving Big Data Analytics With Interactive Augmented Reality","DOI":"10.4018\/IJISMD.315124","Publication year":2022,"Abstract":"Since, data is generated every minute by everyone including consumers and\/or business worldwide, there is an enormous worth for big data analytics. Big data analytics is a technique for extracting important information from large amounts of a data. Visualization is the best medium to analyze and share information. Visual images help to transmit bid data to the human brain within a few seconds. Visual interpretations help in visualizing data from different angles. Visualization helps to outline problems and understand current trends. Augmented reality enables the user to experience the real world, which is digitally augmented in a way. The main objective of this research work is to find the solution to visualize the analyzed data and show it to the users in a 3D view and to improve the angle of visualization with the help of augmented reality techniques.","LowLevel":"big data;data analysis;data visualization","MidLevel":"data","HighLevel":"technology","Venue":"Int. J. Inf. Syst. Model. Des. (USA)","Top20AbsAndTags":[178,166,289,157,486,234,316,424,182,432,80,42,376,181,28,396,148,11,89,226],"Top20Abs":[178,166,486,289,424,157,479,182,181,396,7,28,316,80,370,226,84,263,376,234],"Top20Tags":[378,289,166,284,148,178,42,376,234,309,80,410,134,121,429,11,352,101,22,136],"Citation":"Hirve, S. A., & Pradeep Reddy C. H. (2022). Improving Big Data Analytics With Interactive Augmented Reality. International Journal of Information System Modeling and Design, 13(7), 1\u201311. https:\/\/doi.org\/10.4018\/ijismd.315124\n"},{"Title":"Online Learning for Network Resource Allocation","DOI":"10.1145\/3579342.3579348","Publication year":2022,"Abstract":"Motivation. Connectivity and ubiquity of computing devices enabled a wide spectrum of network applications such as content delivery, interpersonal communication, and intervehicular communication. New use cases (e.g., autonomous driving, augmented reality, and tactile internet) require satisfying user-generated and machine-generated demand with stringent low-latency and high-bandwidth guarantees.","LowLevel":"5g mobile communication;internet;resource allocation","MidLevel":"telecommunication;business planning and management;input;geospatial;networks","HighLevel":"industries;technology;business","Venue":"ACM SIGMETRICS Perform. Eval. Rev. (USA)","Top20AbsAndTags":[268,423,404,447,266,219,448,271,159,110,111,335,330,163,427,251,116,413,264,346],"Top20Abs":[447,268,423,219,266,253,126,111,116,368,404,110,448,159,426,286,410,45,413,163],"Top20Tags":[423,268,257,264,404,427,271,346,330,266,327,335,251,258,256,376,385,8,428,416],"Citation":"Salem, T. S. (2022). Online Learning for Network Resource Allocation. ACM SIGMETRICS Performance Evaluation Review, 50(3), 20\u201323. https:\/\/doi.org\/10.1145\/3579342.3579348\n"},{"Title":"Identifying The Types and Platforms of the Metaverse - a Systematic Literature Review","DOI":"10.1109\/ICCIT55355.2022.10118659","Publication year":2022,"Abstract":"Metaverse is gaining a lot of traction lately, especially after Mark Zuckerberg's Facebook changed its name to Meta as a preparation to enter another universe called metaverse. The eXtended Reality (XR) technology is also rapidly growing since the birth of Virtual Reality (VR), followed by Augmented Reality (AR), and Mixed Reality is the latest development of XR technology. As the XR technology is the perfect companion to support the metaverse and the understanding of the metaverse is already clear but unfortunately, there is a lot of misconception about the types of the metaverse as well as the platform for each type. This paper aims to point out the types and platforms of the metaverse. 49 articles have been collected and reviewed carefully as a quality assessment process, and 45 articles have been selected for further review. This review informs everybody that there are many types and platforms of the metaverse nowadays.","LowLevel":"social networking","MidLevel":"collaboration","HighLevel":"use cases","Venue":"2022 IEEE Creative Communication and Innovative Technology (ICCIT)","Top20AbsAndTags":[377,249,355,363,226,328,442,204,251,354,244,215,448,327,306,314,308,218,257,44],"Top20Abs":[377,355,249,354,363,442,226,204,328,251,244,215,448,306,327,314,257,218,308,49],"Top20Tags":[363,219,249,67,164,368,44,377,402,328,17,244,213,226,203,333,28,214,209,34],"Citation":"Suryodiningrat, S. P., Prabowo, H., Ramadhan, A., & Santoso, H. B. (2022). Identifying The Types and Platforms of the Metaverse \u2013 a Systematic Literature Review. 2022 IEEE Creative Communication and Innovative Technology (ICCIT). https:\/\/doi.org\/10.1109\/iccit55355.2022.10118659\n"},{"Title":"Admission Experts based on Android+VR, AR Technology and Voice Operation","DOI":"10.1145\/3568739.3568805","Publication year":2022,"Abstract":"With the improvement of people's living standards and the increasing abundance of materials, acceptance has become an indispensable part of people's life. Nowadays, the application of smartphone is more and more widely used. A mobile APP based on Android system with the theme of acceptance is developed. Augmented Reality (AR) and Virtual Reality (VR) technologies as well as voice operation functions are applied to bring users a technological, convenient and novel experience of acceptance.","LowLevel":"android;mobile computing;smartphones;speech-based user interfaces","MidLevel":"telecommunication;liberal arts;developers;input;human-computer interaction","HighLevel":"end users and user experience;technology;industries","Venue":"ICDTE '22: Proceedings of the 6th International Conference on Digital Technology in Education","Top20AbsAndTags":[77,62,56,57,200,359,27,151,410,93,419,139,210,82,105,157,220,352,196,203],"Top20Abs":[62,57,359,200,139,93,196,77,27,410,85,419,82,220,219,328,485,152,115,355],"Top20Tags":[77,151,56,93,339,254,221,129,27,82,115,210,348,424,419,309,105,103,70,110],"Citation":"Xiao, J., Yang, W., & Li, F. (2022). Admission Experts based on Android+VR, AR Technology and Voice Operation. Proceedings of the 6th International Conference on Digital Technology in Education. https:\/\/doi.org\/10.1145\/3568739.3568805\n"},{"Title":"Fusing Computer Vision and Wireless Signal for Accurate Sensor Localization in AR View","DOI":"10.1145\/3560905.3568095","Publication year":2022,"Abstract":"Recent years have seen increasing traction to enable new applications that can localize sensors on the screen of an Augmented Reality (AR) device (e.g. smartphone, tablet) so that sensors can be controlled more intuitively. Despite recent advances in this area, both wireless signal dependent and computer vision based localization solutions have seen a slow acceptance due to signal noise, multipath effect, and limited AR device-sensor interactivity. In this paper, we propose a novel solution to combine the complementary advantages of wireless signal based localization solution with the computer vision based solution to track IoT devices and sensors more accurately. Experimental result shows that our system can accurately track IoT devices with an average pixel error of 34 pixels in a 1024 &#215; 768 pixels image, which is a 75.8% improvement from the state-of-the-art model.","LowLevel":"computer vision;internet of things;robot vision;smartphones;wireless sensor networks","MidLevel":"internet of things;telecommunication;computer vision;robotics;liberal arts;sensors;networks","HighLevel":"industries;technology","Venue":"SenSys '22: Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems","Top20AbsAndTags":[129,110,379,445,266,123,182,280,235,287,395,38,18,175,216,228,267,425,421,190],"Top20Abs":[129,445,287,235,18,110,182,266,24,228,162,379,421,267,216,395,316,74,190,277],"Top20Tags":[129,123,110,425,38,266,258,175,121,387,271,347,298,396,286,280,378,339,359,115],"Citation":"Billah, M. F. R. M., Islam, M. M., Saoda, N., Iqbal, T., & Campbell, B. (2022). Fusing Computer Vision and Wireless Signal for Accurate Sensor Localization in AR View. Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems. https:\/\/doi.org\/10.1145\/3560905.3568095\n"},{"Title":"FocalPoint: Adaptive Direct Manipulation for Selecting Small 3D Virtual Objects","DOI":"10.1145\/3580856","Publication year":2022,"Abstract":"We propose FocalPoint, a direct manipulation technique in smartphone augmented reality (AR) for selecting small densely-packed objects within reach, a fundamental yet challenging task in AR due to the required accuracy and precision. FocalPoint adaptively and continuously updates a cylindrical geometry for selection disambiguation based on the user's selection history and hand movements. This design is informed by a preliminary study which revealed that participants preferred selecting objects appearing in particular regions of the screen. We evaluate FocalPoint against a baseline direct manipulation technique in a 12-participant study with two tasks: selecting a 3 mm wide target from a pile of cubes and virtually decorating a house with LEGO pieces. FocalPoint was three times as accurate for selecting the correct object and 5.5 seconds faster on average; participants using FocalPoint decorated their houses more and were more satisfied with the result. We further demonstrate the finer control enabled by FocalPoint in example applications of robot repair, 3D modeling, and neural network visualizations.","LowLevel":"smartphones;user interfaces","MidLevel":"telecommunication;liberal arts;human-computer interaction","HighLevel":"end users and user experience;industries","Venue":"Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. (USA)","Top20AbsAndTags":[198,7,248,32,409,75,415,424,107,419,400,189,202,19,195,142,167,453,399,16],"Top20Abs":[198,32,7,75,409,415,453,107,250,19,202,189,248,167,400,399,195,152,424,143],"Top20Tags":[339,129,309,93,27,216,348,36,82,419,248,424,366,72,182,220,105,142,175,279],"Citation":"Ma, J., Qian, J., Zhou, T., & Huang, J. (2022). FocalPoint. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 7(1), 1\u201326. https:\/\/doi.org\/10.1145\/3580856\n"},{"Title":"Employing AR\/MR Mockups to Imagine Future Custom Manufacturing Practices","DOI":"10.1145\/3572921.3576201","Publication year":2022,"Abstract":"Versatile augmented reality (AR)\/mixed reality (MR) technologies align with custom manufacturers' resource constraints and support their requirement for agility in responding to unique Industry 4.0 challenges. However, for Australian custom manufacturers, AR\/MR uptake remains low. This modest-sized case study seeks to support resource-constrained custom manufacturers by exploring current AR\/MR adoption challenges and potentials. Underpinned by a Research-through-Design (RtD) methodology and building upon Situated and Participative Enactment of Scenarios (SPES) methods, we reflect on using novel Microsoft HoloLens 2 AR\/MR mockups to support in-situ enactments with domain experts to collaboratively imagine and design more productive and efficient augmented fabrication and assembly practices. In exploring new ways of making and doing through AR\/MR, we find promising pathways for Australian custom manufacturers to add value across a product's lifecycle. Our findings identify five key areas for further research, which will be explored and developed through workshops around each identified AR\/MR application area.","LowLevel":"manufacturing systems;virtual manufacturing","MidLevel":"education;manufacturing","HighLevel":"industries","Venue":"OzCHI '22: Proceedings of the 34th Australian Conference on Human-Computer Interaction","Top20AbsAndTags":[439,418,397,395,64,403,360,319,424,246,365,159,419,179,65,2,370,49,17,244],"Top20Abs":[439,397,418,395,319,424,360,403,246,64,365,419,159,49,179,65,370,244,17,400],"Top20Tags":[174,388,370,131,159,449,137,77,84,205,122,325,181,273,407,12,360,45,289,418],"Citation":"Franze, A. P., Caldwell, G. A., Teixeira, M. F. L. A., & Rittenbruch, M. (2022). Employing AR\/MR Mockups to Imagine Future Custom Manufacturing Practices. Proceedings of the 34th Australian Conference on Human-Computer Interaction. https:\/\/doi.org\/10.1145\/3572921.3576201\n"},{"Title":"OCOsense Glasses for Facial Expressions Recognition and Contextual Affective Computing in Real World and Augmented Reality","DOI":"10.1145\/3544793.3560325","Publication year":2022,"Abstract":"The paper presents the novel OCOSenseTM smart glasses with integrated sensors, primarily non-contact optomyographic (OMG) OCOTM sensors, 9-axis inertial measurement unit (IMU), and an altimeter. The glasses connect with a smartphone application, which facilitates the continuous and real-time measurements of facial-muscles activation and head movement, thus allowing for the detection of facial expressions and the activities of the user in real-time. We will demonstrate how the system is used in practice, i.e., a participant will wear the OCOSenseTM glasses, which will stream the sensor data to a tablet, where the real-time visualization of the sensor data and the data interpretation will be presented such as facial expressions (smile, frown, surprise) and activities. We believe that the OCOSenseTM glasses are the next big thing in wearables, which will allow for better understanding of the user's context, activities, emotional state, and more, which can be easily coupled within Augmented and Extended Reality environments.","LowLevel":"emotion recognition;face recognition;smartphones","MidLevel":"telecommunication;liberal arts;human factors;input","HighLevel":"end users and user experience;technology;industries","Venue":"UbiComp\/ISWC'22 Adjunct: Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers","Top20AbsAndTags":[237,376,287,311,445,120,220,4,486,212,439,452,277,118,89,339,248,424,267,396],"Top20Abs":[237,376,287,445,4,277,311,220,120,486,18,396,166,89,452,212,118,178,370,439],"Top20Tags":[160,339,348,129,93,452,221,27,387,239,120,70,82,216,309,243,366,424,419,53],"Citation":"Mavridou, I., Archer, J. A. W., Cleal, A., Fatoorechi, M., Stankoski, S., Kiprijanovska, I., Broulidakis, J., Gjoreski, M., Nduka, C., & Gjoreski, H. (2022). OCOsense Glasses for Facial Expressions Recognition and Contextual Affective Computing in Real World and Augmented Reality. Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing. https:\/\/doi.org\/10.1145\/3544793.3560325\n"},{"Title":"Bridging Curatorial Intent and Visiting Experience: Using AR Guidance as a Storytelling Tool","DOI":"10.1145\/3574131.3574438","Publication year":2022,"Abstract":"Augmented Reality (AR) visits enhances the art exhibition experience by overlaying digital content. Although there has been significant interest in AR guides, few works leverage AR to bridge curatorial intent and audiences understanding. This paper focuses on integrating the curatorial intent within the AR overlays by developing the narrative layers established by the relationships between works. We develop a narrative system that identifies and links the primary art pieces of the exhibition within a digital story consistent with the curator's perspective. The system is applied to a physical exhibition composed of seven art pieces. We evaluate the impact of AR overlays through two user experiments, conducted on art professionals and general audience, respectively. Both groups considered that the AR tour system improved interactivity, self-reported learning, and user satisfaction significantly (&gt; 4\/5). Besides, visitors found the system easy to get to do what they want to (4.7\/5), and would use it for future visits (4.6\/5). This study raises essential design considerations towards designing integrated AR museum guides that combine the perspective of artists and curators towards a better visiting experience.","LowLevel":"art;exhibitions;multimedia computing;museums","MidLevel":"cultural heritage;education;sales and marketing;liberal arts","HighLevel":"industries;business","Venue":"VRCAI'22: Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry","Top20AbsAndTags":[405,6,14,471,292,163,132,45,151,302,118,236,102,40,392,109,135,27,189,139],"Top20Abs":[405,14,6,151,471,45,163,292,118,102,189,135,49,31,40,0,27,122,139,132],"Top20Tags":[105,6,302,405,109,77,16,287,153,39,284,392,236,132,89,25,262,160,13,368],"Citation":"Gao, Z., Wang, A., Hui, P., & Braud, T. (2022). Bridging Curatorial Intent and Visiting Experience: Using AR Guidance as a Storytelling Tool. Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry. https:\/\/doi.org\/10.1145\/3574131.3574438\n"},{"Title":"AI Stethoscope for Home Self-Diagnosis with AR Guidance","DOI":"10.1145\/3560905.3568082","Publication year":2022,"Abstract":"Cardiopulmonary ailments are a major cause of mortality. Stethoscopes are one of the most important tools that healthcare professionals use to screen patients for a variety of ailments, especially those related to the heart and lungs. Despite the growth of digital stethoscopes on the market, it takes years of training to properly use stethoscopes to listen for abnormal sounds within the body. In this demonstration, we present an intelligent stethoscope platform that makes stethoscopes more accessible to the general population. Our platform utilizes augmented reality (AR) to provide real-time guidance on where to properly place the stethoscope on the body, enabling the general population to screen themselves for ailments.","LowLevel":"bioacoustics;cardiology;diseases;health care;learning algorithms;lung;medical signal processing;patient diagnosis","MidLevel":"artificial intelligence;audio;data;medical;sensors","HighLevel":"industries;technology","Venue":"SenSys '22: Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems","Top20AbsAndTags":[84,330,108,63,336,18,253,351,294,250,384,168,356,61,299,439,214,267,337,153],"Top20Abs":[330,84,63,351,153,239,108,18,294,50,267,439,248,188,410,45,126,91,191,43],"Top20Tags":[253,336,108,428,455,386,308,165,69,390,250,326,71,299,42,277,384,356,417,217],"Citation":"Hou, K., Xia, S., Wu, J., Zhao, M., Bejerano, E., & Jiang, X. (2022). AI Stethoscope for Home Self-Diagnosis with AR Guidance. Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems. https:\/\/doi.org\/10.1145\/3560905.3568082\n"},{"Title":"RetroSphere: Self-Contained Passive 3D Controller Tracking for Augmented Reality","DOI":"10.1145\/3569479","Publication year":2022,"Abstract":"Advanced AR\/VR headsets often have a dedicated depth sensor or multiple cameras, high processing power, and a high-capacity battery to track hands or controllers. However, these approaches are not compatible with the small form factor and limited thermal capacity of lightweight AR devices. In this paper, we present RetroSphere, a self-contained 6 degree of freedom (6DoF) controller tracker that can be integrated with almost any device. RetroSphere tracks a passive controller with just 3 retroreflective spheres using a stereo pair of mass-produced infrared blob trackers, each with its own infrared LED emitters. As the sphere is completely passive, no electronics or recharging is required. Each object tracking camera provides a tiny Arduino-compatible ESP32 microcontroller with the 2D position of the spheres. A lightweight stereo depth estimation algorithm that runs on the ESP32 performs 6DoF tracking of the passive controller. Also, RetroSphere provides an auto-calibration procedure to calibrate the stereo IR tracker setup. Our work builds upon Johnny Lee's Wii remote hacks and aims to enable a community of researchers, designers, and makers to use 3D input in their projects with affordable off-the-shelf components. RetroSphere achieves a tracking accuracy of about 96.5% with errors as low as ~3.5 cm over a 100 cm tracking range, validated with ground truth 3D data obtained using a LIDAR camera while consuming around 400 mW. We provide implementation details, evaluate the accuracy of our system, and demonstrate example applications, such as mobile AR drawing, 3D measurement, etc. with our Retrosphere-enabled AR glass prototype.","LowLevel":"calibration;cameras;microcontrollers;object tracking;optical radar;stereo image processing","MidLevel":"optics;computer vision;data;developers;sensors;input;geospatial;human-computer interaction","HighLevel":"end users and user experience;technology;displays","Venue":"Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. (USA)","Top20AbsAndTags":[435,48,433,425,189,277,379,488,151,105,451,210,297,116,158,220,119,8,326,419],"Top20Abs":[48,435,189,425,277,433,379,151,419,105,210,297,111,123,267,326,38,451,488,116],"Top20Tags":[379,425,451,220,18,396,300,347,201,98,120,277,119,221,73,317,422,297,72,433],"Citation":"Balaji, A. N., Kimber, C., Li, D., Wu, S., Du, R., & Kim, D. (2022). RetroSphere. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(4), 1\u201336. https:\/\/doi.org\/10.1145\/3569479\n"},{"Title":"Gesture and Voice Commands to Interact With AR Windshield Display in Automated Vehicle: A Remote Elicitation Study","DOI":"10.1145\/3543174.3545257","Publication year":2022,"Abstract":"Augmented reality (AR) windshield display (WSD) offers promising ways to engage in non-driving tasks in automated vehicles. Previous studies explored different ways WSD can be used to present driving and other tasks-related information and how that can affect driving performance, user experience, and performance in secondary tasks. Our goal for this study was to examine how drivers expect to use gesture and voice commands for interacting with WSD for performing complex, multi-step personal and work-related tasks in an automated vehicle. In this remote unmoderated online elicitation study, 31 participants proposed 373 gestures and 373 voice commands for performing 24 tasks. We analyzed the elicited interactions, their preferred modality of interaction, and the reasons behind this preference. Lastly, we discuss our results and their implications for designing AR WSD in automated vehicles.","LowLevel":"driver information systems;gesture recognition;human computer interaction;internet;user interfaces","MidLevel":"education;human factors;automotive;developers;input;human-computer interaction;networks","HighLevel":"industries;technology;end users and user experience","Venue":"AutomotiveUI '22: 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications","Top20AbsAndTags":[258,177,172,276,66,288,250,180,195,269,432,174,198,419,175,118,142,200,444,136],"Top20Abs":[258,276,172,66,269,177,288,195,180,432,250,444,174,200,142,198,149,104,303,264],"Top20Tags":[288,53,213,66,180,366,84,175,415,348,104,101,244,419,251,330,382,365,286,72],"Citation":"Ch, N. A. N., Tosca, D., Crump, T., Ansah, A., Kun, A., & Shaer, O. (2022). Gesture and Voice Commands to Interact With AR Windshield Display in Automated Vehicle: A Remote Elicitation Study. Proceedings of the 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications. https:\/\/doi.org\/10.1145\/3543174.3545257\n"},{"Title":"Guided Depth Completion with Instance Segmentation Fusion in Autonomous Driving Applications","DOI":"10.3390\/s22249578","Publication year":2022,"Abstract":"Pixel-level depth information is crucial to many applications, such as autonomous driving, robotics navigation, 3D scene reconstruction, and augmented reality. However, depth information, which is usually acquired by sensors such as LiDAR, is sparse. Depth completion is a process that predicts missing pixels' depth information from a set of sparse depth measurements. Most of the ongoing research applies deep neural networks on the entire sparse depth map and camera scene without utilizing any information about the available objects, which results in more complex and resource-demanding networks. In this work, we propose to use image instance segmentation to detect objects of interest with pixel-level locations, along with sparse depth data, to support depth completion. The framework utilizes a two-branch encoder-decoder deep neural network. It fuses information about scene available objects, such as objects' type and pixel-level location, LiDAR, and RGB camera, to predict dense accurate depth maps. Experimental results on the KITTI dataset showed faster training and improved prediction accuracy. The proposed method reaches a convergence state faster and surpasses the baseline model in all evaluation metrics.","LowLevel":"cameras;deep learning (artificial intelligence);feature extraction;image colour analysis;image reconstruction;image segmentation;mobile robots;object detection;optical radar","MidLevel":"artificial intelligence;optics;computer vision;medical;robotics;graphics;construction;liberal arts;chemical;developers;input;geospatial;other","HighLevel":"industries;technology;displays;other","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[81,300,98,170,235,437,162,425,220,451,255,20,396,430,381,116,173,95,358,484],"Top20Abs":[81,300,170,98,437,430,20,425,396,235,451,484,162,453,220,255,139,487,116,144],"Top20Tags":[95,381,415,255,160,81,387,425,98,451,143,318,18,120,370,358,173,280,396,99],"Citation":"El-Yabroudi, M. Z., Abdel-Qader, I., Bazuin, B. J., Abudayyeh, O., & Chabaan, R. C. (2022). Guided Depth Completion with Instance Segmentation Fusion in Autonomous Driving Applications. Sensors, 22(24), 9578. https:\/\/doi.org\/10.3390\/s22249578\n"},{"Title":"The Recommendation Augmented Reality: For Maritime Navigation Applications in Indonesia","DOI":"10.1145\/3575882.3575930","Publication year":2022,"Abstract":"This paper discusses the understanding of augmented reality (AR), types of AR, its current AR use, and reviews the used AR specifically for ship safety navigation. The paper will make recommendations on what type of AR products are possible to develop. The AR can be used to support maritime navigation applications in Indonesia. The method used is primary literature review analysis through Systematic Literature Review (SLR) from thirty articles. Results of the literature review show that four types of AR based on the projection techniques. The types are a marker-based AR (image recognition), markerless AR, projection-based AR (hologram), and super impositing-based AR (object recognition). Currently, in various countries, AR products have begun to be developed for maritime navigation applications such as AR social to determine point of interest (POI), AR navigation, AR safety, and an immersive underwater world. From the literature review analysis results, this paper recommends that the right AR products based on the projection technique are markerless AR and projection-based AR types. However, in the future, it is necessary to make an in-depth study of the need for AR products for maritime navigation in Indonesia. The products include traditional navigation tools, navigational aid, and simulations for marine navigation training and then be implemented so that the AR products for maritime navigation that are produced are truly in accordance with the needs of ship crew and policymakers in Indonesia.","LowLevel":"holography;image recognition;marine navigation;marine safety;object recognition;reviews;ships;traffic engineering computing","MidLevel":"human factors;engineering;government;computer vision;marine;transportation;inspection, safety and quality;navigation;standards;other;display technology","HighLevel":"displays;end users and user experience;use cases;industries;technology;standards;other","Venue":"IC3INA'22: Proceedings of the 2022 International Conference on Computer, Control, Informatics and Its Applications","Top20AbsAndTags":[139,70,270,63,102,163,60,33,87,53,155,46,110,93,135,324,2,65,218,41],"Top20Abs":[139,70,102,270,60,87,110,63,163,46,33,135,155,93,53,152,62,5,376,2],"Top20Tags":[3,20,33,197,120,211,476,97,348,324,39,288,9,456,144,396,421,487,48,41],"Citation":"Rezaldi, M. Y., Napitupulu, H., Husni, E., & Prakasa, E. (2022). The Recommendation Augmented Reality. Proceedings of the 2022 International Conference on Computer, Control, Informatics and Its Applications. https:\/\/doi.org\/10.1145\/3575882.3575930\n"},{"Title":"A Review of Research on Virtual Reality Technology Based on Human-Computer Interaction in Military","DOI":"10.1109\/ACAIT56212.2022.10137916","Publication year":2022,"Abstract":"Recent research shows that virtual reality is starting to be used in various field around the world. Virtual reality also has great application value in the civil industry, entertainment and education industries. However, compares to augmented reality, virtual reality has came into a period of bottleneck. Although virtual reality is developing rapidly, the effectiveness and using situation of virtual reality in practical application is still missing. Military operations are also seeking new developments and changes of virtual reality. By applying virtual reality into combat training, decision-making simulation and soldier training can reduce injury risk efficiently. This article classifies and summarizes current virtual reality technology applied in the military field, and looks forward to the virtual reality in military training, learning and actual warfares practical applications.","LowLevel":"computer based training;decision making;human computer interaction;injuries;military computing","MidLevel":"training;human factors;government;farming and natural science;medical;human-computer interaction","HighLevel":"industries;use cases;end users and user experience","Venue":"2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT)","Top20AbsAndTags":[233,439,47,372,54,355,207,249,2,414,301,226,342,202,179,132,86,204,440,262],"Top20Abs":[207,249,355,226,233,372,439,47,440,342,116,301,2,414,294,54,357,204,363,333],"Top20Tags":[54,233,419,179,33,210,405,314,112,202,339,94,140,439,382,113,262,57,180,122],"Citation":"Qiu, R., Xu, W., Wang, B., & Shen, Q. (2022). A Review of Research on Virtual Reality Technology Based on Human-Computer Interaction in Military. 2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT). https:\/\/doi.org\/10.1109\/acait56212.2022.10137916\n"},{"Title":"Augmented Reality in Primary Education: Adopting the new normal in learning by easily using AR-based Android applications","DOI":"10.1145\/3575879.3576016","Publication year":2022,"Abstract":"Augmented Reality technologies can be considered to be a fascinating choice for educators seeking resources and methods to stimulate their students about the topic they teach. In recent years, the increasing number of relevant educational applications indicates that this new technology has the potential of becoming a leading educational method in schools and universities. The current work overviews the importance and impact of this technology in widening primary students' knowledge and how it can be used by teachers worldwide to motivate and broaden educational horizons. Furthermore, this work examines a wide number of augmented reality applications for the Android mobile operating system, available online for educational purposes and analyzes the ways in which they can be helpful for schoolteachers, lecturers, and parents.","LowLevel":"android;computer aided instruction;educational institutions;mobile computing;teaching","MidLevel":"education;telecommunication;developers;training","HighLevel":"industries;technology;use cases","Venue":"PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics","Top20AbsAndTags":[46,82,77,241,103,171,246,51,188,216,163,57,320,62,184,87,128,30,196,204],"Top20Abs":[46,188,82,163,103,246,184,171,57,241,62,30,124,128,51,196,312,88,320,204],"Top20Tags":[77,82,216,93,164,51,391,241,320,89,87,222,151,3,405,410,33,185,139,376],"Citation":"Roussos, G., Aliprantis, J., Alexandridis, G., & Caridakis, G. (2022). Augmented Reality in Primary Education: Adopting the new normal in learning by easily using AR-based Android applications. Proceedings of the 26th Pan-Hellenic Conference on Informatics. https:\/\/doi.org\/10.1145\/3575879.3576016\n"},{"Title":"Beam Management Technique For 5G Wireless Communication: A Deep Learning Approach","DOI":"10.1109\/IATMSI56455.2022.10119247","Publication year":2022,"Abstract":"The expectation from next-generation 5G technology is to satisfy data-hungry applications such as vehicular applications and augmented and virtual reality. 5G along with millimeter-wave technology is a prominent technology that can satisfy user demands and needs as it provides a high data rate. The accomplishment of a high data rate is possible using a large number of antennas. However, the increased number of antennas results in increased path loss. Apart from increased path loss, beam misalignment is another challenge that can be overcome by robust beam forming techniques. Beam management involves providing a comprehensive solution to various challenges associated with the beam. This paper provides an overview of beam management procedures using deep learning to reduce beam alignment problems. Beam management includes various steps such as channel state estimation, beam training, beam selection, beam alignment, and beamforming. Based on the analysis, the paper also provides a comprehensive framework for improved beam management. Finally, the paper also discusses the open issues in this field of work.","LowLevel":"5g mobile communication;array signal processing;deep learning (artificial intelligence);millimetre wave communication","MidLevel":"artificial intelligence;telecommunication;data;medical;liberal arts;sensors;input;other;semiconductors","HighLevel":"industries;technology;other","Venue":"2022 IEEE Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)","Top20AbsAndTags":[201,212,404,158,401,326,44,256,268,267,456,432,257,416,350,150,426,425,160,423],"Top20Abs":[201,212,404,44,256,267,401,326,159,150,456,74,86,486,171,324,141,432,469,268],"Top20Tags":[18,266,160,271,257,326,95,308,415,416,387,423,230,115,255,426,425,256,404,268],"Citation":"Pawar, S., & Venkatesan, M. (2022). Beam Management Technique For 5G Wireless Communication: A Deep Learning Approach. 2022 IEEE Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI). https:\/\/doi.org\/10.1109\/iatmsi56455.2022.10119247\n"},{"Title":"A Study on Contextual Task Performance of Simulated Homonymous Hemianopia Patients with Computational Glasses-based Compensation","DOI":"10.1145\/3574131.3574441","Publication year":2022,"Abstract":"People with Homonymous Hemianopia (HH) suffer from losing ipsilateral half side of visual field in both eyes, which results in failing to obtain visual information in the lost field. Making using of the remaining of the visual field, the state-of-the-art studies proposed Overlaid Overview Window (OOW) and Edge Indicator (EI) on the basis of Augmented-Reality (AR) glasses for compensation. However, experiments conducted in these studies investigate user performance with tasks involving events in lost field or remaining field singly. On the other hand, both studies recruited normal individuals for mock experiment, while their way to simulate HH, which requiring the participants to fix their view angles, were not practical to real HH patients. In this study, we conduct a contextual information experiment to investigate the user performance involving in the task requiring the information across both the visible and invisible sides of HH, with the compensation of OOW and Flicker-based EI (FEI). At the same time, we also recruit volunteers with normal vision for mock experiment, while the participants in our study are allowed to move their gaze freely, because we simulate the invisible field of HH on AR glasses with eye tracking. The experiment results showed that OOW is better for the task that related to move something from the remaining FoV to the lost FoV, while FEI is better for moving something from the lost FoV to the remaining FoV.","LowLevel":"eye;gaze tracking;vision defects;visual perception","MidLevel":"inspection, safety and quality;computer vision;human factors;input","HighLevel":"end users and user experience;technology;use cases","Venue":"VRCAI'22: Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry","Top20AbsAndTags":[32,358,277,259,263,252,376,237,95,195,444,72,242,123,107,372,316,250,170,118],"Top20Abs":[358,32,277,376,252,259,444,237,248,372,242,195,263,25,95,412,170,341,357,37],"Top20Tags":[95,71,72,259,198,263,107,32,122,250,243,460,277,196,345,123,380,322,182,206],"Citation":"Ge, C., Zhu, Z., Ichinose, K., Fujishiro, I., Toyoura, M., Go, K., Kashiwagi, K., & Mao, X. (2022). A Study on Contextual Task Performance of Simulated Homonymous Hemianopia Patients with Computational Glasses-based Compensation. Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry. https:\/\/doi.org\/10.1145\/3574131.3574441\n"},{"Title":"Exploring the Use of Smartphones as Input Devices for the Mixed Reality Environment","DOI":"10.1145\/3574131.3574451","Publication year":2022,"Abstract":"Nowadays, researches on mixed reality (MR) have made a lot of exploration in the aspects of user experience, hardware devices, interaction technologies, application systems, etc. However, more research is still needed to explore how to improve the experience in the shared MR Environment and design appropriate collaborative interaction models. The traditional VR handle can realize tool simulation and 3D interaction, while smartphones can realize 2D interactions such as fast 2D gestures, text input and handwriting. Using the mobile phone as the handle of the MR headset can realize the complementary advantages of the two. In this study, we design a cross-device collaborative system sharing a hybrid reality environment. In the MR Environment, the smartphone will realize the functions of the controller for remote selection and manipulation of objects, and its advantages in 2D interaction can be well applied to some special tasks. We design the user interface with three levels of immersion: high, medium and low. High immersion: We provide a hybrid user interface combining a HoloLens2 and a smartphone. Medium immersion: We provide a 3D user interface represented by HoloLens2. Low immersion: We provide a multi-touch user interface represented by tablet. User studies show that the hybrid user interfaces can bring users satisfactory immersion and interactive experience, but it also needs to design accurate and efficient input methods according to the interactive tasks.","LowLevel":"computer based training;gesture recognition;human computer interaction;mobile computing;production engineering computing;smartphones;user interfaces","MidLevel":"training;human factors;engineering;telecommunication;farming and natural science;liberal arts;input;manufacturing;human-computer interaction","HighLevel":"industries;technology;use cases;end users and user experience","Venue":"VRCAI'22: Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry","Top20AbsAndTags":[424,248,418,397,439,360,79,303,348,365,142,27,440,331,32,203,279,77,216,224],"Top20Abs":[424,418,397,248,303,439,79,440,360,153,224,80,75,32,365,189,395,331,142,200],"Top20Tags":[248,339,407,348,77,388,93,424,370,422,142,82,112,415,205,167,301,436,221,12],"Citation":"Pan, Z., Pan, Z., Luo, T., & Zhang, M. (2022). Exploring the Use of Smartphones as Input Devices for the Mixed Reality Environment. Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry. https:\/\/doi.org\/10.1145\/3574131.3574451\n"},{"Title":"School foodservice directors' national training practices","DOI":"10.1111\/jfs.13010","Publication year":2022,"Abstract":"In the United States, over 29 million students daily are consuming meals at school. This number continues to increase as more diverse meal options are presented and increased demand for meal assistance. Very young students (under the age of 5) are considered a high-risk population for foodborne illness. Though numerous interventions have been implemented previously, several predominant food safety violations continue to persist. To better understand and improve targeted food safety trainings and interventions this study aimed to conduct a needs assessment of school foodservice directors' training programs and topics. A total of 1,250 electronic surveys were sent nationally to K-12 school foodservice directors. Of that, 264 useable responses were analyzed. Descriptive statistics were used to summarize demographics, as well as identified training methods and topics. Structural modeling was used to identify relatedness ratings between training methods. Approximately 67% of directors reported currently make less than 40% of the menu items from scratch. On-the-job training (94%) was the most frequent training method, while augmented reality and virtual reality were the least with 1.9% and 1.5%, respectively. Structural modeling identified two central nodes, on-the-job (six pairings) and lecture-style (four pairings). All other modalities had one to three relationship pairings. For the majority (73%), the average length of training is less than 2 hr. The top two training topics school foodservice directors reported providing were: food safety (95.5% very likely\/likely) and food production (94.3% very likely\/likely). Study results provide insights into current training methods and topics provided by school foodservice directors at the time of the survey. &#169; 2022 Wiley Periodicals LLC.","LowLevel":"educational institutions;food processing industry;food products;food safety;on-the-job training;personnel","MidLevel":"education;training;human resources;data;inspection, safety and quality;farming and natural science","HighLevel":"industries;technology;business;use cases","Venue":"J. Food Saf. (USA)","Top20AbsAndTags":[196,241,312,56,320,54,436,113,168,91,414,218,140,163,145,87,369,408,112,233],"Top20Abs":[91,196,168,312,241,294,113,436,54,408,140,56,414,218,320,340,163,87,104,233],"Top20Tags":[54,94,140,68,418,84,273,117,78,325,360,241,260,345,90,113,374,405,188,134],"Citation":"Reynolds, J., Jeong, H., & Nam, C. S. (2022). School foodservice directors\u2019 national training practices. Journal of Food Safety, 42(6). Portico. https:\/\/doi.org\/10.1111\/jfs.13010\n"},{"Title":"A Low-Power mmWave Platform for the Internet of Things","DOI":"10.1145\/3583571.3583575","Publication year":2022,"Abstract":"With the advancement of the Internet of Things (IoT), billions of devices will be connected to the Internet, enabling new applications such as digital twin, augmented reality, and smart home. These applications have placed a huge strain on today's wireless network. mmWave technology is promising to solve this problem by providing a large bandwidth over the very-high-frequency spectrum band. However, most mmWave radios and platforms have much higher power consumption than what IoT devices and their applications can afford. Hence, mmWave networks cannot be utilized in most IoT applications today. In this work, we present a novel low-power mmWave platform, which brings this technology to IoT applications. Our approach to design this platform is to take a holistic view and optimize the whole wireless system by considering practical challenges in mmWave communication. Our lowcost and low-power platform not only brings mmWave communication to IoT applications, but also enables researchers that do not have hardware background to work on mmWave research.","LowLevel":"5g mobile communication;digital twins;internet of things;millimetre wave communication;power consumption;telecommunication power management","MidLevel":"internet of things;device energy management;smart cities;telecommunication;input;networks;other","HighLevel":"displays;use cases;industries;technology;other","Venue":"GetMob., Mob. Comput. Commun. (USA)","Top20AbsAndTags":[129,404,447,395,209,423,175,268,220,123,38,271,448,159,345,215,359,286,121,110],"Top20Abs":[404,129,447,395,423,209,448,220,268,175,159,215,345,271,18,38,316,273,210,486],"Top20Tags":[123,258,428,129,220,38,264,271,298,175,209,110,286,121,395,378,423,359,289,268],"Citation":"Mazaheri, M., & Abari, O. (2023). A Low-Power mmWave Platform for the Internet of Things. GetMobile: Mobile Computing and Communications, 26(4), 14\u201318. https:\/\/doi.org\/10.1145\/3583571.3583575\n"},{"Title":"Envisioning A Hyper-Learning System in the Age of Metaverse","DOI":"10.1145\/3574131.3574427","Publication year":2022,"Abstract":"The digital technologies, such as interactive interfaces, augmented reality (AR), and virtual reality (VR), are emerging as the latest examples of an ongoing trend of digitizing learning in the metaverse. Their pervasive impact requires us to rethink the notion of information gathering and learning [Pokhrel and Chhetri 2021]. Although much research has been devoted to AR\/VR\/metaverse education, there is little research on the pedagogical interactions of pre-learning information in the metaverse. Pre-learning refers to the preparatory activity performed before formal learning. Whether we use pre-learning to discover new concepts, or as a travel guide throughout a deeper understanding of a topic, pre-learning requires massive information access. Currently, information presentation remains simple, constrained by the access media. For instance, web pages are mostly arranged in tabs as a form of listing, which does not enable users to organize the information well in relation to each other. With instant online information access updated constantly, the severity of these problems is becoming more signi&#58907;cant. Although information organization tools like Miro are now available to create associations and hierarchies and custom edits, they remain constrained to the 2 dimensions of the computer screen. By introducing a third dimension, immersive technologies and the metaverse help us present this information in a hybrid 3D way thatis more diverse and aesthetic. Therefore, we design a prototype of an extended reality (XR) learning interactive system in the metaverse. The system relies on custom catalogs to guide the user through the pre-learning process, allowing categorization and custom combinations of primary and secondary information in a virtual reality environment. It addresses the need for personalization and browsing categorization of web-based information access in pre-learning. The research gives acritical lens of the interaction with information interface in the process of pre-learning in the metaverse.","LowLevel":"computer aided instruction;internet;user interfaces","MidLevel":"training;human-computer interaction;networks","HighLevel":"end users and user experience;technology;use cases","Venue":"VRCAI'22: Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry","Top20AbsAndTags":[363,442,193,163,249,377,355,306,226,204,328,251,57,262,215,296,142,114,320,39],"Top20Abs":[363,442,193,377,355,163,249,226,204,354,306,251,328,215,262,77,114,39,296,87],"Top20Tags":[363,306,57,163,62,330,33,414,204,376,242,251,405,213,364,34,238,17,9,327],"Citation":"Wang, A., Gao, Z., Wang, Z., Hui, P., & Braud, T. (2022). Envisioning A Hyper-Learning System in the Age of Metaverse. Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry. https:\/\/doi.org\/10.1145\/3574131.3574427\n"},{"Title":"Design of a Hybrid Total-internal-reflection Collimator","DOI":"10.1109\/Nanoman-AETS56035.2022.10119463","Publication year":2022,"Abstract":"A hybrid structural total internal reflection (TIR) collimator is designed based on Snell's law for the secondary light distribution for the projectors. The performance of the system were evaluated through simulation. The illumination uniformity of the system reached over 94% and the luminous flux utilization rate was over 91% based on simulation using by LightTools software. The axial-symmetrical collimating system has various impressive features, for instance, simple and compact structure, small size, lightweight, and high illumination uniformity, which can meet the requirements of the augmented reality (AR) glasses or projectors.","LowLevel":"optical design techniques;optical projectors","MidLevel":"optics;human-computer interaction","HighLevel":"end users and user experience;displays","Venue":"2022 8th International Conference on Nanomanufacturing &amp; 4th AET Symposium on ACSM and Digital Manufacturing (Nanoman-AETS)","Top20AbsAndTags":[141,45,212,153,120,98,175,0,318,324,23,444,137,12,345,62,437,57,158,169],"Top20Abs":[141,212,153,45,120,248,62,98,318,324,345,294,446,12,175,287,152,23,158,137],"Top20Tags":[422,343,52,153,169,107,317,444,210,205,290,436,122,314,0,33,344,74,339,255],"Citation":"Zhou, Y., Fang, F., & Zhang, J. (2022). Design of a hybrid Total-internal-reflection Collimator. 2022 8th International Conference on Nanomanufacturing &amp; 4th AET Symposium on ACSM and Digital Manufacturing (Nanoman-AETS). https:\/\/doi.org\/10.1109\/nanoman-aets56035.2022.10119463\n"},{"Title":"Augmented reality and image processing solutions for sport events and training : principles and practice","DOI":"10.1109\/ITSIS56166.2022.10118428","Publication year":2022,"Abstract":"Emergence of new information technologies (NTIC), cameras and video cameras have become ubiquitous. Image processing algorithms and applications are used in many fields, particularly in sports. This is the context of our article. In the first section, we will present the interest of using these applications for sports competitions. In the second section, the identification of players experienced in various applications using a tracking camera during sports competitions will be described. In the third section, a real-time distributed solution that we have proposed allowing to automatically building models of the actors followed thanks to its capacity to support the probabilistic fusion of various sources of information and to structure the incoming perceptions will be described. Finally, this article will end with the description of a system solution that we have proposed and which has been tested with real images.","LowLevel":"cameras;image processing;sports;video cameras","MidLevel":"cultural heritage;video;computer vision;data;input","HighLevel":"industries;technology","Venue":"2022 IEEE Information Technologies &amp; Smart Industrial Systems (ITSIS)","Top20AbsAndTags":[215,48,76,252,81,425,486,178,263,267,433,26,98,20,12,116,1,173,421,105],"Top20Abs":[215,48,252,1,76,20,41,81,263,12,425,267,178,401,435,73,26,467,421,116],"Top20Tags":[99,267,98,76,23,353,48,381,421,370,144,173,198,71,396,72,425,73,243,81],"Citation":"Yosra, M., Lotfi, M., Najeh, L. M., Imed, J., & Tahar, B. (2022). Augmented reality and image processing solutions for sport events and training\u202f: principles and practice. 2022 IEEE Information Technologies &amp; Smart Industrial Systems (ITSIS). https:\/\/doi.org\/10.1109\/itsis56166.2022.10118428\n"},{"Title":"Evaluating Working Postures with the Use of Augmented Reality and NIOSH Mobile Application","DOI":"10.1145\/3572549.3572644","Publication year":2022,"Abstract":"The evaluation of the maximum weight that a person can support during the performance of an activity related to the production or provision of services is a task of vital importance to ensure that our workers are not at risk of injuries or illnesses that impede them from doing their day-to-day job in the medium and long termSeveral studies affirm that about 20% of all injuries produced in the workplace are back injuries, and that about 30% are due to overexertion. These data provide an idea of the importance of a correct evaluation of the tasks that involve lifting loads and the adequate preparation of the positions involved. There are many measurements that we can carry out thanks to the use of methodologies such as RULAS, OWAS or the calculation of the NIOSH equation, this last topic will be addressed in this paper and how the use of NIOSH mobile applications and augmented reality achieve that the evaluation of an operator's task is carried out more efficiently.Early results have proven that there is a gain in the time that it takes to perform the activity without sacrificing the accuracy of the analysis.","LowLevel":"ergonomics;injuries;lifting;occupational health;occupational safety;personnel","MidLevel":"human factors;human resources;medical;inspection, safety and quality;other","HighLevel":"end users and user experience;use cases;business;industries;other","Venue":"ICETC '22: Proceedings of the 14th International Conference on Education Technology and Computers","Top20AbsAndTags":[328,360,78,436,84,114,101,269,179,454,87,195,461,117,54,311,264,242,134,388],"Top20Abs":[360,78,269,114,454,328,264,461,101,242,436,179,87,479,311,399,388,267,134,77],"Top20Tags":[84,54,380,328,436,273,375,140,117,206,78,345,68,205,360,481,101,66,195,307],"Citation":"Gonzalez-Mendivil, J. A., Rodriguez-Paz, M. X., & Zamora-Hernandez, I. (2022). Evaluating Working Postures with the Use of Augmented Reality and NIOSH Mobile Application. Proceedings of the 14th International Conference on Education Technology and Computers. https:\/\/doi.org\/10.1145\/3572549.3572644\n"},{"Title":"How is Technology Accepted? Fundamental Works in User Technology Acceptance from Diffusion of Innovations to UTAUT-2","DOI":"10.1145\/3568834.3568903","Publication year":2022,"Abstract":"The decision to accept and use technology innovations has long been a source of debate across disciplines due to the complexity involved in predicting behavior. Recognizing that the subject is vast and fragmented, this paper examines the mainstream technology works to assist researchers to understand, conceptualize and select the most appropriate theoretical framework for their study. Starting with the pioneering effort on Diffusion of Innovations (DOI\/IDT), the analysis considers the Theory of Reasoned Action (TRA), the Theory of Planned Behavior (TPB), the Technology Acceptance Model (TAM\/TAM-2\/TAM-3), the Value-based Acceptance Model (VAM), and the Unified Theory of Acceptance and Use of Technology (UTAUT\/UTAUT-2) among the most important. A review of the key literature is vital to assessing and identifying research trends, as well as contributing to the discussion of emerging technologies such as Artificial Intelligence (AI), Augmented Reality (AR), Blockchain, Cloud Computing, Internet of Things (IoT), Mobile Apps, etc. Suggestions for future research paths are also provided.","LowLevel":"artificial intelligence;blockchains;cloud computing;data analysis;human factors;internet of things;mobile computing;technology acceptance model","MidLevel":"internet of things;artificial intelligence;human factors;telecommunication;data;security;liberal arts;human-computer interaction;networks;other","HighLevel":"end users and user experience;technology;industries;other","Venue":"ICIBE '22: Proceedings of the 8th International Conference on Industrial and Business Engineering","Top20AbsAndTags":[62,57,289,216,246,110,447,159,175,136,196,448,286,355,271,315,281,308,102,298],"Top20Abs":[62,57,246,216,281,447,196,159,448,136,110,308,102,184,289,87,355,315,175,103],"Top20Tags":[378,289,129,62,271,298,226,355,377,339,146,123,175,220,121,258,57,286,110,38],"Citation":"Lampo, A. (2022). How is Technology Accepted? Fundamental Works in User Technology Acceptance from Diffusion of Innovations to UTAUT-2. Proceedings of the 8th International Conference on Industrial and Business Engineering. https:\/\/doi.org\/10.1145\/3568834.3568903\n"},{"Title":"Physical and Augmented Reality based Playful Activities for Refresher Training of ASHA Workers in India","DOI":"10.1145\/3516492.3558788","Publication year":2022,"Abstract":"Recent health surveys in India highlight the alarming child malnutrition levels and lower rates of complete child immunization in many parts of India. Previous researches report that the conventional training pedagogy of the CHWs (Community Healthcare Workers) or the ASHAs (Accredited Social Health Activists) in India is ineffective in enhancing their capacity. Considering that the CHWs are getting equipped with smartphones, it calls for a rethinking of their training pedagogy using the ICT approach. Two refresher training tools were developed to make learning the child immunization schedule more exciting and conceptually engaging for ASHAs. The physical and AR (Augmented Reality) versions of designed card games were compared for effectiveness and knowledge retention, pre, and post-intervention through questionnaire tests conducted immediately before and after playing multiple sessions. The AR-based play was found to be better in learning and knowledge retention with more engagement, mainly due to its interactive and intuitive nature of play.","LowLevel":"biomedical education;computer based training;health care;medical computing;personnel;scheduling;smartphones","MidLevel":"education;training;human resources;telecommunication;medical;liberal arts;developers;farming and natural science","HighLevel":"industries;technology;business;use cases","Venue":"CHI22: Asian HCI Symposium'22","Top20AbsAndTags":[115,83,442,180,42,57,91,136,241,163,62,408,188,103,113,117,82,34,164,233],"Top20Abs":[115,83,91,442,180,136,188,42,163,241,113,57,222,292,62,400,164,408,34,284],"Top20Tags":[117,390,376,232,93,408,233,47,112,42,301,82,9,54,336,419,204,380,113,262],"Citation":"Majhi, A., Agnihotri, S., & Mondal, A. (2022). Physical and Augmented Reality based Playful Activities for Refresher Training of ASHA Workers in India. Asian HCI Symposium \u201922. https:\/\/doi.org\/10.1145\/3516492.3558788\n"},{"Title":"Designing a new approach to augmented reality for early childhood self-learning capability at home.","DOI":"10.1145\/3568231.3568269","Publication year":2022,"Abstract":"Augmented reality is a technology that can improve students' learning experience, motivation, and learning achievement. This technology can be used to teach concepts and materials that are abstract and difficult to understand in early childhood. In its application, a good learning process should be centered on the learner. But the challenge must be overcome to design AR media that can be used for independent learning. AR design and features must be easy to use and understand by all students with different technological knowledge backgrounds. In this study, AR learning media was successfully designed with a new, more humanist, and interactive approach to teaching symbols and concepts of numbers 1-20. Each stage of the media design process that has been carried out is described in a structured manner by combining the ADDIE (Analysis, Design, Development, Implementation, and Evaluation) and RAD (Rapid Application Development) methods. We have validated the design of media and materials with three experts each. The validation results obtained are 93.7% for the feasibility of media design and 92.1% for the feasibility of the material. These results prove that AR design by displaying avatar objects as 3D virtual friends can support early childhood to study independently at home with more fun, easy, and exciting learning atmosphere and experience.","LowLevel":"computer aided instruction;human factors;teaching","MidLevel":"education;human factors;training","HighLevel":"industries;use cases;end users and user experience","Venue":"SIET22: 7th International Conference on Sustainable Information Engineering and Technology 2022","Top20AbsAndTags":[9,77,163,262,39,87,320,306,82,145,185,57,418,369,342,105,136,33,88,62],"Top20Abs":[9,77,163,262,39,87,306,82,418,369,145,320,185,342,105,57,88,33,62,115],"Top20Tags":[33,87,196,154,204,135,320,195,46,17,88,145,51,147,62,363,23,57,161,306],"Citation":"Riwu, M. C. H., Tolle, H., & Budi, A. S. (2022). Designing a new approach to augmented reality for early childhood self-learning capability at home. 7th International Conference on Sustainable Information Engineering and Technology 2022. https:\/\/doi.org\/10.1145\/3568231.3568269\n"},{"Title":"A Comparative Study of Two Marker-Based Mobile Augmented Reality Applications for Solving 3D Anamorphic Illusion Puzzles","DOI":"10.1145\/3574131.3574443","Publication year":2022,"Abstract":"Anamorphic illusions are a class of optical illusions wherein objects are perspectively distorted in some way so that the object becomes recognizable when viewing them from a certain point of view or direction. We develop two marker-based mobile augmented reality applications to demonstrate 3D anamorphic illusions. We frame this as a puzzle-solving mechanic where users must align the anamorphic pieces through the movement of the device camera to form a distinguishable virtual model. The first AR proposed utilizes 2D printable markers (2D marker-based AR). In contrast, the second AR uses tabletop items as markers, such as cereal boxes, tin cans, action figures, and the like (3D marker-based AR). The AR applications differ regarding scene setup, user interactions, and examples of anamorphic illusions. We sliced public 3D models, and the corresponding slices were randomly distributed in a given virtual space, using a camera viewpoint where the model would become recognizable. Our proposed framework ensures that each playthrough provides a new anamorphic illusion. Early user testing results show that our 2D-marker-based AR application is more effective in showcasing anamorphic illusions.","LowLevel":"cameras;cans;computer games;mobile computing","MidLevel":"telecommunication;liberal arts;other;input","HighLevel":"industries;technology;other","Venue":"VRCAI'22: Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry","Top20AbsAndTags":[105,41,48,155,116,189,53,49,163,82,130,419,51,142,294,415,340,79,151,89],"Top20Abs":[105,48,155,41,116,189,53,49,82,130,163,415,142,151,340,419,45,87,98,316],"Top20Tags":[41,48,197,147,164,410,204,111,254,51,72,14,93,146,82,73,59,351,105,129],"Citation":"Buhion, D. R., Dizon, M. N., Go, T. E., Oafallas, K. N., Joya, P. J., Mangune, A. C., Nerie, S. P., & Del Gallego, N. P. (2022). A Comparative Study of Two Marker-Based Mobile Augmented Reality Applications for Solving 3D Anamorphic Illusion Puzzles. Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry. https:\/\/doi.org\/10.1145\/3574131.3574443\n"},{"Title":"A 3D Scene Information Enhancement Method Applied in Augmented Reality","DOI":"10.3390\/electronics11244123","Publication year":2022,"Abstract":"Aiming at the problem that the detection of small planes with unobvious texture is easy to be missed in augmented reality scene, a 3D scene information enhancement method to grab the planes for augmented reality scene is proposed based on a series of images of a real scene taken by a monocular camera. Firstly, we extract the feature points from the images. Secondly, we match the feature points from different images, and build the three-dimensional sparse point cloud data of the scene based on the feature points and the camera internal parameters. Thirdly, we estimate the position and size of the planes based on the sparse point cloud. The planes can be used to provide extra structural information for augmented reality. In this paper, an optimized feature points extraction and matching algorithm based on Scale Invariant Feature Transform (SIFT) is proposed, and a fast spatial planes recognition method based on a RANdom SAmple Consensus (RANSAC) is established. Experiments show that the method can achieve higher accuracy compared to the Oriented Fast and Rotated Brief (ORB), Binary Robust Invariant Scalable Keypoints (BRISK) and Super Point. The proposed method can effectively solve the problem of missing detection of faces in ARCore, and improve the integration effect between virtual objects and real scenes.","LowLevel":"cameras;feature extraction;image matching;image texture;object detection;transforms","MidLevel":"graphics;computer vision;chemical;input","HighLevel":"industries;technology","Venue":"Electronics (Switzerland)","Top20AbsAndTags":[318,321,451,76,132,438,471,20,350,379,340,48,341,92,107,81,301,116,99,198],"Top20Abs":[318,321,451,76,438,20,471,350,132,379,340,92,341,48,301,433,99,435,107,299],"Top20Tags":[132,318,99,81,379,71,387,48,143,255,381,76,116,5,172,396,425,274,358,343],"Citation":"Li, B., Wang, X., Gao, Q., Song, Z., Zou, C., & Liu, S. (2022). A 3D Scene Information Enhancement Method Applied in Augmented Reality. Electronics, 11(24), 4123. https:\/\/doi.org\/10.3390\/electronics11244123\n"},{"Title":"Color Offset Compensation Method of Art Image Based on Augmented Reality Technology","DOI":"10.1109\/ACAIT56212.2022.10137889","Publication year":2022,"Abstract":"In the production of multi-scale block-fused art images, the image quality is poor due to the sudden change and occlusion of color shift. This paper puts forward a compensation method for color shift of multi-scale block-fused art images based on augmented reality technology. Based on the attenuation of optical parameters and the control of color balance, a multi-dimensional color fusion model of background light correlation of multi-scale block fused art images is established, and the augmented reality model of multi-scale block fused art images is constructed by combining the analysis method of surface features and illumination features distribution model of art images. Through the parameter analysis of each level feature map model of the similarity degree of the previous frame under color deviation, The gray texture and color texture features of multi-scale block-fused art images with complementary advantages and disadvantages are extracted, and augmented reality technology is adopted to realize gray scale enhancement and color enhancement in the process of color compensation of art images. Combined parameter identification method is adopted to realize color adjustment and feedback compensation control of output stability of art images. According to the characteristics of high-order moment output stability of color features, color offset compensation and optimal imaging processing of multi-scale block-fused art images are realized by calculating and counting boundary corner information and texture parameter analysis. The test shows that this method performs the color offset processing of multi-scale block fusion art image sensor, improves the color offset compensation ability of art images and the true color imaging quality of images, and increases the peak signal-to-noise ratio of output images.","LowLevel":"compensation;feature extraction;feedback;image colour analysis;image fusion;image processing;image texture;parameter estimation","MidLevel":"video;computer vision;data;graphics;chemical;human-computer interaction;other","HighLevel":"industries;technology;other;end users and user experience","Venue":"2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT)","Top20AbsAndTags":[95,438,76,132,405,318,98,476,467,149,441,480,473,20,350,469,288,300,486,462],"Top20Abs":[95,438,76,405,480,318,98,132,476,20,288,467,453,350,473,469,486,149,300,462],"Top20Tags":[98,255,76,381,415,119,379,71,81,387,422,72,421,132,144,425,318,172,353,116],"Citation":"Zhang, Y. (2022). Color Offset Compensation Method of Art Image Based on Augmented Reality Technology. 2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT). https:\/\/doi.org\/10.1109\/acait56212.2022.10137889\n"},{"Title":"The Necessity of Emotion Recognition from Speech Signals for Natural and Effective Human-Robot Interaction in Society 5.0","DOI":"10.1109\/ISDFS55398.2022.9800837","Publication year":2022,"Abstract":"The history of humanity has reached Industry 4.0 that aims to the integration of information technologies and especially artificial intelligence with all life-sustaining mechanisms in the 21st century, and consecutively, the transformation of Society 5.0 has begun. Society 5.0 means a smart society in which humans share life with physical robots and software robots as well as smart devices based on augmented reality. Industry 4.0 contains main structures such as the internet of things, big data analytics, digital transformation, cyber-physical systems, artificial intelligence, and business processes optimization. It is impossible to consider the machines to be without emotions and emotional intelligence within the transformation of smart tools and artificial intelligence, in addition, while it is planned to give most of the commands with voice and speaking, it became more important to develop algorithms that can detect emotions. In the smart society, new and rapid methods are needed for speech recognition, emotion recognition, and speech emotion recognition areas to maximize human-computer (HCI) or human-robot interaction (HRI) and collaboration. In this study, speech recognition and speech emotion recognition studies in robot technology are investigated and developments are revealed.","LowLevel":"artificial intelligence;big data;emotion recognition;human computer interaction;human-robot interaction;speech recognition","MidLevel":"artificial intelligence;human factors;data;robotics;liberal arts;input;human-computer interaction","HighLevel":"end users and user experience;technology;industries","Venue":"2022 10th International Symposium on Digital Forensics and Security (ISDFS)","Top20AbsAndTags":[281,19,174,361,70,180,429,159,122,447,205,203,311,296,487,289,255,376,328,216],"Top20Abs":[281,361,19,180,174,70,447,311,203,159,487,296,376,110,328,289,429,224,44,89],"Top20Tags":[255,19,70,339,407,205,122,308,166,159,58,313,174,415,429,18,289,368,160,281],"Citation":"S\u00f6nmez, Y. \u00dc., & Varol, A. (2022). The Necessity of Emotion Recognition from Speech Signals for Natural and Effective Human-Robot Interaction in Society 5.0. 2022 10th International Symposium on Digital Forensics and Security (ISDFS). https:\/\/doi.org\/10.1109\/isdfs55398.2022.9800837\n"},{"Title":"Genie in the Model: Automatic Generation of Human-in-the-Loop Deep Neural Networks for Mobile Applications","DOI":"10.1145\/3580815","Publication year":2022,"Abstract":"Advances in deep neural networks (DNNs) have fostered a wide spectrum of intelligent mobile applications ranging from voice assistants on smartphones to augmented reality with smart-glasses. To deliver high-quality services, these DNNs should operate on resource-constrained mobile platforms and yield consistent performance in open environments. However, DNNs are notoriously resource-intensive, and often suffer from performance degradation in real-world deployments. Existing research strives to optimize the resource-performance trade-off of DNNs by compressing the model without notably compromising its inference accuracy. Accordingly, the accuracy of these compressed DNNs is bounded by the original ones, leading to more severe accuracy drop in challenging yet common scenarios such as low-resolution, small-size, and motion-blur. In this paper, we propose to push forward the frontiers of the DNN performance-resource trade-off by introducing human intelligence as a new design dimension. To this end, we explore human-in-the-loop DNNs (H-DNNs) and their automatic performance-resource optimization. We present H-Gen, an automatic H-DNN compression framework that incorporates human participation as a new hyperparameter for accurate and efficient DNN generation. It involves novel hyperparameter formulation, metric calculation, and search strategy in the context of automatic H-DNN generation. We also propose human participation mechanisms for three common DNN architectures to showcase the feasibility of H-Gen. Extensive experiments on twelve categories of challenging samples with three common DNN structures demonstrate the superiority of H-Gen in terms of the overall trade-off between performance (accuracy, latency), and resource (storage, energy, human labour).","LowLevel":"deep learning (artificial intelligence);mobile computing;smartphones","MidLevel":"artificial intelligence;telecommunication;medical;liberal arts;other","HighLevel":"industries;technology;other","Venue":"Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. (USA)","Top20AbsAndTags":[280,271,95,416,467,403,346,257,404,227,423,118,111,255,205,387,385,122,341,268],"Top20Abs":[280,271,95,416,404,205,423,467,268,257,403,263,447,387,111,12,341,123,401,264],"Top20Tags":[238,95,339,18,27,387,115,93,415,160,385,308,255,329,271,7,428,227,129,122],"Citation":"Wang, Y., Yu, Z., Liu, S., Zhou, Z., & Guo, B. (2022). Genie in the Model. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 7(1), 1\u201329. https:\/\/doi.org\/10.1145\/3580815\n"},{"Title":"Privacy-Enhancing Technology and Everyday Augmented Reality: Understanding Bystanders' Varying Needs for Awareness and Consent","DOI":"10.1145\/3569501","Publication year":2022,"Abstract":"Fundamental to Augmented Reality (AR) headsets is their capacity to visually and aurally sense the world around them, necessary to drive the positional tracking that makes rendering 3D spatial content possible. This requisite sensing also opens the door for more advanced AR-driven activities, such as augmented perception, volumetric capture and biometric identification - activities with the potential to expose bystanders to significant privacy risks. Existing Privacy-Enhancing Technologies (PETs) often safeguard against these risks at a low level e.g., instituting camera access controls. However, we argue that such PETs are incompatible with the need for always-on sensing given AR headsets' intended everyday use. Through an online survey (N=102), we examine bystanders' awareness of, and concerns regarding, potentially privacy infringing AR activities; the extent to which bystanders' consent should be sought; and the level of granularity of information necessary to provide awareness of AR activities to bystanders. Our findings suggest that PETs should take into account the AR activity type, and relationship to bystanders, selectively facilitating awareness and consent. In this way, we can ensure bystanders feel their privacy is respected by everyday AR headsets, and avoid unnecessary rejection of these powerful devices by society.","LowLevel":"biometrics;data privacy;rendering","MidLevel":"graphics;human-computer interaction;policy","HighLevel":"end users and user experience;technology;business","Venue":"Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. (USA)","Top20AbsAndTags":[113,27,102,42,226,163,104,49,232,219,57,470,48,124,62,65,110,123,377,385],"Top20Abs":[113,27,102,49,470,42,226,163,124,232,104,48,62,123,65,57,110,54,180,107],"Top20Tags":[377,104,410,327,385,129,331,219,167,130,239,126,263,199,73,32,339,210,355,259],"Citation":"O\u2019Hagan, J., Saeghe, P., Gugenheimer, J., Medeiros, D., Marky, K., Khamis, M., & McGill, M. (2022). Privacy-Enhancing Technology and Everyday Augmented Reality. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(4), 1\u201335. https:\/\/doi.org\/10.1145\/3569501\n"},{"Title":"Perceptual Modifications in Augmented Reality: A Short Survey","DOI":"10.1145\/3544793.3561318","Publication year":2022,"Abstract":"Views on Augmented Reality (AR) and its technology have changed. Rather than being a research area on its own, with a focus on computer vision and computer-generated imagery, we now have to deal with the integration of AR technology with ubiquitous computing where we have sensors, actuators, and processing capabilities beyond what is possible with a single AR device. The plethora of AR applications that follow from this point of view has not yet fully been investigated. In this paper, we review how in reported AR research this issue is dealt with. We survey the attempts from the AR community to integrate perception technology with AR. These attempts should be added to and integrated with augmented human and multisensorial research that nowadays is being done in the context of ubiquitous computing (Internet of Things, Ambient Intelligence, smart environments).","LowLevel":"ambient intelligence;computer vision;internet of things;ubiquitous computing","MidLevel":"internet of things;computer vision;human-computer interaction;networks","HighLevel":"end users and user experience;technology","Venue":"UbiComp\/ISWC'22 Adjunct: Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers","Top20AbsAndTags":[220,159,57,123,62,87,38,49,359,175,10,53,139,41,17,190,111,286,122,102],"Top20Abs":[159,49,87,41,62,139,57,17,111,2,281,10,102,151,45,135,53,190,122,220],"Top20Tags":[220,123,129,286,38,258,175,121,298,271,409,359,378,266,289,31,115,395,280,452],"Citation":"Nijholt, A. (2022). Perceptual Modifications in Augmented Reality: A Short Survey. Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing. https:\/\/doi.org\/10.1145\/3544793.3561318\n"},{"Title":"AR HUD Interface Optimization Model for Balancing Driver's Visual Sensitivity and Fatigue","DOI":"10.1016\/j.procs.2022.11.345","Publication year":2022,"Abstract":"With the wide application of intelligent driving assistance system, the augmented reality head-up display (AR HUD) interface plays an increasingly important role in drivers' real-time perception of traffic environment. As the AR HUD is vulnerable to be affected by changed ambient light, it weakens the legibility of the virtual interface and driver's visual comfort to a certain extent. The content color, as one of interface visual design elements, directly affects the performance of driver, which include the visual sensitivity and fatigue and further influences the vehicle safety. The selection between luminance contrast (affects sensitive and number of applicable environments) and driver performance (affects fatigue) are contradictory issues. In order to solve this contradictory problem in the design of AR HUD interface, and balance the cognitive load and situation awareness of driver, an extended analysis and optimization method of interface element was proposed. This method quantifies the discrete interface elements and makes the opposition problem consistent. The content color selection models under two conditions are constructed, which describe the distance from the chosen color to the optimal interval. It has been proved to be effective in finding the best answer for driver performance in interface design. Other interface elements in AR HUD can also be further optimized based on this method. All rights reserved Elsevier.","LowLevel":"cognition;data visualization;driver information systems;head up displays;helmet mounted displays;human factors;optimization;road safety;user interfaces","MidLevel":"education;human factors;automotive;data;transportation;inspection, safety and quality;business performance metrics;wearables;developers;human-computer interaction;display technology","HighLevel":"displays;end users and user experience;use cases;business;industries;technology","Venue":"Procedia Comput. Sci. (Netherlands)","Top20AbsAndTags":[149,276,214,444,95,101,365,242,120,248,397,419,65,121,99,72,382,481,84,132],"Top20Abs":[149,276,214,444,95,419,248,120,365,397,99,242,72,101,121,65,60,123,132,170],"Top20Tags":[242,348,101,21,206,80,339,69,238,248,243,195,383,439,65,382,276,68,217,440],"Citation":"Dou, J., Xu, C., Chen, S., Xue, C., & Li, X. (2022). AR HUD Interface Optimization Model for Balancing Driver\u2019s Visual Sensitivity and Fatigue. Procedia Computer Science, 214, 1568\u20131580. https:\/\/doi.org\/10.1016\/j.procs.2022.11.345\n"},{"Title":"The Second Workshop on Multiple Input Modalities and Sensations for VR\/AR Interactions (MIMSVAI)","DOI":"10.1145\/3544793.3560372","Publication year":2022,"Abstract":"With the advance of VR\/AR technology, more and more VR\/AR applications are emerging and have been popular among new users. Interacting with virtual reality and augmented reality technologies will require the development of alternative input modalities as well as coherent integration of multiple realistic sensations to increase the level of perceived realism. These developments will create a more immersive VR\/AR experience. However, a lack of robust and intuitive interaction interfaces and realistic sensations hinders users' experience for achieving a fascinating acceptance in various application areas of VR\/AR interactions. This workshop discusses the challenges and applications of designing a higher coherence between different input modalities and sensations to offer more engaging VR\/AR experiences, which can create opportunities for the researchers from both UbiComp and VR\/AR fields to jointly discuss and brainstorm alternative input modalities and sensations for VR\/AR interactions.","LowLevel":"other","MidLevel":"other","HighLevel":"other","Venue":"UbiComp\/ISWC'22 Adjunct: Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers","Top20AbsAndTags":[210,60,234,165,216,64,175,316,142,27,383,79,152,208,338,53,410,44,306,62],"Top20Abs":[234,60,165,210,175,316,142,348,64,27,216,383,152,419,338,410,306,79,62,53],"Top20Tags":[208,458,187,7,267,56,462,28,239,355,87,461,345,2,193,428,226,257,374,300],"Citation":"You, C.-W., Chen, Y.-C., Tsai, H.-R., & Chen, C.-Y. (2022). The Second Workshop on Multiple Input Modalities and Sensations for VR\/AR Interactions (MIMSVAI). Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing. https:\/\/doi.org\/10.1145\/3544793.3560372\n"},{"Title":"Research on Intelligent Patrol Inspection of Power Line based on Hybrid reality Technology","DOI":"10.1109\/MLBDBI58171.2022.00045","Publication year":2022,"Abstract":"In order to solve the problem of low image resolution and detection accuracy of power line inspection system in complex environment, an intelligent inspection system of power line based on MR technology is proposed. Based on Mixed Reality (MR) technology, the whole architecture of intelligent patrol system is designed, which includes operation interface, comprehensive analysis, guidance and supervision, etc., and the real-time image of the line is obtained by the combination of virtual and reality. Combined with the characteristics of human eyes, a power line detection method with linear edge is proposed to further improve the image processing ability of the system. The proposed method is tested based on the hybrid reality platform, and the experimental results show that the designed power line inspection system can accurately detect the obstacles in the line in a short time, and the detection error is small.","LowLevel":"image resolution;inspection;power cables","MidLevel":"graphics;networks;other;inspection, safety and quality","HighLevel":"technology;use cases;other","Venue":"2022 4th International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)","Top20AbsAndTags":[322,12,439,462,76,20,202,395,433,316,476,98,215,456,340,479,486,116,481,5],"Top20Abs":[322,12,20,462,76,439,202,395,433,467,479,215,486,411,98,456,116,476,419,461],"Top20Tags":[322,396,68,418,130,265,126,192,206,374,148,255,370,340,72,98,54,300,101,482],"Citation":"Jin, H., Gang, Y., Deng, X., Zhao, J., & Li, D. (2022). Research on Intelligent Patrol Inspection of Power Line based on Hybrid reality Technology. 2022 4th International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI). https:\/\/doi.org\/10.1109\/mlbdbi58171.2022.00045\n"},{"Title":"Innovative Cultural Experience (ICE), an Augmented Reality system for promoting cultural heritage","DOI":"10.1145\/3575879.3576001","Publication year":2022,"Abstract":"Innovative Cultural Experience (ICE) is an Augmented Reality (AR) system for promoting cultural heritage. ICE combines cutting-edge technologies such as an interactive transparent screen, AR, motion sensors and multimedia material in order to provide a unique personal or mass-touring experience, utilizing information based on material and intangible cultural heritage, through narrative scenarios. Part of the ICE system is an interactive transparent box in which an exhibit can be placed. When a user\/visitor approaches the exhibit, multimedia information is displayed on the transparent screen of the box, creating an interactive AR experience for the user. Users can interact with the content which can be text, images, videos, 360 images, 360 videos, 3D models or even play games based on the exhibit that is in front of them. By combining the real exhibit with digital information displayed on top, an interactive AR experience is created. Additionally, users can provide feedback by recording and uploading text, images, and videos to the ICE system. ICE is cognitively neutral (domain independent) technology, which makes it useful for a variety of thematic items (from museum exhibits to folk customs, local recipes, etc.) and it can be used also in education, commercial and in the tourist sectors. This paper presents the architecture of the ICE system, and the technologies used for building it. Initial internal evaluation results show that the system is easy to use, and users tend to stay longer in front of the exhibit, interacting with it, thus collecting more information about it.","LowLevel":"history;museums;travel industry","MidLevel":"cultural heritage;liberal arts;transportation","HighLevel":"industries","Venue":"PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics","Top20AbsAndTags":[153,31,105,6,16,302,89,287,267,229,236,166,218,275,10,358,292,139,45,178],"Top20Abs":[153,31,6,287,105,89,16,267,302,45,75,166,178,275,244,10,419,229,358,218],"Top20Tags":[302,16,229,105,6,218,96,392,236,90,23,77,64,153,197,72,50,204,10,405],"Citation":"Kazanidis, I., Terzopoulos, G., Tsinakos, A., Georgiou, D., Karampatzakis, D., Georgiou, D., & Karampatzakis, D. (2022). Innovative Cultural Experience (ICE), an Augmented Reality system for promoting cultural heritage. Proceedings of the 26th Pan-Hellenic Conference on Informatics. https:\/\/doi.org\/10.1145\/3575879.3576001\n"},{"Title":"Through an AR Lens: Augmented Reality Magnification through Feature Detection and Matching","DOI":"10.1145\/3560905.3568067","Publication year":2022,"Abstract":"Sensing and Augmented Reality (AR) can benefit a wide range of applications that involve the use of magnifying lenses. Recent developments in AR magnification provide a direct overlay of the magnified scenes in AR. However, instrumentation tasks that require high precision and visual acuity need to selectively magnify a region of interest while maintaining the visual perception of the rest of the environment. In this demo, we presentAR-Magnifier, an AR magnification system through feature detection and matching. We propose a general framework based on an edge-computing architecture that can be applied to various types of instrumentation tasks. A pipeline is developed for detecting feature points and computing the homography matching to identify the magnified region of an object. We showcase how selective magnification in AR through sensing can assist the user in complex instrumentation tasks by providing visualization-based guidance.","LowLevel":"feature extraction;lenses;visual perception","MidLevel":"computer vision;chemical;input;display technology","HighLevel":"industries;technology;displays","Venue":"SenSys '22: Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems","Top20AbsAndTags":[71,285,318,98,451,132,151,111,49,76,331,110,139,437,7,341,189,258,123,195],"Top20Abs":[71,285,318,151,451,98,49,331,437,111,110,341,139,7,189,76,132,123,316,195],"Top20Tags":[71,263,318,98,343,387,260,259,381,76,99,132,234,422,116,379,370,415,69,81],"Citation":"Eom, S., Hadziahmetovic, M., Pajic, M., & Gorlatova, M. (2022). Through an AR Lens. Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems. https:\/\/doi.org\/10.1145\/3560905.3568067\n"},{"Title":"Strategic Information System Planning in the Industry 4.0 Era: A Systematic Literature Review","DOI":"10.1109\/ICCIT55355.2022.10119002","Publication year":2022,"Abstract":"Information systems and technology have an important role in building the Company's competitive advantage in facing the Industrial Era 4.0. Strategic Information System Planning (SISP) helps companies formulate reliable information systems that are aligned with business strategies. The academic world should contribute in this regard, but currently, the amount of research related to SISP is very limited. This research is a systematic literature review, which aims to find out more about SISP research trends, research motivations and backgrounds, industries that have been handled, and the methods used in developing SISP. This research is expected to be a trigger for the emergence of new research to support the successful development of SISP in companies in various industries. The results of this research show that several topics are widely researched and need to be improved in the future, namely the alignment of IS strategies with business strategies, the development of specific SISPs in various industries, and the reliability of IS strategies by considering the application of the latest information technology, such as blockchain, artificial intelligence, big data, augmented reality, Internet of Things, and cloud computing. In addition, it is also necessary to develop a SISP framework that is more accommodating to changes in the business environment that is increasingly fast, agile, and collaborative, which includes aspects of business architecture, data\/information architecture, technology architecture, information system security, governance, and human resources.","LowLevel":"artificial intelligence;big data;blockchains;business data processing;cloud computing;information systems;internet of things;production engineering computing;strategic planning","MidLevel":"education;artificial intelligence;internet of things;engineering;data;security;business planning and management;business performance metrics;liberal arts;developers;manufacturing;networks;other","HighLevel":"industries;technology;business;other","Venue":"2022 IEEE Creative Communication and Innovative Technology (ICCIT)","Top20AbsAndTags":[315,359,4,159,361,368,226,175,429,447,166,448,181,5,179,352,129,110,244,8],"Top20Abs":[315,4,368,361,159,181,244,179,447,25,448,175,281,96,352,139,5,2,166,252],"Top20Tags":[378,359,129,271,429,315,298,308,159,226,377,273,286,175,258,123,38,4,280,395],"Citation":"Mahendra, I., Ramadhan, A., Trisetyarso, A., Abdurachman, E., & Zarlis, M. (2022). Strategic Information System Planning in the Industry 4.0 Era: A Systematic Literature Review. 2022 IEEE Creative Communication and Innovative Technology (ICCIT). https:\/\/doi.org\/10.1109\/iccit55355.2022.10119002\n"},{"Title":"Augmented Reality as an Educational Tool and Assistive Technology for People with Intellectual Disabilities: Scoping Review","DOI":"10.1145\/3572921.3576218","Publication year":2022,"Abstract":"Many people with intellectual disability seek opportunities to develop their self-determination, personal development, interpersonal relationships, and well-being. Emerging technologies, such as augmented reality (AR), present an opportunity for new approaches to supporting inclusion and personal development. AR interactively integrates digital information and the real world, and has been increasingly used as a tool for intervention, education, or as an assistive technology. However, there has been little attention to how AR applications can engage and support people with intellectual disability. This paper presents a scoping review of seventeen studies in the past decade and discusses the benefits highlighted by the authors. Most studies conducted with people with intellectual disability have shown that AR performs a positive and effective role as an instructional or assistive tool in the areas of education, daily living, or health. AR provides an opportunity to help people acquire new knowledge (e.g., foreign language and numeracy), form new habits (e.g., teeth brushing technique), and experience environments inclusively (e.g., arrive to the classroom with the support of an AR navigation app on the smartphone). Hence, this paper provides a comprehensive overview of known useful aspects of AR technology in the support and inclusion of people with intellectual disabilities.","LowLevel":"computer aided instruction;handicapped aids;mobile computing;smartphones","MidLevel":"telecommunication;liberal arts;medical;training","HighLevel":"industries;use cases","Venue":"OzCHI '22: Proceedings of the 34th Australian Conference on Human-Computer Interaction","Top20AbsAndTags":[42,27,222,139,219,179,216,49,17,351,51,184,38,125,103,214,3,136,87,62],"Top20Abs":[42,49,222,27,179,139,219,216,38,159,184,17,125,136,411,135,87,62,351,5],"Top20Tags":[82,376,103,164,204,51,351,222,410,77,147,129,27,56,367,115,89,214,9,339],"Citation":"Zhu, Y., Roomkham, S., & Sitbon, L. (2022). Augmented Reality as an Educational Tool and Assistive Technology for People with Intellectual Disabilities: Scoping Review. Proceedings of the 34th Australian Conference on Human-Computer Interaction. https:\/\/doi.org\/10.1145\/3572921.3576218\n"},{"Title":"Learning Media Innovation about Keris Cultural Heritage through Augmented Reality","DOI":"10.1145\/3568231.3568275","Publication year":2022,"Abstract":"Keris is a special weapon that is a cultural heritage in Indonesia which is of high value. Until now, the introduction and learning of culture about keris are still limited to 2D from books, or in exhibitions or museums. Learning about keris culture certainly needs to be supported by technological developments. The process of delivering information is currently experiencing rapid development, the latest technology used in delivering information is Augmented Reality (AR) technology. Users can visualize historical objects or objects in three-dimensional (3D) form via Android smartphones. The method used in this research is the ADDIE model, starting from Analysis, Design, Development, Implementation, and Evaluation. The results reveal that distance, marker angle, and device specifications all have a sizeable impact on the camera's marker readout. The shortest distance between the marker and the camera where 3D objects can be displayed is 20 cm, while the farthest distance at which 3D items cannot be displayed is 100 cm. And the marker reading angle is only about 0&#176; until 45&#176;. The application of Augmented Reality technology is interactive and real-time, so Augmented Reality can be implemented in various fields and become a learning medium to introduce historical objects that are cultural heritage.","LowLevel":"cameras;data visualization;graphical user interfaces;history;mobile computing;museums;smartphones","MidLevel":"cultural heritage;telecommunication;data;graphics;liberal arts;input;human-computer interaction","HighLevel":"industries;technology;end users and user experience","Venue":"SIET22: 7th International Conference on Sustainable Information Engineering and Technology 2022","Top20AbsAndTags":[77,97,114,48,16,116,236,207,218,109,400,39,275,41,316,284,6,155,302,392],"Top20Abs":[77,97,114,116,207,16,48,400,284,275,218,163,109,39,236,155,6,316,244,46],"Top20Tags":[302,77,229,16,82,55,109,93,410,27,424,339,309,42,22,129,376,221,327,421],"Citation":"Marcellino, L., Harischandra, I. B. R., Buana, I. M. K. W., Maulana, F. I., & Ramadhani, M. (2022). Learning Media Innovation about Keris Cultural Heritage through Augmented Reality. 7th International Conference on Sustainable Information Engineering and Technology 2022. https:\/\/doi.org\/10.1145\/3568231.3568275\n"},{"Title":"The Role of Artificial Intelligence in the Education Process of Political Science Field","DOI":"10.1109\/ICETSIS55481.2022.9888844","Publication year":2022,"Abstract":"Over the last two decades, the field of artificial intelligence in education has undergone significant changes. So, Recent years have witnessed very remarkable developments in the technical and technological fields. Political Science can help people understand themselves and their relationships with the governors and government. This valuable field helps people make better decisions and improve their lives. It is also helping address some of the world's most pressing problems, such as global poverty and national security. So, there is no doubt that political science as a branch of social sciences differs from practical fields such as Physics, Mathematics, and Biological Sciences, and therefore the use of artificial intelligence applications in the field will be different in some applications due to the characteristics of the field. The research problem revolved around the following question what is the role of artificial intelligence in improving the educational process in the political science field? The paper aims to highlight the AI Applications used in Educational Process, the difference between Virtual Reality and Augmented Reality in learning and teaching, as well as the applications of VR in teaching political science. The paper concluded that AI applications in political science allow students to experience the world around them in a completely different way, and it can be done anytime and anywhere. And students can learn how to express themselves fully and confidently. It is the technology that has the potential to transform the way students learn.","LowLevel":"artificial intelligence;computer aided instruction;politics;teaching","MidLevel":"education;artificial intelligence;training;policy;liberal arts","HighLevel":"industries;technology;business;use cases","Venue":"2022 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS)","Top20AbsAndTags":[388,414,241,47,226,436,82,342,87,115,320,252,188,39,306,56,363,46,163,207],"Top20Abs":[388,241,226,47,252,188,82,115,306,414,436,207,87,46,342,163,56,316,320,88],"Top20Tags":[363,354,308,391,33,17,58,82,115,414,87,222,246,204,3,164,136,51,232,253],"Citation":"Khalifa, M. (2022). The Role of Artificial Intelligence in the Education Process of Political Science Field. 2022 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS). https:\/\/doi.org\/10.1109\/icetsis55481.2022.9888844\n"},{"Title":"SOCRAR: Semantic OCR through Augmented Reality","DOI":"10.1145\/3567445.3567453","Publication year":2022,"Abstract":"To enable people to interact more efficiently with virtual and physical services in their surroundings, it would be beneficial if information could more fluently be passed across digital and non-digital spaces. To this end, we propose to combine semantic technologies with Optical Character Recognition on an Augmented Reality (AR) interface to enable the semantic integration of (written) information located in our everyday environments with Internet of Things devices. We hence present SOCRAR, a system that is able to detect written information from a user's physical environment while contextualizing this data through a semantic backend. The SOCRAR system enables in-band semantic translation on an AR interface, permits semantic filtering and selection of appropriate device interfaces, and provides cognitive offloading by enabling users to store information for later use. We demonstrate the feasibility of SOCRAR through the implementation of three concrete scenarios.","LowLevel":"cognition;data visualization;internet of things;optical character recognition","MidLevel":"internet of things;human factors;data;input;networks","HighLevel":"end users and user experience;technology","Venue":"IoT 2022: Proceedings of the 12th International Conference on the Internet of Things","Top20AbsAndTags":[318,365,27,440,276,244,419,175,288,38,123,110,447,129,21,220,289,267,219,226],"Top20Abs":[318,365,440,27,276,244,419,288,447,21,412,468,226,129,158,267,0,326,465,281],"Top20Tags":[129,123,175,220,38,110,258,298,286,271,359,378,266,289,31,395,339,376,410,280],"Citation":"Strecker, J., Garc\u00eda, K., Bekta\u015f, K., Mayer, S., & Ramanathan, G. (2022). SOCRAR: Semantic OCR through Augmented Reality. Proceedings of the 12th International Conference on the Internet of Things. https:\/\/doi.org\/10.1145\/3567445.3567453\n"},{"Title":"Smart Teaching Materials with Real-Time Augmented Reality Support for Introductory Physics Education","DOI":"10.1145\/3544793.3560322","Publication year":2022,"Abstract":"In this demonstration, we present a system design that helps to reduce the split attention effect in multimedia learning by providing an interactive environment with augmented reality support for elementary physics education. The system consists of three main components: smart boxes, smart cables, and visualization app. Each smart box contains an input to plug different electrical components (bulb, battery, and switches) and two sockets to interconnect the boxes with each other using smart cables. These boxes are equipped with various sensor modalities that provide information related to connected cable identifications and the physical status of the boxes. This information is shared through a Bluetooth Low Energy interface with the connected visualization device. Visualization devices range between handheld tablets with augmented reality capabilities and headwear smart glasses. These devices are used to run the supportive app. The app is responsible to track the smart boxes using markers and provide a 3D augmented visualization of information coming from them. This system targets introductory physics education, in addition holds the potential to provide assistance for more advanced electrical circuits in secondary or higher physics education.","LowLevel":"bluetooth;computer aided instruction;mobile computing;physics education;teaching","MidLevel":"education;training;engineering;telecommunication;networks","HighLevel":"industries;technology;use cases","Venue":"UbiComp\/ISWC'22 Adjunct: Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers","Top20AbsAndTags":[287,376,310,306,262,161,224,109,178,134,424,80,4,332,182,228,136,52,171,129],"Top20Abs":[287,376,310,224,306,109,161,80,178,424,262,38,281,4,352,129,228,171,332,182],"Top20Tags":[52,82,376,93,306,164,56,342,222,3,262,51,410,33,39,136,103,320,115,128],"Citation":"Javaheri, H., Lauer, F., Lauer, L., Altmeyer, K., Br\u00fcnken, R., Peschel, M., Wehn, N., & Lukowicz, P. (2022). Smart Teaching Materials with Real-Time Augmented Reality Support for Introductory Physics Education. Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing. https:\/\/doi.org\/10.1145\/3544793.3560322\n"},{"Title":"A unified framework for automated registration of point clouds, mesh surfaces and 3D models by using planar surfaces","DOI":"10.1111\/phor.12428","Publication year":2022,"Abstract":"Registration of 3D spatial data and models is a fundamental task in applications such as mapping, positioning and virtual\/augmented reality. Most of the existing 3D registration methods such as iterative closest point (ICP) and recent learning-based methods are dedicated to point cloud registration, and rely heavily on point-wise correspondences, which limits their ability to address registration problems across different data types. Since man-made objects and buildings usually contain many planar surfaces, it is possible to use the planes for accurate registration of different data and models. In this paper, a unified registration framework is proposed consisting of a plane extraction module, which can extract planes from various forms of spatial data such as point clouds or surface-based 3D models, and a registration module, which performs automatic registration based on the extracted planes. Tests show that the proposed method can handle small-overlap registration across all these data types with high success rates. The result of point cloud registration also indicates that the method achieves better accuracy as compared to ICP.&lt;i&gt;The Photogrammetric Record&lt;\/i&gt;&#169; 2022 The Remote Sensing and Photogrammetry Society and John Wiley &amp; Sons Ltd.","LowLevel":"image registration;iterative methods;photogrammetry;remote sensing;solid modelling","MidLevel":"manufacturing;artificial intelligence;computer vision;sensors","HighLevel":"industries;technology","Venue":"Photogramm. Rec. (USA)","Top20AbsAndTags":[98,451,350,467,24,340,81,318,186,433,33,76,92,471,488,5,40,390,486,146],"Top20Abs":[98,467,451,350,81,318,340,433,33,486,92,24,488,390,76,40,186,146,471,378],"Top20Tags":[172,277,132,76,186,81,370,301,340,350,10,220,326,456,356,234,72,397,439,425],"Citation":"Zhao, Y., Zhao, H., Radanovic, M., & Khoshelham, K. (2022). A unified framework for automated registration of point clouds, mesh surfaces and <scp>3D<\/scp> models by using planar surfaces. The Photogrammetric Record, 37(180), 366\u2013384. Portico. https:\/\/doi.org\/10.1111\/phor.12428\n"},{"Title":"Aaron: Compile-Time Kernel Adaptation for Multi-DNN Inference Acceleration on Edge GPU","DOI":"10.1145\/3560905.3568050","Publication year":2022,"Abstract":"AI applications powered by deep learning are increasingly running on edge devices. Meanwhile, many real-world IoT applications demand multiple real-time tasks to run on the same device, for example, to achieve both object tracking and image segmentation simultaneously on an augmented reality glass. However, the current solutions can not yet support such multi-tenant real-time DNN inference on edge devices. Techniques such as on-device model compression trade inference accuracy for speed, while traditional DNN compilers mainly focus on single-tenant DNN model optimization. To fill this gap, we propose Aaron, which leverages DNN compiling techniques to accelerate multi-DNN inference on edge GPU based on compile-time kernel adaptation with no accuracy loss. Aaron integrates both DNN graph and kernel optimization to maximize on-device parallelism and minimize contention brought by concurrent inference.","LowLevel":"cloud computing;deep learning (artificial intelligence);graphics processing units;image segmentation;inference mechanisms;internet of things;object tracking;optimization","MidLevel":"internet of things;artificial intelligence;computer vision;data;medical;liberal arts;business performance metrics;semiconductors;networks;other","HighLevel":"industries;technology;business;other","Venue":"SenSys '22: Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems","Top20AbsAndTags":[271,325,95,129,194,220,341,264,425,20,116,123,382,111,486,151,346,483,290,212],"Top20Abs":[271,325,129,95,194,341,20,264,382,290,111,220,116,151,228,346,425,486,123,335],"Top20Tags":[271,95,425,220,289,379,160,129,378,178,418,298,8,415,123,359,115,286,258,110],"Citation":"Zhao, Z., Ling, N., Guan, N., & Xing, G. (2022). Aaron. Proceedings of the Twentieth ACM Conference on Embedded Networked Sensor Systems. https:\/\/doi.org\/10.1145\/3560905.3568050\n"},{"Title":"Design of Augmented Reality based Environment to Promote Spatial Imagination for Mathematics Education in Elementary School","DOI":"10.1145\/3544793.3560380","Publication year":2022,"Abstract":"This paper investigates the use of augmented reality (AR) technology to deliver augmented lectures to support students in acquiring the curricular competency of using spatial imagination in mathematics education. As a very important stage in education to develop spatial abilities this paper focuses on elementary school children. Due to the challenges of working with children in experimental studies, this education level has received comparatively little attention in terms of support through ubiquitous technology. A theory-driven design approach was adopted in the development of an AR-based learning environment to visualize various virtual 3D cube buildings aligned with real blueprints. The designed environment was evaluated by a group of 1st and 2nd grade students in terms of system usability. The results showed that the theory-driven design was successful with a score of 86.56 on the System Usability test. In our future work, we aimed to assess the effectiveness of the proposed AR environment in terms of learning gains by performing more task-focused studies before and after experiment and comparing the results with the traditional teaching methods.","LowLevel":"computer aided instruction;data visualization;educational courses;educational institutions;mathematics computing;solid modelling;teaching","MidLevel":"education;training;engineering;data;manufacturing","HighLevel":"industries;technology;use cases","Venue":"UbiComp\/ISWC'22 Adjunct: Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers","Top20AbsAndTags":[320,185,342,369,171,163,88,115,134,414,114,180,87,363,39,287,57,306,17,82],"Top20Abs":[369,171,185,320,88,163,342,180,115,306,363,83,39,287,114,82,57,46,284,87],"Top20Tags":[246,39,320,134,363,33,185,82,232,56,89,87,52,241,3,372,275,88,17,262],"Citation":"Javaheri, H., Lehmann, J., Altmeyer, K., M\u00fcller, L. M., Br\u00fcnken, R., & Lukowicz, P. (2022). Design of Augmented Reality based Environment to Promote Spatial Imagination for Mathematics Education in Elementary School. Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing. https:\/\/doi.org\/10.1145\/3544793.3560380\n"},{"Title":"Individualization Of Head Related Transfer Function Based On PCA And RBF Network","DOI":"10.1145\/3579895.3579912","Publication year":2022,"Abstract":"Head-Related Transfer Function (HRTF) describes the acoustic reflection and diffraction effect caused by the influence of the human body (head, torso, etc.) in the transmission of sound waves to the human ear. In Virtual Reality(VR) \/ Augmented Reality(AR), HRTF is often used to generate virtual 3D audio due to its ability to recreate perceptions of natural sound scenes realistically. However, HRTF varies from person to person due to the differences in anthropometric features. Using non-individualized HRTF to produce 3D sounds may lead to hearing localization bias in users. Therefore, how to obtain individualized HRTF is a hot topic in the field of VR \/ AR. This paper proposes an effective method to establish the relationship between anthropometric features and HRTF. At first, a method based on multimodal principal component analysis is proposed for the representation of HRTF models with low dimensions. Then a nonlinear mapping representation model between the low-dimensional features of HRTF and anthropometric features is established using Radial Basis Function Neural Network (RBFNN). Objective experiments show that the proposed HRTF Individualization method can reduce the spectral distortion as low as 4.48 dB. The subjective listening experiments based on the principal sagittal plane show that the individualized HRTF obtained using this method can effectively improve the accuracy of subjective listening (about 33%).","LowLevel":"acoustic signal processing;anthropometry;audio signal processing;hearing;principal component analysis;radial basis function networks;signal representation;transfer functions","MidLevel":"artificial intelligence;human factors;engineering;audio;data;medical;developers;sensors;other","HighLevel":"end users and user experience;technology;industries;other","Venue":"ICNCC '22: Proceedings of the 2022 11th International Conference on Networks, Communication and Computing","Top20AbsAndTags":[98,386,318,283,76,451,237,340,217,341,294,165,402,358,32,438,208,316,81,26],"Top20Abs":[98,318,341,283,76,386,402,237,451,438,316,340,358,208,87,165,294,387,26,20],"Top20Tags":[386,217,230,428,165,18,160,250,337,86,84,95,253,415,255,214,417,451,416,108],"Citation":"Chen, W., Zhang, H., Yu, J., & Luo, F. (2022). Individualization Of Head Related Transfer Function Based On PCA And RBF Network. Proceedings of the 2022 11th International Conference on Networks, Communication and Computing. https:\/\/doi.org\/10.1145\/3579895.3579912\n"},{"Title":"IoT-Enabled Environment Illuminance Optimization for Augmented Reality","DOI":"10.1145\/3544793.3560357","Publication year":2022,"Abstract":"In order to provide high quality augmented reality (AR) experiences, environmental conditions such as light level must be conducive to a high level of virtual content stability, accurate eye tracking and good virtual content visibility. This is challenging due to the dynamic nature of environmental conditions and AR application requirements. In this poster we present the first automatic environment optimization system for AR which adapts to both environment lighting and texture. First we conduct experiments that demonstrate the effect of light level on virtual object stability and eye tracking. We then detail our edge computing system architecture which uses IoT devices to sense and adjust environment conditions, and lays the foundation for future environment-aware AR applications.","LowLevel":"edge computing;gaze tracking;internet of things","MidLevel":"internet of things;computer vision;networks;input","HighLevel":"technology","Venue":"UbiComp\/ISWC'22 Adjunct: Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2022 ACM International Symposium on Wearable Computers","Top20AbsAndTags":[129,111,175,365,277,110,73,67,220,266,259,440,142,271,316,395,65,280,194,435],"Top20Abs":[365,277,67,73,111,440,142,129,41,65,316,151,226,259,415,194,435,155,136,81],"Top20Tags":[129,220,258,175,110,31,38,266,121,298,286,271,378,111,359,289,395,280,335,472],"Citation":"Scargill, T., Dabrowski, A., Xu, A., & Gorlatova, M. (2022). IoT-Enabled Environment Illuminance Optimization for Augmented Reality. Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing. https:\/\/doi.org\/10.1145\/3544793.3560357\n"},{"Title":"Effects of minimum content in cultural informatics","DOI":"10.1145\/3575879.3575996","Publication year":2022,"Abstract":"Over the last years our research group has been working with a novel concept for HCI in Cultural Informatics, that of minimum content. Responding to the cognitive overload and the museum fatigue that is well documented in the literature, the concept of minimum meaningful content was introduced and studied, with regards to the user experience, concerning possible learning benefits and overall satisfaction. The present work presents findings from different past efforts that explored the effects of minimum content on cultural heritage visitors. Minimum content was tested with visitors of different sites and lab experiment participants, focusing on augmented reality, games, and photography. The results reveal the potential of minimum content in cultural visits. The implications for HCI and future works are also discussed.","LowLevel":"history;human computer interaction;museums","MidLevel":"cultural heritage;liberal arts;human-computer interaction","HighLevel":"industries;end users and user experience","Venue":"PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics","Top20AbsAndTags":[6,365,31,236,16,267,109,229,105,275,164,132,145,218,79,263,153,195,237,309],"Top20Abs":[6,31,365,16,267,275,236,164,109,132,229,145,263,218,79,188,237,309,105,195],"Top20Tags":[16,105,236,109,229,59,405,339,77,204,6,319,153,197,314,66,72,218,58,142],"Citation":"Antoniou, A., Theodoropoulos, A., Rompa, J., Giannakopoulou, F., Lepouras, G., & Triantafyllou, I. (2022). Effects of minimum content in cultural informatics. Proceedings of the 26th Pan-Hellenic Conference on Informatics. https:\/\/doi.org\/10.1145\/3575879.3575996\n"},{"Title":"Application of analytical hierarchy process model in selecting an appropriate digital marketing communication technology: A case study of a textile company","DOI":"10.1145\/3584202.3584241","Publication year":2022,"Abstract":"Differently from traditional marketing communication, digital marketing communication is characterized to facilitate efficiently communication between businesses and consumers through different digital technologies such as social media platforms, artificial intelligence, virtual reality, and augmented reality. However, the successful selection of an appropriate digital marketing communication technology has become crucial for the businesses that aims to promote their products efficiently. In the case of a textile company in Uzbekistan, our work aims to show how to select an appropriate digital marketing communication tool compatible with company marketing strategy and target market by using analytical hierarchy process model, so as to support the marketing communication-based decision making of businesses. In this regard, selection criteria are identified through extensive review of relevant literature and stakeholder interview (including questionnaire) and then importance of each criterion is measured based on the analytical hierarchy process model to develop hierarchy of those criteria. The ranking of five alternative marketing communication technologies is identified based on the success scores derived from consumer-based survey and selection criteria weights. The results show that social media platforms such as telegram are more preferable to be implemented in digital marketing communication activities of the company. Through the analytical hierarchy process model, we uncovered important recommendations for businesses in retail industry about enhancing their marketing communication strategies.","LowLevel":"analytic hierarchy process;artificial intelligence;decision making;marketing data processing;social networking","MidLevel":"artificial intelligence;human factors;collaboration;data;liberal arts;sales and marketing","HighLevel":"end users and user experience;use cases;business;industries;technology","Venue":"ICFNDS '22: Proceedings of the 6th International Conference on Future Networks &amp; Distributed Systems","Top20AbsAndTags":[102,1,40,219,289,96,25,203,253,202,286,159,447,32,412,209,44,315,373,160],"Top20Abs":[102,1,40,289,96,219,253,25,159,209,286,32,315,447,412,203,202,44,77,222],"Top20Tags":[160,193,13,308,219,377,164,102,40,339,333,352,251,363,166,249,373,254,18,213],"Citation":"Mukhsinov, B. T., & Ergashxodjayeva, S. D. (2022). Application of analytical hierarchy process model in selecting an appropriate digital marketing communication technology: A case study of a textile company. Proceedings of the 6th International Conference on Future Networks &amp; Distributed Systems. https:\/\/doi.org\/10.1145\/3584202.3584241\n"},{"Title":"Using Augmented Reality to Explore Gender and Power Dynamics in STEM Higher Education","DOI":"10.1145\/3572921.3576203","Publication year":2022,"Abstract":"We report on results from a pilot study that used Augmented Reality (AR) to explore gender and power dynamics in STEM Higher Education. The so called \"leaky pipeline\", affecting retention and recruitment of women in higher education fields of Science, Technology, Engineering and Mathematics (STEM) is a well-documented issue. We conducted a co-design workshop with six academics, asking to reflect on an AR storyboard prototype, discuss aspects of personal and professional identity, and imagine alternative endings for the storyboard. The storyboard depicted a hypothetical workplace conflict involving an episode of gender-based discrimination. Participants collaboratively imagined possible responses to the situation depicted using a combination of paper-prototyping and open discussions. The found themes were that 1) reflecting on personal identities can drive empathy and understanding for others; 2) co-creating the AR experiences was an engaging way of scaffolding these reflections, and 3) the co-created AR prototypes offered constructive and community-centred approaches to supporting diversity. We contribute a preliminary format for an identity-based AR co-creation workshop and opportunities for applying AR to support dialogue on gender and power dynamics.","LowLevel":"computer aided instruction;educational institutions;further education;gender issues;groupware;user interfaces","MidLevel":"education;training;collaboration;human-computer interaction;other","HighLevel":"industries;use cases;end users and user experience;other","Venue":"OzCHI '22: Proceedings of the 34th Australian Conference on Human-Computer Interaction","Top20AbsAndTags":[55,336,53,42,70,102,27,251,83,93,161,377,26,200,372,17,241,239,117,136],"Top20Abs":[336,55,42,102,27,251,53,377,93,161,83,200,78,49,70,391,135,117,184,62],"Top20Tags":[17,405,287,33,113,29,239,279,391,171,88,244,241,204,196,78,303,145,104,414],"Citation":null},{"Title":"Semantic-assisted Unified Network for Feature Point Extraction and Matching","DOI":"10.1145\/3574131.3574433","Publication year":2022,"Abstract":"Feature point matching between two images is an essential part of 3D reconstruction, augmented reality, panorama stitching, etc. The quality of the initial feature point matching stage greatly affects the overall performance of a system. We present a unified feature point extraction-matching method, making use of semantic segmentation results to constrain feature point matching. To integrate high-level semantic information into feature points efficiently, we propose a unified feature point extraction and matching network, called SP-Net, which can detect feature points and generate feature descriptors simultaneously and perform feature point matching with accurate outcomes. Compared with previous works, our method can extract multi-scale context of the image, including shallow information and high-level semantic information of the local area, which is more stable when handling complex conditions such as changing illumination or large viewpoint. In evaluating the feature-matching benchmark, our method shows superior performance over the state-of-art method. As further validation, we propose SP-Net++ as an extension for 3D reconstruction. The experimental results show that our neural network can obtain accurate feature point positioning and robust feature matching to recover more cameras and get a well-shaped point cloud. Our semantic-assisted method can improve the stability of feature points as well as specific applicability for complex scenes.","LowLevel":"feature extraction;image matching;image reconstruction;image segmentation","MidLevel":"construction;computer vision;chemical","HighLevel":"industries;technology","Venue":"VRCAI'22: Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry","Top20AbsAndTags":[98,451,132,341,438,76,107,471,340,350,473,321,121,48,99,435,92,358,409,488],"Top20Abs":[98,451,438,341,76,132,107,350,471,473,340,121,321,92,48,435,99,480,488,20],"Top20Tags":[98,132,370,379,107,71,76,387,73,381,99,225,255,130,178,234,81,116,280,340],"Citation":"Ji, D., You, W., Chen, Y., Wang, G., & Li, S. (2022). Semantic-assisted Unified Network for Feature Point Extraction and Matching. Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry. https:\/\/doi.org\/10.1145\/3574131.3574433\n"},{"Title":"A web-based campus navigation system with mobile augmented reality intervention","DOI":"10.1088\/1742-6596\/1997\/1\/012038","Publication year":2021,"Abstract":"In this era of information technology, the introduction of new and greatly improved technologies is becoming vital especially with the progressive development of computing power to fulfill current technological needs. Augmented Reality (AR) is a part of the emerging technologies that are gaining great attention and interest with new and better innovations being developed. AR is an immersive technology where it complements real-world objects together with computer-generated objects to give more details and meaning to the objects in the real world. AR has been successfully growing nowadays in many industries, mainly in gaming and entertainment, education and navigation. However, AR has not been standardized and to be widely used in navigation systems even though there are many applications developed for this purpose. This paper proposed a web-based AR-UUM Campus Navigation System using ARToolKit, that is accessible via mobile devices that uses AR to overlay the information as images of the searched location on campus, especially for indoor navigations for lecture halls, tutorial rooms, laboratories, and offices which rarely are covered by the normal maps. Based on the respondents' evaluation, they managed to successfully interact with the AR-UUM Campus Navigation System, however, some improvements are required in terms of the functions expected from the system. Nevertheless, the use of AR in a navigation system shows promising results and has the potential to be widely used due to high interest in augmented reality.","LowLevel":"educational institutions;mobile computing","MidLevel":"education;telecommunication","HighLevel":"industries","Venue":"J. Phys., Conf. Ser. (UK)","Top20AbsAndTags":[49,70,63,326,87,2,113,93,41,194,110,216,60,267,20,175,136,62,152,103],"Top20Abs":[49,70,63,326,87,2,41,113,110,216,93,152,60,194,151,159,62,45,267,136],"Top20Tags":[171,241,56,103,115,51,36,82,164,13,93,89,320,222,405,262,142,339,279,424],"Citation":"Nordin, N., Markom, M. A., Suhaimi, F. A., & Ishak, S. (2021). A Web-Based Campus Navigation System with Mobile Augmented Reality Intervention. Journal of Physics: Conference Series, 1997(1), 012038. https:\/\/doi.org\/10.1088\/1742-6596\/1997\/1\/012038\n"},{"Title":"GSV-NET: A Multi-Modal Deep Learning Network for 3D Point Cloud Classification","DOI":"10.3390\/app12010483","Publication year":2021,"Abstract":"Light Detection and Ranging (LiDAR), which applies light in the formation of a pulsed laser to estimate the distance between the LiDAR sensor and objects, is an effective remote sensing technology. Many applications use LiDAR including autonomous vehicles, robotics, and virtual and augmented reality (VR\/AR). The 3D point cloud classification is now a hot research topic with the evolution of LiDAR technology. This research aims to provide a high performance and compatible real-world data method for 3D point cloud classification. More specifically, we introduce a novel framework for 3D point cloud classification, namely, GSV-NET, which uses Gaussian Supervector and enhancing region representation. GSV-NET extracts and combines both global and regional features of the 3D point cloud to further enhance the information of the point cloud features for the 3D point cloud classification. Firstly, we input the Gaussian Supervector description into a 3D wide-inception convolution neural network (CNN) structure to define the global feature. Secondly, we convert the regions of the 3D point cloud into color representation and capture region features with a 2D wide-inception network. These extracted features are inputs of a 1D CNN architecture. We evaluate the proposed framework on the point cloud dataset: ModelNet and the LiDAR dataset: Sydney. The ModelNet dataset was developed by Princeton University (New Jersey, United States), while the Sydney dataset was created by the University of Sydney (Sydney, Australia). Based on our numerical results, our framework achieves more accuracy than the state-of-the-art approaches.","LowLevel":"convolutional neural nets;deep learning (artificial intelligence);feature extraction;gaussian processes;image classification;image representation;optical radar;radar computing;remote sensing by radar;stereo image processing","MidLevel":"artificial intelligence;optics;computer vision;data;medical;liberal arts;chemical;developers;sensors;geospatial;networks;other","HighLevel":"industries;technology;displays;other","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[318,98,350,120,92,321,340,5,76,379,158,107,298,427,471,480,409,143,341,95],"Top20Abs":[318,350,98,92,120,321,340,76,107,379,5,341,480,158,471,488,301,234,275,409],"Top20Tags":[379,255,387,172,160,18,381,143,173,81,358,428,95,425,415,115,76,71,308,271],"Citation":"Hoang, L., Lee, S.-H., Lee, E.-J., & Kwon, K.-R. (2022). GSV-NET: A Multi-Modal Deep Learning Network for 3D Point Cloud Classification. Applied Sciences, 12(1), 483. https:\/\/doi.org\/10.3390\/app12010483\n"},{"Title":"SAR.IoT: secured augmented reality for iot devices management","DOI":"10.3390\/s21186001","Publication year":2021,"Abstract":"Currently, solutions based on the Internet of Things (IoT) concept are increasingly being adopted in several fields, namely, industry, agriculture, and home automation. The costs associated with this type of equipment is reasonably small, as IoT devices usually do not have output peripherals to display information about their status (e.g., a screen or a printer), although they may have informative LEDs, which is sometimes insufficient. For most IoT devices, the price of a minimalist display, to output and display the device's running status (i.e., what the device is doing), might cost much more than the actual IoT device. Occasionally, it might become necessary to visualize the IoT device output, making it necessary to find solutions to show the hardware output information in real time, without requiring extra equipment, only what the administrator usually has with them. In order to solve the above, a technological solution that allows for the visualization of IoT device information in actual time, using augmented reality and a simple smartphone, was developed and analyzed. In addition, the system created integrates a security layer, at the level of AR, to secure the shown data from unwanted eyes. The results of the tests carried out allowed us to validate the operation of the solution when accessing the information of the IoT devices, verify the operation of the security layer in AR, analyze the interaction between smartphones, the platform, and the devices, and check which AR markers are most optimized for this use case. This work results in a secure augmented reality solution, which can be used with a simple smartphone, to monitor\/manage IoT devices in industrial, laboratory or research environments.","LowLevel":"internet of things;smartphones","MidLevel":"internet of things;liberal arts;networks;telecommunication","HighLevel":"industries;technology","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[220,266,175,447,395,123,280,38,31,18,248,228,240,424,289,315,271,121,137,224],"Top20Abs":[220,266,395,447,175,280,38,228,18,248,123,240,429,224,315,80,424,137,87,481],"Top20Tags":[123,175,220,31,286,289,38,378,359,271,258,110,121,298,93,339,27,266,221,82],"Citation":"Fuentes, D., Correia, L., Costa, N., Reis, A., Barroso, J., & Pereira, A. (2021). SAR.IoT: Secured Augmented Reality for IoT Devices Management. Sensors, 21(18), 6001. https:\/\/doi.org\/10.3390\/s21186001\n"},{"Title":"Multimodal Interaction Systems Based on Internet of Things and Augmented Reality: A Systematic Literature Review","DOI":"10.3390\/app11041738","Publication year":2021,"Abstract":"Technology developments have expanded the diversity of interaction modalities that can be used by an agent (either a human or machine) to interact with a computer system. This expansion has created the need for more natural and user-friendly interfaces in order to achieve effective user experience and usability. More than one modality can be provided to an agent for interaction with a system to accomplish this goal, which is referred to as a multimodal interaction (MI) system. The Internet of Things (IoT) and augmented reality (AR) are popular technologies that allow interaction systems to combine the real-world context of the agent and immersive AR content. However, although MI systems have been extensively studied, there are only several studies that reviewed MI systems that used IoT and AR. Therefore, this paper presents an in-depth review of studies that proposed various MI systems utilizing IoT and AR. A total of 23 studies were identified and analyzed through a rigorous systematic literature review protocol. The results of our analysis of MI system architectures, the relationship between system components, input\/output interaction modalities, and open research challenges are presented and discussed to summarize the findings and identify future research and development avenues for researchers and MI developers.","LowLevel":"human computer interaction;interactive systems;internet;internet of things;mobile computing;user interfaces","MidLevel":"education;internet of things;telecommunication;input;human-computer interaction;networks","HighLevel":"industries;technology;end users and user experience","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[129,38,187,464,200,123,384,118,45,266,31,395,110,286,210,289,359,139,142,447],"Top20Abs":[464,200,187,129,45,118,38,384,153,174,326,224,0,142,395,139,49,419,281,33],"Top20Tags":[129,123,286,31,298,121,38,110,258,220,289,271,359,210,266,242,378,376,58,159],"Citation":"Kim, J. C., Laine, T. H., & \u00c5hlund, C. (2021). Multimodal Interaction Systems Based on Internet of Things and Augmented Reality: A Systematic Literature Review. Applied Sciences, 11(4), 1738. https:\/\/doi.org\/10.3390\/app11041738\n"},{"Title":"Investigation of the Effectiveness of an Augmented Reality and a Dynamic Simulation System Collaboration in Oil Pump Maintenance","DOI":"10.3390\/app12010350","Publication year":2021,"Abstract":"The maintenance of oil pumps is a complex task for any operating organization, and for an industrial enterprise in the oil and gas sector of the economy, this issue has a high degree of urgency. One of the reasons for this is a wide spread of pumping equipment in all areas of oil and gas enterprises. At the same time, an aggressive environment, uneven load, remote facilities, and harsh climatic zones (especially in the areas of the Arctic region or production platforms) are factors that make it relevant to develop special systems that help or simplify the maintenance of pumping equipment. Dynamic modeling is one of the modern technologies which allows for solving the urgent issue of assessing the technical condition of equipment. It is the basis of systems that carry out diagnostics and prognostic calculations and allow for assessing the dynamic state of objects under various conditions of their operation, among other functions. Augmented reality technology is a technology that allows for reducing the time for equipment maintenance by reducing the time for searching and processing various information required in the maintenance process. This paper presents an investigation of the effectiveness of an augmented reality and a dynamic simulation system collaboration in oil pump maintenance. Since there is insufficient research on the joint application of these two technologies, the urgent issue is to prove the effectiveness of such collaboration. For this purpose, this paper provides a description of the system structure, gives a description of the development process of the augmented reality system application and tests the application using Microsoft HoloLens 2.","LowLevel":"computer simulation;gas industry;groupware;maintenance engineering;petroleum industry;pumps","MidLevel":"oil and gas;simulation;collaboration;power and energy;manufacturing","HighLevel":"industries;use cases","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[261,389,315,360,179,190,233,159,144,35,137,106,12,395,303,267,157,342,212,414],"Top20Abs":[389,315,261,360,159,190,179,233,395,12,157,35,144,106,414,342,137,118,457,205],"Top20Tags":[303,227,29,17,304,190,35,203,279,144,199,239,261,104,389,267,69,179,337,329],"Citation":"Koteleva, N., Valnev, V., & Frenkel, I. (2021). Investigation of the Effectiveness of an Augmented Reality and a Dynamic Simulation System Collaboration in Oil Pump Maintenance. Applied Sciences, 12(1), 350. https:\/\/doi.org\/10.3390\/app12010350\n"},{"Title":"Programming Robots by Demonstration Using Augmented Reality","DOI":"10.3390\/s21175976","Publication year":2021,"Abstract":"The world is living the fourth industrial revolution, marked by the increasing intelligence and automation of manufacturing systems. Nevertheless, there are types of tasks that are too complex or too expensive to be fully automated, it would be more efficient if the machines were able to work with the human, not only by sharing the same workspace but also as useful collaborators. A possible solution to that problem is on human-robot interaction systems, understanding the applications where they can be helpful to implement and what are the challenges they face. This work proposes the development of an industrial prototype of a human-machine interaction system through Augmented Reality, in which the objective is to enable an industrial operator without any programming experience to program a robot. The system itself is divided into two different parts: the tracking system, which records the operator's hand movement, and the translator system, which writes the program to be sent to the robot that will execute the task. To demonstrate the concept, the user drew geometric figures, and the robot was able to replicate the operator's path recorded.","LowLevel":"automatic programming;control engineering computing;human-robot interaction;industrial robots;manufacturing systems;production engineering computing;robot programming","MidLevel":"education;engineering;robotics;developers;manufacturing;other","HighLevel":"industries;technology;other","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[19,205,84,312,45,371,399,352,255,179,159,12,122,137,429,411,322,70,311,175],"Top20Abs":[19,312,205,371,84,45,137,399,352,12,311,465,175,179,269,401,0,322,157,411],"Top20Tags":[205,19,407,159,255,429,70,227,388,122,399,370,352,420,45,449,120,84,179,28],"Citation":"Soares, I., Petry, M., & Moreira, A. P. (2021). Programming Robots by Demonstration Using Augmented Reality. Sensors, 21(17), 5976. https:\/\/doi.org\/10.3390\/s21175976\n"},{"Title":"Augmented Reality Assisted Assembly Training Oriented Dynamic Gesture Recognition and Prediction","DOI":"10.3390\/app11219789","Publication year":2021,"Abstract":"Augmented reality assisted assembly training (ARAAT) is an effective and affordable technique for labor training in the automobile and electronic industry. In general, most tasks of ARAAT are conducted by real-time hand operations. In this paper, we propose an algorithm of dynamic gesture recognition and prediction that aims to evaluate the standard and achievement of the hand operations for a given task in ARAAT. We consider that the given task can be decomposed into a series of hand operations and furthermore each hand operation into several continuous actions. Then, each action is related with a standard gesture based on the practical assembly task such that the standard and achievement of the actions included in the operations can be identified and predicted by the sequences of gestures instead of the performance throughout the whole task. Based on the practical industrial assembly, we specified five typical tasks, three typical operations, and six standard actions. We used Zernike moments combined histogram of oriented gradient and linear interpolation motion trajectories to represent 2D static and 3D dynamic features of standard gestures, respectively, and chose the directional pulse-coupled neural network as the classifier to recognize the gestures. In addition, we defined an action unit to reduce the dimensions of features and computational cost. During gesture recognition, we optimized the gesture boundaries iteratively by calculating the score probability density distribution to reduce interferences of invalid gestures and improve precision. The proposed algorithm was evaluated on four datasets and proved to increase recognition accuracy and reduce the computational cost from the experimental results.","LowLevel":"computer vision;feature extraction;gesture recognition;image classification;image denoising;image motion analysis;image sequences;interpolation;neural networks;object detection;probability;solid modelling","MidLevel":"artificial intelligence;human factors;video;computer vision;data;chemical;input;manufacturing","HighLevel":"industries;technology;end users and user experience","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[177,387,242,118,370,250,276,366,137,445,348,311,66,416,23,12,374,375,255,81],"Top20Abs":[177,118,387,250,242,370,445,66,366,137,374,22,157,276,416,311,375,269,264,389],"Top20Tags":[277,387,451,321,439,98,381,255,116,81,143,379,144,276,71,415,160,190,76,32],"Citation":"Dong, J., Xia, Z., & Zhao, Q. (2021). Augmented Reality Assisted Assembly Training Oriented Dynamic Gesture Recognition and Prediction. Applied Sciences, 11(21), 9789. https:\/\/doi.org\/10.3390\/app11219789\n"},{"Title":"UX in AR-Supported Industrial Human-Robot Collaborative Tasks: A Systematic Review","DOI":"10.3390\/app112110448","Publication year":2021,"Abstract":"The fourth industrial revolution is promoting the Operator 4.0 paradigm, originating from a renovated attention towards human factors, growingly involved in the design of modern, human-centered processes. New technologies, such as augmented reality or collaborative robotics are thus increasingly studied and progressively applied to solve the modern operators' needs. Human-centered design approaches can help to identify user's needs and functional requirements, solving usability issues, or reducing cognitive or physical stress. The paper reviews the recent literature on augmented reality-supported collaborative robotics from a human-centered perspective. To this end, the study analyzed 21 papers selected after a quality assessment procedure and remarks the poor adoption of user-centered approaches and methodologies to drive the development of human-centered augmented reality applications to promote an efficient collaboration between humans and robots. To remedy this deficiency, the paper ultimately proposes a structured framework driven by User eXperience approaches to design augmented reality interfaces by encompassing previous research works. Future developments are discussed, stimulating fruitful reflections and a decisive standardization process.","LowLevel":"human-robot interaction;industrial robots;production engineering computing;user centered design;user experience","MidLevel":"human factors;engineering;robotics;developers;manufacturing;human-computer interaction","HighLevel":"end users and user experience;technology;industries","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[174,19,179,84,122,159,70,12,255,45,286,407,405,114,118,352,311,419,397,399],"Top20Abs":[179,84,174,19,122,12,286,311,405,114,118,70,153,443,444,366,419,388,399,397],"Top20Tags":[174,407,19,255,70,159,122,388,84,352,370,45,179,429,419,405,365,415,142,435],"Citation":"Khamaisi, R. K., Prati, E., Peruzzini, M., Raffaeli, R., & Pellicciari, M. (2021). UX in AR-Supported Industrial Human\u2013Robot Collaborative Tasks: A Systematic Review. Applied Sciences, 11(21), 10448. https:\/\/doi.org\/10.3390\/app112110448\n"},{"Title":"Towards a New Learning Experience through a Mobile Application with Augmented Reality in Engineering Education","DOI":"10.3390\/app11114921","Publication year":2021,"Abstract":"With the rise of information technology and digitization, education has been faced with the need to adopt new learning models using technology to create innovative educational methodologies. In addition, due to pandemic restrictions and in order to help contain the spread of the virus (COVID-19), all educational institutions have been forced to switch immediately to online education. The application of augmented reality (AR) in education provides important benefits, such as increased engagement and interactivity, and can help to minimize the negative effects of the disruption of face-to-face education. Therefore, this paper focuses on describing the effect of an augmented reality mobile application (NetAR) that was developed for engineering students as a complement to traditional education. To achieve this objective, an experimental group and a control group were established to work with the application for three weeks for three hours a day. Moreover, there are a number of usability issues with AR that may impact learning effectiveness and motivation. Therefore, the usability of the application was evaluated with the IBM Computer System Usability Questionnaire (CSUQ) tool. The usability results show that users are satisfied with NetAR, and the statistical data from the control group indicate that the application positively affects learning.","LowLevel":"computer aided instruction;diseases;educational institutions;engineering education;epidemics;further education;microorganisms;mobile learning;user experience","MidLevel":"education;medical;human factors;training","HighLevel":"industries;use cases;end users and user experience","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[391,320,390,136,363,272,383,87,185,56,103,163,145,342,57,46,306,241,154,134],"Top20Abs":[136,363,391,87,185,46,383,390,320,163,306,272,103,342,62,88,369,70,56,118],"Top20Tags":[391,145,320,405,134,241,124,390,139,262,272,88,363,196,103,113,336,56,61,246],"Citation":"Criollo-C, S., Abad-V\u00e1squez, D., Martic-Nieto, M., Vel\u00e1squez-G, F. A., P\u00e9rez-Medina, J.-L., & Luj\u00e1n-Mora, S. (2021). Towards a New Learning Experience through a Mobile Application with Augmented Reality in Engineering Education. Applied Sciences, 11(11), 4921. https:\/\/doi.org\/10.3390\/app11114921\n"},{"Title":"Augmented Reality Maintenance Assistant Using YOLOv5","DOI":"10.3390\/app11114758","Publication year":2021,"Abstract":"Maintenance professionals and other technical staff regularly need to learn to identify new parts in car engines and other equipment. The present work proposes a model of a task assistant based on a deep learning neural network. A YOLOv5 network is used for recognizing some of the constituent parts of an automobile. A dataset of car engine images was created and eight car parts were marked in the images. Then, the neural network was trained to detect each part. The results show that YOLOv5s is able to successfully detect the parts in real time video streams, with high accuracy, thus being useful as an aid to train professionals learning to deal with new equipment using augmented reality. The architecture of an object recognition system using augmented reality glasses is also designed.","LowLevel":"human computer interaction;learning algorithms;maintenance engineering;neural networks;object recognition;video streaming","MidLevel":"artificial intelligence;human factors;video;computer vision;medical;manufacturing;human-computer interaction","HighLevel":"end users and user experience;technology;industries","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[1,116,85,453,143,160,190,106,41,131,401,174,341,311,148,255,274,268,487,415],"Top20Abs":[1,85,453,41,143,116,160,106,268,174,341,131,401,487,447,255,458,159,190,379],"Top20Tags":[190,277,256,415,116,214,172,339,421,370,179,99,213,198,72,131,422,35,32,255],"Citation":"Malta, A., Mendes, M., & Farinha, T. (2021). Augmented Reality Maintenance Assistant Using YOLOv5. Applied Sciences, 11(11), 4758. https:\/\/doi.org\/10.3390\/app11114758\n"},{"Title":"Mixed, Augmented and Virtual, Reality Applied to the Teaching of Mathematics for Architects","DOI":"10.3390\/app11157125","Publication year":2021,"Abstract":"This paper examines the possibilities of Mixed Reality, the combination of two emerging technologies&#8212;Augmented Reality and Virtual Reality&#8212;in university education. For this purpose, an object was elaborated in Mixed Reality that underwent the evaluation of 44 first-year students from the degree in architecture who were enrolled in the subject \"Mathematical Foundations for Architecture.\" The instrument utilized was based on the TAM model, which analyzes the degree of acceptance of the technology used. The analysis of the responses provided by students supported the 23 hypotheses formulated in this study. It was found that MR significantly influences the perceived usefulness and ease of use. The results imply that MR utilization has positive effects on the mathematical teaching-learning processes in architecture from the students' perception of their mastery of technology. It becomes necessary to offer support to those university teachers who promote the use of active MR-based methodologies in classrooms.","LowLevel":"computer aided instruction;educational institutions;mathematics computing;teaching","MidLevel":"education;training;engineering","HighLevel":"industries;technology;use cases","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[39,56,62,134,439,88,87,204,306,82,163,320,342,414,57,395,410,136,128,17],"Top20Abs":[39,62,88,306,439,359,56,204,87,410,395,134,163,82,227,57,418,46,124,397],"Top20Tags":[136,39,241,33,232,128,363,17,51,88,56,154,320,372,405,222,171,204,196,61],"Citation":"Cabero-Almenara, J., Barroso-Osuna, J., & Martinez-Roig, R. (2021). Mixed, Augmented and Virtual, Reality Applied to the Teaching of Mathematics for Architects. Applied Sciences, 11(15), 7125. https:\/\/doi.org\/10.3390\/app11157125\n"},{"Title":"Calculating and Analyzing Angular Head Jerk in Augmented and Virtual Reality: Effect of AR Cue Design on Angular Jerk","DOI":"10.3390\/app112110082","Publication year":2021,"Abstract":"In this work, we propose a convenient method for evaluating levels of angular jerk in augmented reality (AR) and virtual reality (VR). Jerk is a rarely analyzed metric in usability studies, although it can be measured and calculated easily with most head-worn displays and can yield highly relevant information to designers. Here, we developed and implemented a system capable of calculating and analyzing jerk in real-time based on orientation data from an off-the-shelf head-worn display. An experiment was then carried out to determine whether the presence of AR user interface annotations results in changes to users' angular head jerk when conducting a time-pressured visual search task. Analysis of the data indicates that a decrease in jerk is significantly associated with the use of AR augmentations. As noted in the limitations section, however, the conclusions drawn from this work should be limited, as this analysis method is novel in the VR\/AR space and because of methodological limitations that limited the reliability of the jerk data. The work presented herein considerably facilitates the use of jerk as a quick component measure of usability and serves as an initial point off which future research involving jerk in VR and AR can be performed.","LowLevel":"helmet mounted displays;user interfaces","MidLevel":"human-computer interaction;wearables;display technology","HighLevel":"displays;end users and user experience","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[165,60,210,200,118,237,316,248,69,21,80,180,65,243,64,279,267,38,75,195],"Top20Abs":[60,165,200,248,316,237,118,243,276,267,210,65,49,64,38,69,234,178,166,283],"Top20Tags":[325,206,279,331,21,364,414,483,80,248,217,371,75,288,313,440,165,69,244,122],"Citation":"Van Dam, J., Tanous, K., Werner, M., & Gabbard, J. L. (2021). Calculating and Analyzing Angular Head Jerk in Augmented and Virtual Reality: Effect of AR Cue Design on Angular Jerk. Applied Sciences, 11(21), 10082. https:\/\/doi.org\/10.3390\/app112110082\n"},{"Title":"Investigating the Usability of a Head-Mounted Display Augmented Reality Device in Elementary School Children","DOI":"10.3390\/s21196623","Publication year":2021,"Abstract":"Augmenting reality via head-mounted displays (HMD-AR) is an emerging technology in education. The interactivity provided by HMD-AR devices is particularly promising for learning, but presents a challenge to human activity recognition, especially with children. Recent technological advances regarding speech and gesture recognition concerning Microsoft's HoloLens 2 may address this prevailing issue. In a within-subjects study with 47 elementary school children (2nd to 6th grade), we examined the usability of the HoloLens 2 using a standardized tutorial on multimodal interaction in AR. The overall system usability was rated \"good\". However, several behavioral metrics indicated that specific interaction modes differed in their efficiency. The results are of major importance for the development of learning applications in HMD-AR as they partially deviate from previous findings. In particular, the well-functioning recognition of children's voice commands that we observed represents a novelty. Furthermore, we found different interaction preferences in HMD-AR among the children. We also found the use of HMD-AR to have a positive effect on children's activity-related achievement emotions. Overall, our findings can serve as a basis for determining general requirements, possibilities, and limitations of the implementation of educational HMD-AR environments in elementary school classrooms.","LowLevel":"computer aided instruction;gesture recognition;handicapped aids;helmet mounted displays;human computer interaction","MidLevel":"training;human factors;medical;wearables;input;human-computer interaction;display technology","HighLevel":"displays;end users and user experience;use cases;industries;technology","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[217,364,38,115,69,80,425,136,83,57,118,224,352,62,163,301,87,140,284,185],"Top20Abs":[217,115,69,364,136,38,83,425,352,87,62,118,163,57,140,284,70,184,185,34],"Top20Tags":[80,279,364,75,21,242,66,217,206,38,301,93,165,424,415,351,366,53,419,250],"Citation":"Lauer, L., Altmeyer, K., Malone, S., Barz, M., Br\u00fcnken, R., Sonntag, D., & Peschel, M. (2021). Investigating the Usability of a Head-Mounted Display Augmented Reality Device in Elementary School Children. Sensors, 21(19), 6623. https:\/\/doi.org\/10.3390\/s21196623\n"},{"Title":"Silhouettes from Real Objects Enable Realistic Interactions with a Virtual Human in Mobile Augmented Reality","DOI":"10.3390\/app11062763","Publication year":2021,"Abstract":"Realistic interactions with real objects (e.g., animals, toys, robots) in an augmented reality (AR) environment enhances the user experience. The common AR apps on the market achieve realistic interactions by superimposing pre-modeled virtual proxies on the real objects in the AR environment. This way user perceives the interaction with virtual proxies as interaction with real objects. However, catering to environment change, shape deformation, and view update is not a trivial task. Our proposed method uses the dynamic silhouette of a real object to enable realistic interactions. Our approach is practical, lightweight, and requires no additional hardware besides the device camera. For a case study, we designed a mobile AR application to interact with real animal dolls. Our scenario included a virtual human performing four types of realistic interactions. Results demonstrated our method's stability that does not require pre-modeled virtual proxies in case of shape deformation and view update. We also conducted a pilot study using our approach and reported significant improvements in user perception of spatial awareness and presence for realistic interactions with a virtual human.","LowLevel":"human computer interaction;mobile computing;user experience","MidLevel":"telecommunication;human-computer interaction;human factors","HighLevel":"end users and user experience;industries","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[415,301,79,224,200,440,148,244,419,80,439,424,208,70,177,53,210,48,175,267],"Top20Abs":[415,301,224,440,200,208,80,79,424,419,177,439,148,70,123,267,175,210,19,486],"Top20Tags":[365,29,382,415,248,70,339,13,36,79,37,419,279,205,259,147,63,251,103,195],"Citation":"Kim, H., Ali, G., Pastor, A., Lee, M., Kim, G. J., & Hwang, J.-I. (2021). Silhouettes from Real Objects Enable Realistic Interactions with a Virtual Human in Mobile Augmented Reality. Applied Sciences, 11(6), 2763. https:\/\/doi.org\/10.3390\/app11062763\n"},{"Title":"Supporting Visualization Analysis in Industrial Process Tomography by Using Augmented Reality-A Case Study of an Industrial Microwave Drying System &#8224;","DOI":"10.3390\/s21196515","Publication year":2021,"Abstract":"Industrial process tomography (IPT) based process control is an advisable approach in industrial heating processes for improving system efficiency and quality. When using it, appropriate dataflow pipelines and visualizations are key for domain users to implement precise data acquisition and analysis. In this article, we propose a complete data processing and visualizing workflow regarding a specific case&#8212;microwave tomography (MWT) controlled industrial microwave drying system. Furthermore, we present the up-to-date augmented reality (AR) technique to support the corresponding data visualization and on-site analysis. As a pioneering study of using AR to benefit IPT systems, the proposed AR module provides straightforward and comprehensible visualizations pertaining to the process data to the related users. Inside the dataflow of the case, a time reversal imaging algorithm, a post-imaging segmentation, and a volumetric visualization module are included. For the time reversal algorithm, we exhaustively introduce each step for MWT image reconstruction and then present the simulated results. For the post-imaging segmentation, an automatic tomographic segmentation algorithm is utilized to reveal the significant information contained in the reconstructed images. For volumetric visualization, the 3D generated information is displayed. Finally, the proposed AR system is integrated with the on-going process data, including reconstructed, segmented, and volumetric images, which are used for facilitating interactive on-site data analysis for domain users. The central part of the AR system is implemented by a mobile app that is currently supported on iOS\/Android platforms.","LowLevel":"data acquisition;data analysis;data visualization;drying;image reconstruction;image segmentation;medical image processing;mobile computing;process control;tomography","MidLevel":"telecommunication;computer vision;data;medical;construction;industrial equipment;input;other","HighLevel":"industries;technology;other","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[157,4,234,148,479,20,432,7,69,424,181,340,411,134,418,252,45,89,80,370],"Top20Abs":[157,4,479,20,7,432,148,45,424,181,166,252,234,370,84,87,80,401,89,239],"Top20Tags":[234,418,71,157,356,280,326,370,42,186,376,225,73,4,69,318,379,181,381,130],"Citation":"Zhang, Y., Omrani, A., Yadav, R., & Fjeld, M. (2021). Supporting Visualization Analysis in Industrial Process Tomography by Using Augmented Reality\u2014A Case Study of an Industrial Microwave Drying System. Sensors, 21(19), 6515. https:\/\/doi.org\/10.3390\/s21196515\n"},{"Title":"How to Make Augmented Reality a Tool for Railway Maintenance Operations: Operator 4.0 Perspective","DOI":"10.3390\/app11062656","Publication year":2021,"Abstract":"In the last few decades, several initiatives and approaches are set up to support maintenance procedures for the railway industry in adopting the principles of Industry 4.0. Contextualized maintenance technologies such as Augmented Reality (AR) overlay can integrate virtual information on physical objects to improve decision-making and action-taking processes. Operators work in a dynamic working environment requiring both high adaptive capabilities and expert knowledge. There is a need to support the operators with tailor-based information that is customized and contextualized to their expertise and experience. It calls for AR tools and approaches that combine complex methodologies with high usability requirements. The development of these AR tools could benefit from a structured approach. Therefore, the objective of this paper is to propose an adaptive architectural framework aimed at shaping and structuring the process that provides operators with tailored support when using an AR tool. Case study research is applied within a revelatory railway industry setting. It was found that the framework ensures that self-explanatory AR systems can capture the knowledge of the operator, support the operator during maintenance activities, conduct failure analysis, provide problem-solving strategies, and improve learning capabilities. This study contributes to the necessity of having a human-centered approach for the successful adaption of AR technology tools for the railway industry.","LowLevel":"decision making;failure analysis;maintenance engineering;production engineering computing;railways;user interfaces","MidLevel":"human factors;engineering;inspection, safety and quality;manufacturing;human-computer interaction;other","HighLevel":"end users and user experience;use cases;industries;technology;other","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[84,205,315,45,135,86,137,2,388,261,174,68,389,122,54,159,93,233,131,373],"Top20Abs":[84,205,315,135,45,2,137,389,261,388,93,86,68,159,136,54,131,35,373,174],"Top20Tags":[84,388,382,205,370,122,407,436,86,45,360,419,144,106,174,411,12,101,54,273],"Citation":"Scheffer, S., Martinetti, A., Damgrave, R., Thiede, S., & van Dongen, L. (2021). How to Make Augmented Reality a Tool for Railway Maintenance Operations: Operator 4.0 Perspective. Applied Sciences, 11(6), 2656. https:\/\/doi.org\/10.3390\/app11062656\n"},{"Title":"Formalization of the Burning Process of Virtual Reality Objects in Adaptive Training Complexes","DOI":"10.3390\/jimaging7050086","Publication year":2021,"Abstract":"Within the scope of this article, the problem of the formalization of physical processes in adaptive training complexes is considered on the example of virtual objects burning. Despite a fairly complete study of this process, the existing mathematical models are not adapted for the application in training complexes, which leads to a significant increase in costs and lower productivity due to the complexity of the calculations. Therefore, an adapted mathematical model is proposed that allows us to formalize the structure of virtual objects of burning, their basic properties and the processes of changing states, starting from the flame development of an object and ending with their complete destruction or extinguishment. The article proposes the use of threshold value diagrams and rules for changing the states of virtual reality objects to solve the problem of the formalization of burning processes. This tool is quite multi-purpose, which allows you to describe various physical processes, such as smoke, flooding, the spread of toxic gases, etc. The area of the proposed formalization approach includes the design and implementation of physical processes in simulators and multimedia complexes using virtual and augmented reality. Thus, the presented scientific research can be used to formalize the physical processes in adaptive training complexes for professional ergatic systems.","LowLevel":"combustion;computer based training;costing","MidLevel":"training;business performance metrics;farming and natural science;other","HighLevel":"industries;business;use cases;other","Venue":"J. Imaging (Switzerland)","Top20AbsAndTags":[436,252,388,87,414,179,131,207,246,408,294,224,400,178,132,54,370,340,114,432],"Top20Abs":[252,436,388,179,207,414,294,87,131,224,408,246,178,340,370,432,45,222,208,91],"Top20Tags":[54,140,113,436,204,87,408,33,390,93,419,9,17,301,112,469,167,34,51,196],"Citation":"Krasnyanskiy, M., Obukhov, A., & Dedov, D. (2021). Formalization of the Burning Process of Virtual Reality Objects in Adaptive Training Complexes. Journal of Imaging, 7(5), 86. https:\/\/doi.org\/10.3390\/jimaging7050086\n"},{"Title":"Multi-Shape Free-Form Deformation Framework for Efficient Data Transmission in AR-Based Medical Training Simulators","DOI":"10.3390\/app11219925","Publication year":2021,"Abstract":"Augmented reality medical training simulators can provide a realistic and immersive experience by overlapping the virtual scene on to the real world. Latency in augmented reality (AR) medical training simulators is an important issue as it can lead to motion sickness for users. This paper proposes a framework that can achieve real-time rendering of the 3D scene aligned to the real world using a head-mounted display (HMD). Model deformation in the 3D scene is categorised into local deformation derived from user interaction and global deformation determined by the simulation scenario. Target shapes are predefined by a simulation scenario, and control points are placed to embed the predefined shapes. Free-form deformation (FFD) is applied to multiple shapes to efficiently transfer the simulated model to the HMD. Global deformation is computed by blending a mapping matrix of each FFD with an assigned weighting value. The local and global deformation are then transferred through the control points updated from a deformed surface mesh and its corresponding weighting value. The proposed framework is verified in terms of latency caused by data transmission and the accuracy of a transmitted surface mesh in a vaginal examination (VE) training simulation. The average latency is reduced to 7 ms, less than the latency causing motion sickness in virtual reality simulations. The maximum relative error is less than 3%. Our framework allows seamless rendering of a virtual scene to the real world with substantially reduced latency and without the need for an external tracking system.","LowLevel":"computer based training;helmet mounted displays;image registration;medical computing;rendering;solid modelling","MidLevel":"training;computer vision;medical;graphics;wearables;manufacturing;farming and natural science;display technology","HighLevel":"displays;technology;industries;use cases","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[148,217,142,440,259,120,212,180,436,98,364,464,439,152,38,294,279,69,194,488],"Top20Abs":[142,148,152,212,98,294,120,440,259,439,340,464,217,20,436,488,431,194,486,445],"Top20Tags":[206,217,424,112,419,80,180,140,364,21,243,69,233,279,126,167,54,238,275,38],"Citation":"Kim, M., & Bello, F. (2021). Multi-Shape Free-Form Deformation Framework for Efficient Data Transmission in AR-Based Medical Training Simulators. Applied Sciences, 11(21), 9925. https:\/\/doi.org\/10.3390\/app11219925\n"},{"Title":"Smart Facility Management System Based on Open BIM and Augmented Reality Technology","DOI":"10.3390\/app112110283","Publication year":2021,"Abstract":"With the wave of the Fourth Industrial Revolution, the construction industry is also witnessing the application of numerous state-of-the-art technologies. Among these, augmented reality (AR) technology has the advantage of utilizing existing 3D models and BIM data and is thus an area of active research. However, the main area of research to date has either been in visualizing information during the design phase, where architects and project stakeholders can share viewings, or in confirming the required information for construction management through visualization during the construction phase. As such, more research is required in the application of AR during the facility management (FM) phase. Research utilizing BIM in the FM phase, which constitutes the longest period during the lifecycle of a building, has been continuously carried out but has faced challenges with regard to on-site application. The reason for this is that information required for BIM during the design, construction and FM phases is different, and the reproduced information is vast, so identifying the required BIM data for FM and interfacing with other systems is difficult. As a measure to overcome this limitation, advanced countries such as the US and UK have developed and are using Construction Operations Building information exchange (COBie), which is an open-source BIM-based information exchange system. In order to effectively convert open-source BIM data to AR data, this research defined COBie data for windows and doors, converted them to a system and validated that it could actually be applied for on-site FM. The results of this system's creation and validation showed that the proposed AR-based smart FMS demonstrated faster and easier access to information compared with existing 2D blueprint-based FM work, while information obtained through AR allowed for immediate, more visual and easier means to express the information when integrated with actual objects.","LowLevel":"buildings;civil engineering computing;construction industry;facilities management;production engineering computing;project management;solid modelling;structural engineering computing","MidLevel":"engineering;business planning and management;construction;manufacturing;other","HighLevel":"industries;technology;business;other","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[389,146,86,412,35,431,178,289,418,4,244,44,273,47,166,252,472,2,411,139],"Top20Abs":[146,389,86,35,412,431,178,4,289,472,244,166,273,47,44,139,486,418,159,252],"Top20Tags":[146,86,412,418,389,35,370,388,411,205,10,174,12,77,395,90,329,407,178,227],"Citation":"Chung, S., Cho, C.-S., Song, J., Lee, K., Lee, S., & Kwon, S. (2021). Smart Facility Management System Based on Open BIM and Augmented Reality Technology. Applied Sciences, 11(21), 10283. https:\/\/doi.org\/10.3390\/app112110283\n"},{"Title":"Quality Assessment of 3D Synthesized Images Based on Textural and Structural Distortion Estimation","DOI":"10.3390\/app11062666","Publication year":2021,"Abstract":"Emerging 3D-related technologies such as augmented reality, virtual reality, mixed reality, and stereoscopy have gained remarkable growth due to their numerous applications in the entertainment, gaming, and electromedical industries. In particular, the 3D television (3DTV) and free-viewpoint television (FTV) enhance viewers' television experience by providing immersion. They need an infinite number of views to provide a full parallax to the viewer, which is not practical due to various financial and technological constraints. Therefore, novel 3D views are generated from a set of available views and their depth maps using depth-image-based rendering (DIBR) techniques. The quality of a DIBR-synthesized image may be compromised for several reasons, e.g., inaccurate depth estimation. Since depth is important in this application, inaccuracies in depth maps lead to different textural and structural distortions that degrade the quality of the generated image and result in a poor quality of experience (QoE). Therefore, quality assessment DIBR-generated images are essential to guarantee an appreciative QoE. This paper aims at estimating the quality of DIBR-synthesized images and proposes a novel 3D objective image quality metric. The proposed algorithm aims to measure both textural and structural distortions in the DIBR image by exploiting the contrast sensitivity and the Hausdorff distance, respectively. The two measures are combined to estimate an overall quality score. The experimental evaluations performed on the benchmark MCL-3D dataset show that the proposed metric is reliable and accurate, and performs better than existing 2D and 3D quality assessment metrics.","LowLevel":"rendering;stereo image processing;three-dimensional television;video signal processing","MidLevel":"users;computer vision;data;graphics;sensors;semiconductors;other","HighLevel":"end users and user experience;technology;other","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[379,437,81,235,290,278,425,76,170,265,413,99,340,295,20,189,263,350,178,259],"Top20Abs":[379,437,278,235,81,290,413,425,170,295,265,99,76,20,12,350,340,10,189,259],"Top20Tags":[130,50,274,358,48,459,487,98,456,327,255,148,173,72,428,220,411,95,219,73],"Citation":"Alvi, H. M. U. H., Farid, M. S., Khan, M. H., & Grzegorzek, M. (2021). Quality Assessment of 3D Synthesized Images Based on Textural and Structural Distortion Estimation. Applied Sciences, 11(6), 2666. https:\/\/doi.org\/10.3390\/app11062666\n"},{"Title":"How to Promote User Purchase in Metaverse? A Systematic Literature Review on Consumer Behavior Research and Virtual Commerce Application Design","DOI":"10.3390\/app112311087","Publication year":2021,"Abstract":"Virtual commerce applies immersive technology such as augmented reality and virtual reality into e-commerce to shift consumer perception from 2D product catalogs to 3D immersive virtual spaces. In virtual commerce, the alignment of application design paradigms and the factors influencing consumer behavior is paramount to promote purchase of products and services. The question of their relation needs to be answered, together with the possible improvement of application design. This paper used a systematic literature review approach to synthesize research on virtual commerce from both application design and consumer behavior research, considering the promotion of purchase in virtual commerce settings. Throughout the review, influential factors to purchase and preeminent design artifacts were identified. Then, the research gaps were discovered by mapping the design artifacts to the influential factors, which can inspire future research opportunities on the synergy of these two research directions. Moreover, the evolution of virtual commerce research along with multiple directions were discussed, including the suggestion of meta-commerce as a future trend.","LowLevel":"consumer behaviour;electronic commerce;purchasing;user interfaces","MidLevel":"human-computer interaction;sales and marketing;logistics","HighLevel":"end users and user experience;business","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[102,355,436,156,161,1,279,25,418,388,219,376,33,205,208,5,267,251,410,448],"Top20Abs":[102,436,156,355,418,376,388,279,205,5,33,429,219,49,410,251,448,289,208,359],"Top20Tags":[40,102,355,13,156,1,161,25,368,244,238,160,377,193,210,414,382,221,36,331],"Citation":"Shen, B., Tan, W., Guo, J., Zhao, L., & Qin, P. (2021). How to Promote User Purchase in Metaverse? A Systematic Literature Review on Consumer Behavior Research and Virtual Commerce Application Design. Applied Sciences, 11(23), 11087. https:\/\/doi.org\/10.3390\/app112311087\n"},{"Title":"Hybrid Spine Simulator Prototype for X-ray Free Pedicle Screws Fixation Training","DOI":"10.3390\/app11031038","Publication year":2021,"Abstract":"Simulation for surgical training is increasingly being considered a valuable addition to traditional teaching methods. 3D-printed physical simulators can be used for preoperative planning and rehearsal in spine surgery to improve surgical workflows and postoperative patient outcomes. This paper proposes an innovative strategy to build a hybrid simulation platform for training of pedicle screws fixation: the proposed method combines 3D-printed patient-specific spine models with augmented reality functionalities and virtual X-ray visualization, thus avoiding any exposure to harmful radiation during the simulation. Software functionalities are implemented by using a low-cost tracking strategy based on fiducial marker detection. Quantitative tests demonstrate the accuracy of the method to track the vertebral model and surgical tools, and to coherently visualize them in either the augmented reality or virtual fluoroscopic modalities. The obtained results encourage further research and clinical validation towards the use of the simulator as an effective tool for training in pedicle screws insertion in lumbar vertebrae.","LowLevel":"biomechanics;bone;computerized tomography;diagnostic radiography;medical computing;medical image processing;medical robotics;neurophysiology;orthopedics;prosthetics;surgery","MidLevel":"telecommunication;computer vision;robotics;medical;data","HighLevel":"industries;technology","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[152,356,326,91,186,234,168,340,116,465,408,69,301,384,432,417,397,285,233,464],"Top20Abs":[356,152,326,91,301,340,116,408,168,233,432,212,436,465,285,464,208,148,433,485],"Top20Tags":[186,326,71,356,417,152,397,91,133,234,168,425,69,108,112,384,376,465,178,408],"Citation":"Condino, S., Turini, G., Mamone, V., Parchi, P. D., & Ferrari, V. (2021). Hybrid Spine Simulator Prototype for X-ray Free Pedicle Screws Fixation Training. Applied Sciences, 11(3), 1038. https:\/\/doi.org\/10.3390\/app11031038\n"},{"Title":"Design and Validation of a Virtual Chemical Laboratory&#8212;An Example of Natural Science in Elementary Education","DOI":"10.3390\/app112110070","Publication year":2021,"Abstract":"In the natural science curriculum, chemistry is a very important domain. However, when conducting chemistry experiments, safety issues need to be taken seriously, and excessive material waste may be caused during the experiment. Based on the 11-year-old student science curriculum, this paper proposed a virtual chemistry laboratory, which was designed by combining a virtual experiment application with physical teaching materials. The virtual experiment application was a virtual experiment laboratory environment created by using selected experimental equipment cards in combination with augmented reality (AR) technology. The physical teaching materials included all virtual equipment required for experiment units. Each piece of equipment had corresponding cards for learners to choose from and utilize in specific experimental operations. It was hoped that students were able to achieve the desired learning effectiveness of experimental teaching while reducing the waste of experimental materials through the virtual experimental environment. This study employed the quasi-experimental and questionnaire survey methods to evaluate both learning effectiveness and learning motivation. Eighty-one students and eight elementary school teachers were surveyed as research subjects. The experimental results revealed that significant differences in learning effectiveness existed between the experimental group and control group, indicating that the application of AR technology to teaching substantively helped enhance students' learning effectiveness and motivation. In addition, the results of the teacher questionnaire demonstrated that the virtual chemistry laboratory proposed in this study could effectively assist with classroom teaching.","LowLevel":"computer aided instruction;physics education;teaching","MidLevel":"education;training;engineering","HighLevel":"industries;technology;use cases","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[185,320,136,262,39,163,383,369,196,114,82,306,312,134,87,171,124,17,103,272],"Top20Abs":[185,163,262,320,39,383,136,369,312,196,82,124,306,114,171,134,11,87,103,207],"Top20Tags":[52,89,306,33,369,154,363,128,272,134,136,3,82,246,372,17,87,9,232,204],"Citation":"Tsai, C.-Y., Ho, Y.-C., & Nisar, H. (2021). Design and Validation of a Virtual Chemical Laboratory\u2014An Example of Natural Science in Elementary Education. Applied Sciences, 11(21), 10070. https:\/\/doi.org\/10.3390\/app112110070\n"},{"Title":"Control Strategies for Gait Tele-Rehabilitation System Based on Parallel Robotics","DOI":"10.3390\/app112311095","Publication year":2021,"Abstract":"Among end-effector robots for lower limb rehabilitation, systems based on Stewart-Gough platforms enable independent movement of each foot in six degrees of freedom. Nevertheless, control strategies described in recent literature have not been able to fully explore the potential of such a mechatronic system. In this work, we propose two novel approaches for controlling a gait simulator based on Stewart-Gough platforms. The first strategy provides the therapist direct control of each platform using movement data measured by wearable sensors. The following scheme is designed to improve the level of engagement of the patient by enabling a limited degree of control based on trunk inclination. Both strategies are designed to facilitate future studies in tele-rehabilitation settings. Experimental results have illustrated the feasibility of both control interfaces, either in terms of system performance or user subjective evaluation. Technical capacity to deploy in tele-rehabilitation was also verified in this work.","LowLevel":"end effectors;mechatronics;medical robotics;patient rehabilitation","MidLevel":"internet of things;medical;robotics","HighLevel":"industries;technology","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[417,384,420,294,91,174,371,401,330,214,267,121,382,353,18,356,126,158,289,0],"Top20Abs":[417,384,420,174,158,294,91,330,18,371,267,126,214,401,261,0,356,442,289,165],"Top20Tags":[417,326,384,399,425,294,18,177,129,175,91,286,220,253,371,38,123,258,31,298],"Citation":"Bo, A. P. L., Casas, L., Cucho-Padin, G., Hayashibe, M., & Elias, D. (2021). Control Strategies for Gait Tele-Rehabilitation System Based on Parallel Robotics. Applied Sciences, 11(23), 11095. https:\/\/doi.org\/10.3390\/app112311095\n"},{"Title":"Design and Validation of an Augmented Reality Teaching System for Primary Logic Programming Education","DOI":"10.3390\/s22010389","Publication year":2021,"Abstract":"Programming is a skill that requires high levels of logical thinking and problem-solving abilities. According to the Curriculum Guidelines for the 12-Year Basic Education currently implemented in Taiwan, programming has been included in the mandatory courses of middle and high schools. Nevertheless, the guidelines simply recommend that elementary schools conduct fundamental instructions in related fields during alternative learning periods. This may result in the problem of a rough transition in programming learning for middle school freshmen. To alleviate this problem, this study proposes an augmented reality (AR) logic programming teaching system that combines AR technologies and game-based teaching material designs on the basis of the fundamental concepts for seventh-grade structured programming. This system can serve as an articulation curriculum for logic programming in primary education. Thus, students are able to develop basic programming logic concepts through AR technologies by performing simple command programming. This study conducted an experiment using the factor-based quasi-experimental research design and questionnaire survey method, with 42 fifth and sixth graders enrolled as the experimental subjects. The statistical analysis showed the following results: In terms of learning effectiveness, both AR-based and traditional learning groups displayed a significant performance. However, of the two groups, the former achieved more significant effectiveness in the posttest results. Regarding learning motivation, according to the evaluation results of the Attention, Relevance, Confidence, and Satisfaction (ARCS) motivation model, the AR-based learning group manifested significantly higher levels of learning motivation than the traditional learning group, with particularly significant differences observed in the dimension of Attention. Therefore, the experimental results validate that the proposed AR-based logic programming teaching system has significant positive effects on enhancing students' learning effectiveness and motivation.","LowLevel":"computer aided instruction;computer science education;design of experiments;educational courses;educational institutions;logic programming;statistical analysis;structured programming;teaching","MidLevel":"education;training;human factors;data;developers;human-computer interaction;other","HighLevel":"end users and user experience;use cases;industries;technology;other","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[312,342,163,320,136,232,128,410,383,369,88,124,114,154,262,171,87,306,57,147],"Top20Abs":[312,342,163,320,88,128,136,410,232,369,383,124,306,171,262,154,114,87,39,103],"Top20Tags":[232,154,88,128,56,136,13,241,405,33,363,246,134,449,320,57,40,163,55,262],"Citation":"Tsai, C.-Y., & Lai, Y.-C. (2022). Design and Validation of an Augmented Reality Teaching System for Primary Logic Programming Education. Sensors, 22(1), 389. https:\/\/doi.org\/10.3390\/s22010389\n"},{"Title":"Research on problems and countermeasures in the application of substation intelligent inspection system","DOI":"10.1088\/1742-6596\/1983\/1\/012084","Publication year":2021,"Abstract":"Substation plays an important role in the power system as the pivot of the power grid. With the continuous expansion of the power grid, the intelligent inspection system based on robots is widely used in the substation industry. However, the existing inspection robot has some drawbacks, such as low battery capacity, weak obstacle performance, slow movement speed, and blind areas during the inspection. This paper proposed some measurements aimed at these drawbacks, such as optimizing the robot performance, adding the image acquisition PTZ (Pan, Tilt, and Zoom camera) in fixed point, using the AR (Augmented Reality) glasses assistance. These approaches improve the efficiency and quality of the existing intelligent inspection system in the substation.","LowLevel":"cameras;inspection;mobile robots;power grids;substations","MidLevel":"inspection, safety and quality;power and energy;robotics;input","HighLevel":"industries;technology;use cases","Venue":"J. Phys., Conf. Ser. (UK)","Top20AbsAndTags":[345,478,462,19,371,486,479,174,467,12,278,307,215,84,269,139,179,312,401,370],"Top20Abs":[345,478,462,371,19,12,486,467,479,174,215,278,139,312,465,295,370,404,269,352],"Top20Tags":[345,396,399,60,398,258,379,120,68,19,90,374,192,206,84,126,362,381,445,462],"Citation":"Zhao, M., Mao, Y., Hen, Q., & Zhou, Y. (2021). Research on problems and countermeasures in the application of substation intelligent inspection system. Journal of Physics: Conference Series, 1983(1), 012084. https:\/\/doi.org\/10.1088\/1742-6596\/1983\/1\/012084\n"},{"Title":"Augmented Reality Based Surgical Navigation of Complex Pelvic Osteotomies-A Feasibility Study on Cadavers","DOI":"10.3390\/app11031228","Publication year":2021,"Abstract":"Augmented reality (AR)-based surgical navigation may offer new possibilities for safe and accurate surgical execution of complex osteotomies. In this study we investigated the feasibility of navigating the periacetabular osteotomy of Ganz (PAO), known as one of the most complex orthopedic interventions, on two cadaveric pelves under realistic operating room conditions. Preoperative planning was conducted on computed tomography (CT)-reconstructed 3D models using an in-house developed software, which allowed creating cutting plane objects for planning of the osteotomies and reorientation of the acetabular fragment. An AR application was developed comprising point-based registration, motion compensation and guidance for osteotomies as well as fragment reorientation. Navigation accuracy was evaluated on CT-reconstructed 3D models, resulting in an error of 10.8 mm for osteotomy starting points and 5.4&#176; for osteotomy directions. The reorientation errors were 6.7&#176;, 7.0&#176; and 0.9&#176; for the x-, y- and z-axis, respectively. Average postoperative error of LCE angle was 4.5&#176;. Our study demonstrated that the AR-based execution of complex osteotomies is feasible. Fragment realignment navigation needs further improvement, although it is more accurate than the state of the art in PAO surgery.","LowLevel":"biomechanics;bone;computerized tomography;image reconstruction;image registration;medical image processing;motion compensation;orthopedics;prosthetics;surgery","MidLevel":"artificial intelligence;computer vision;medical;data;construction;other","HighLevel":"industries;technology;other","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[294,152,326,321,191,356,340,488,63,232,69,234,189,105,433,139,49,70,98,178],"Top20Abs":[152,326,63,49,139,191,105,321,488,189,356,70,433,43,232,340,294,225,170,98],"Top20Tags":[294,71,397,356,234,133,326,69,370,417,178,321,91,168,112,130,425,152,143,379],"Citation":"Ackermann, J., Liebmann, F., Hoch, A., Snedeker, J. G., Farshad, M., Rahm, S., Zingg, P. O., & F\u00fcrnstahl, P. (2021). Augmented Reality Based Surgical Navigation of Complex Pelvic Osteotomies\u2014A Feasibility Study on Cadavers. Applied Sciences, 11(3), 1228. https:\/\/doi.org\/10.3390\/app11031228\n"},{"Title":"Bias and Repeatability of Measurements from 3D Scans Made Using iOS-Based Lidar","DOI":"10.4271\/2021-01-0891","Publication year":2021,"Abstract":"Apple introduced a lidar-based depth sensor and enhanced Augmented Reality (AR) application programming interface (API) to the 2020 iPad Pro and iPhone 12 Pro, making widespread use of 3D scanning possible. Here we quantified the bias and repeatability of the 3D scans made using this system. The exterior and interior of a single vehicle and the exterior of a filing cabinet were scanned four times by eight different operators. Each operator then extracted four measurements from each of the exterior scans, five measurements from each of the interior scans, and three measurements from each of the filing cabinet scans. Hand measurements using a string and tape measure were used as reference values to estimate the bias. The values extracted from the 3D scans were biased between 0.9 and 9.3 cm below the actual exterior measurements and between 2.2 cm below and 0.3 cm above the actual interior measurements. The repeatability standard deviation varied from 1.8 to 5.8 cm for the exterior measurements and from 0.6 to 1.4 cm for the interior measurements. Based on the measurements we used here, the interior 3D scans were more accurate than the exterior scans, with the interior scans having a similar bias, though larger variance, to hand measurements.","LowLevel":"application program interfaces;ios;optical radar;optical scanners;optical sensors;radar receivers","MidLevel":"optics;telecommunication;developers;sensors;geospatial;other","HighLevel":"displays;technology;industries;other","Venue":"SAE Int. J. Adv. Curr. Pract. Mobil. (USA)","Top20AbsAndTags":[461,169,173,182,105,451,379,201,445,158,189,120,401,294,488,300,316,68,152,287],"Top20Abs":[461,169,182,173,105,401,445,379,201,189,316,152,451,94,68,488,287,118,322,300],"Top20Tags":[120,379,173,158,451,317,271,403,445,419,422,348,182,201,240,126,324,248,41,209],"Citation":"Heinrichs, B. E., & Yang, M. (2021). Bias and Repeatability of Measurements from 3D Scans Made Using iOS-Based Lidar. SAE International Journal of Advances and Current Practices in Mobility, 3(5), 2219\u20132226. https:\/\/doi.org\/10.4271\/2021-01-0891\n"},{"Title":"Augmented Reality and Machine Learning Incorporation Using YOLOv3 and ARKit","DOI":"10.3390\/app11136006","Publication year":2021,"Abstract":"Augmented reality is one of the fastest growing fields, receiving increased funding for the last few years as people realise the potential benefits of rendering virtual information in the real world. Most of today's augmented reality marker-based applications use local feature detection and tracking techniques. The disadvantage of applying these techniques is that the markers must be modified to match the unique classified algorithms or they suffer from low detection accuracy. Machine learning is an ideal solution to overcome the current drawbacks of image processing in augmented reality applications. However, traditional data annotation requires extensive time and labour, as it is usually done manually. This study incorporates machine learning to detect and track augmented reality marker targets in an application using deep neural networks. We firstly implement the auto-generated dataset tool, which is used for the machine learning dataset preparation. The final iOS prototype application incorporates object detection, object tracking and augmented reality. The machine learning model is trained to recognise the differences between targets using one of YOLO's most well-known object detection methods. The final product makes use of a valuable toolkit for developing augmented reality applications called ARKit.","LowLevel":"feature extraction;learning algorithms;neural networks;object detection;object tracking","MidLevel":"artificial intelligence;computer vision;chemical;medical","HighLevel":"industries;technology","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[456,163,453,76,485,8,41,48,144,435,105,5,274,259,400,79,97,143,381,98],"Top20Abs":[456,163,76,105,485,400,435,453,41,48,8,144,87,155,259,207,294,5,150,97],"Top20Tags":[98,274,144,379,172,387,143,5,81,381,71,79,8,76,358,214,425,280,255,99],"Citation":"Le, H., Nguyen, M., Yan, W. Q., & Nguyen, H. (2021). Augmented Reality and Machine Learning Incorporation Using YOLOv3 and ARKit. Applied Sciences, 11(13), 6006. https:\/\/doi.org\/10.3390\/app11136006\n"},{"Title":"Computer-based learning: 3D visualization and animation as content development for digital learning materials for traditional Indonesian cloth (Songket Palembang)","DOI":"10.1088\/1742-6596\/1987\/1\/012003","Publication year":2021,"Abstract":"The animation is currently developing very rapidly, especially in 3D animation. Animated 3D visualization not only develops in the field of film, but also develops in digital-based learning such as interactive applications, presentation slides, e-books, and games, as well as supporting content for immersive technology, such as Augmented Reality, Virtual Reality, Mix Reality, and others. The development of 3D animation nowadays, but not accompanied by the use of animation as the delivery of information with local cultural content. The young generation's lack of interest in local cultural arts is accompanied by a lack of digital learning materials and the delivery of interesting information for the younger generation. Therefore, the researcher aims to create 3D animated visualization assets that can motivate students to learn Indonesian cultural arts. Researchers applying the method Subdivision Surface, UV-mapping, 3d Painting, and walk cycle animation in the 3D visualization animation production process. The final result of this research is 3D Visualization and Animation as Content Development for Digital Learning Material for Indonesian Traditional Fabrics: Songket Palembang based on cloud for elementary school students.","LowLevel":"computer aided instruction;computer animation;solid modelling","MidLevel":"graphics;manufacturing;training","HighLevel":"industries;technology;use cases","Venue":"J. Phys., Conf. Ser. (UK)","Top20AbsAndTags":[284,145,163,105,92,39,106,157,82,207,87,302,467,244,114,16,365,369,306,405],"Top20Abs":[284,145,163,105,207,39,87,82,157,302,365,467,244,106,16,306,369,114,400,192],"Top20Tags":[106,204,136,405,3,34,33,9,351,10,284,93,17,410,301,82,222,236,246,77],"Citation":"Sari, I. P., Permana, F. C., Firmansyah, F. H., & Hernawan, A. H. (2021). Computer-based learning: 3D visualization and animation as content development for digital learning materials for traditional Indonesian cloth (Songket Palembang). Journal of Physics: Conference Series, 1987(1), 012003. https:\/\/doi.org\/10.1088\/1742-6596\/1987\/1\/012003\n"},{"Title":"Augmented Reality, Virtual Reality and Artificial Intelligence in Orthopedic Surgery: A Systematic Review","DOI":"10.3390\/app11073253","Publication year":2021,"Abstract":"Background: The application of virtual and augmented reality technologies to orthopaedic surgery training and practice aims to increase the safety and accuracy of procedures and reducing complications and costs. The purpose of this systematic review is to summarise the present literature on this topic while providing a detailed analysis of current flaws and benefits. Methods: A comprehensive search on the PubMed, Cochrane, CINAHL, and Embase database was conducted from inception to February 2021. The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines were used to improve the reporting of the review. The Cochrane Risk of Bias Tool and the Methodological Index for Non-Randomized Studies (MINORS) was used to assess the quality and potential bias of the included randomized and non-randomized control trials, respectively. Results: Virtual reality has been proven revolutionary for both resident training and preoperative planning. Thanks to augmented reality, orthopaedic surgeons could carry out procedures faster and more accurately, improving overall safety. Artificial intelligence (AI) is a promising technology with limitless potential, but, nowadays, its use in orthopaedic surgery is limited to preoperative diagnosis. Conclusions: Extended reality technologies have the potential to reform orthopaedic training and practice, providing an opportunity for unidirectional growth towards a patient-centred approach.","LowLevel":"biomedical equipment;medical computing;medical information systems;orthopedics;reviews;surgery","MidLevel":"education;standards;medical;developers","HighLevel":"industries;technology;standards","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[408,294,222,234,152,91,67,356,376,218,308,87,436,432,380,170,46,33,49,108],"Top20Abs":[408,222,294,91,308,380,67,218,234,436,87,49,432,356,170,54,376,278,281,46],"Top20Tags":[152,222,33,294,326,390,356,40,186,408,108,91,234,336,376,230,210,67,354,47],"Citation":"Longo, U. G., De Salvatore, S., Candela, V., Zollo, G., Calabrese, G., Fioravanti, S., Giannone, L., Marchetti, A., De Marinis, M. G., & Denaro, V. (2021). Augmented Reality, Virtual Reality and Artificial Intelligence in Orthopedic Surgery: A Systematic Review. Applied Sciences, 11(7), 3253. https:\/\/doi.org\/10.3390\/app11073253\n"},{"Title":"Wearable Haptic Device for Stiffness Rendering of Virtual Objects in Augmented Reality","DOI":"10.3390\/app11156932","Publication year":2021,"Abstract":"We propose a novel wearable haptic device that can provide kinesthetic haptic feedback for stiffness rendering of virtual objects in augmented reality (AR). Rendering stiffness of objects using haptic feedback is crucial for realistic finger-based object manipulation, yet challenging particularly in AR due to the co-presence of a real hand, haptic device, and rendered AR objects in the scenes. By adopting passive actuation with a tendon-based transmission mechanism, the proposed haptic device can generate kinesthetic feedback strong enough for immersive manipulation and prevention of inter-penetration in a small-form-factor, while maximizing the wearability and minimizing the occlusion in AR usage. A selective locking module is adopted in the device to allow for the rendering of the elasticity of objects. We perform an experimental study of two-finger grasping to verify the efficacy of the proposed haptic device for finger-based manipulation in AR. We also quantitatively compare\/articulate the effects of different types of feedbacks across haptic and visual sense (i.e., kinesthetic haptic feedback, vibrotactile haptic feedback, and visuo-haptic feedback) for stiffness rendering of virtual objects in AR for the first time.","LowLevel":"elasticity;feedback;haptic interfaces;rendering","MidLevel":"graphics;human-computer interaction;input;engineering","HighLevel":"end users and user experience;technology","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[240,464,455,420,463,126,112,87,415,487,331,194,142,400,403,301,424,108,54,60],"Top20Abs":[240,464,420,455,415,87,194,487,142,112,463,400,108,151,129,60,49,139,221,123],"Top20Tags":[126,455,112,422,240,419,421,420,424,219,331,32,301,73,463,233,239,327,72,99],"Citation":"Lee, Y., Lee, S., & Lee, D. (2021). Wearable Haptic Device for Stiffness Rendering of Virtual Objects in Augmented Reality. Applied Sciences, 11(15), 6932. https:\/\/doi.org\/10.3390\/app11156932\n"},{"Title":"Designing Procedure Execution Tools with Emerging Technologies for Future Astronauts","DOI":"10.3390\/app11041607","Publication year":2021,"Abstract":"NASA's human spaceflight efforts are moving towards long-duration exploration missions requiring asynchronous communication between onboard crew and an increasingly remote ground support. In current missions aboard the International Space Station, there is a near real-time communication loop between Mission Control Center and astronauts. This communication is essential today to support operations, maintenance, and science requirements onboard, without which many tasks would no longer be feasible. As NASA takes the next leap into a new era of human space exploration, new methods and tools compensating for the lack of continuous, real-time communication must be explored. The Human-Computer Interaction Group at NASA Ames Research Center has been investigating emerging technologies and their applicability to increase crew autonomy in missions beyond low Earth orbit. Interactions using augmented reality and the Internet of Things have been researched as possibilities to facilitate usability within procedure execution operations. This paper outlines four research efforts that included technology demonstrations and usability studies with prototype procedure tools implementing emerging technologies. The studies address habitat feedback integration, analogous procedure testing, task completion management, and crew training. Through these technology demonstrations and usability studies, we find that low- to medium-fidelity prototypes, evaluated early in the design process, are both effective for garnering stakeholder buy-in and developing requirements for future systems. In this paper, we present the findings of the usability studies for each project and discuss ways in which these emerging technologies can be integrated into future human spaceflight operations.","LowLevel":"aerospace computing;human computer interaction;internet of things;space research;space vehicles","MidLevel":"education;internet of things;automotive;medical;aviation and aerospace;human-computer interaction;networks","HighLevel":"industries;technology;end users and user experience","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[118,101,408,64,219,205,175,179,374,171,110,448,86,359,176,366,210,136,432,159],"Top20Abs":[118,408,64,101,205,219,448,179,86,171,176,374,224,432,136,388,366,368,159,77],"Top20Tags":[129,175,258,65,123,101,38,110,64,220,289,31,298,271,121,118,359,378,266,472],"Citation":"Karasinski, J. A., Torron Valverde, I. C., Brosnahan, H. L., Gale, J. W., Kim, R., Yashar, M., & Marquez, J. J. (2021). Designing Procedure Execution Tools with Emerging Technologies for Future Astronauts. Applied Sciences, 11(4), 1607. https:\/\/doi.org\/10.3390\/app11041607\n"},{"Title":"Recording a hologram transmitted over a communication channel on one sideband","DOI":"10.3390\/app112311468","Publication year":2021,"Abstract":"The paper presents experimental results on the recording and restoration of 3D holographic frames suitable for transmitting 3D holographic images with the frame rate required for TV images and a resolution of the Full HD standard and higher. The Patent RF No. 2707582 proposed a method for compressing holographic information and transmitting it using a procedure similar to SSB over conventional communication channels. In this work, the holographic information of a 3D portrait of a person, transmitted and received via the Wi-Fi communication channel, was restored in the form of a rainbow hologram, as one of a variety of holograms, by the computer addition of the carrier spatial frequency, and then hologram was actually produced on a photoresist. This technology can be used to create a holographic phototelegraph or, if there is a dynamic holographic display, to create a holographic television and 3D augmented reality.","LowLevel":"holography;image coding;image resolution;image restoration;telecommunication channels;wireless lan","MidLevel":"computer vision;graphics;construction;input;human-computer interaction;networks;other;display technology","HighLevel":"displays;end users and user experience;industries;technology;other","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[344,225,416,235,278,228,74,332,431,231,413,290,300,424,149,307,351,469,162,317],"Top20Abs":[344,225,235,278,228,74,416,332,431,149,307,300,290,413,351,469,317,201,231,424],"Top20Tags":[416,225,424,132,345,450,130,106,309,72,307,422,255,415,110,370,190,242,99,266],"Citation":"Shoydin, S., Odinokov, S., Pazoev, A., Tsyganov, I., & Drozdova, E. (2021). Recording a Hologram Transmitted over a Communication Channel on One Sideband. Applied Sciences, 11(23), 11468. https:\/\/doi.org\/10.3390\/app112311468\n"},{"Title":"Cross-Modal Sentiment Sensing with Visual-Augmented Representation and Diverse Decision Fusion","DOI":"10.3390\/s22010074","Publication year":2021,"Abstract":"The rising use of online media has changed the social customs of the public. Users have become accustomed to sharing daily experiences and publishing personal opinions on social networks. Social data carrying emotion and attitude has provided significant decision support for numerous tasks in sentiment analysis. Conventional methods for sentiment classification only concern textual modality and are vulnerable to the multimodal scenario, while common multimodal approaches only focus on the interactive relationship among modalities without considering unique intra-modal information. A hybrid fusion network is proposed in this paper to capture both inter-modal and intra-modal features. Firstly, in the stage of representation fusion, a multi-head visual attention is proposed to extract accurate semantic and sentimental information from textual contents, with the guidance of visual features. Then, multiple base classifiers are trained to learn independent and diverse discriminative information from different modal representations in the stage of decision fusion. The final decision is determined based on fusing the decision supports from base classifiers via a decision fusion method. To improve the generalization of our hybrid fusion network, a similarity loss is employed to inject decision diversity into the whole model. Empiric results on five multimodal datasets have demonstrated that the proposed model achieves higher accuracy and better generalization capacity for multimodal sentiment analysis.","LowLevel":"emotion recognition;feature extraction;sentiment analysis;social networking","MidLevel":"human factors;computer vision;collaboration;chemical;input","HighLevel":"industries;technology;use cases;end users and user experience","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[148,467,468,81,476,202,426,341,318,132,166,326,274,86,458,179,485,451,283,381],"Top20Abs":[467,148,468,81,476,426,341,86,318,458,166,326,485,202,132,177,451,469,480,175],"Top20Tags":[193,396,283,249,219,98,71,368,164,387,313,381,172,76,132,363,318,182,213,81],"Citation":"Zhang, S., Li, B., & Yin, C. (2021). Cross-Modal Sentiment Sensing with Visual-Augmented Representation and Diverse Decision Fusion. Sensors, 22(1), 74. https:\/\/doi.org\/10.3390\/s22010074\n"},{"Title":"Interoperability between Real and Virtual Environments Connected by a GAN for the Path-Planning Problem","DOI":"10.3390\/app112110445","Publication year":2021,"Abstract":"Path planning is a fundamental issue in robotic systems because it requires coordination between the environment and an agent. The path-planning generator is composed of two modules: perception and planning. The first module scans the environment to determine the location, detect obstacles, estimate objects in motion, and build the planner module's restrictions. On the other hand, the second module controls the flight of the system. This process is computationally expensive and requires adequate performance to avoid accidents. For this reason, we propose a novel solution to improve conventional robotic systems' functions, such as systems having a small-capacity battery, a restricted size, and a limited number of sensors, using fewer elements. A navigation dataset was generated through a virtual simulator and a generative adversarial network to connect the virtual and real environments under an end-to-end approach. Furthermore, three path generators were analyzed using deep-learning solutions: a deep convolutional neural network, hierarchical clustering, and an auto-encoder. Since the path generators share a characteristic vector, transfer learning approaches complex problems by using solutions with fewer features, minimizing the costs and optimizing the resources of conventional system architectures, thus improving the limitations with respect to the implementation in embedded devices. Finally, a visualizer applying augmented reality was used to display the path generated by the proposed system.","LowLevel":"collision avoidance;data visualization;intelligent robots;learning algorithms;mobile robots;motion control;neural networks;path planning","MidLevel":"artificial intelligence;automotive;data;medical;robotics;business planning and management;manufacturing","HighLevel":"industries;technology;business","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[381,116,160,174,144,178,356,465,291,428,101,12,353,407,480,418,371,341,379,432],"Top20Abs":[381,116,174,356,291,178,160,480,371,144,465,418,407,139,18,341,175,432,345,255],"Top20Tags":[399,420,381,60,144,379,116,19,398,4,347,322,396,101,190,172,15,269,382,418],"Citation":"Maldonado-Romo, J., & Aldape-P\u00e9rez, M. (2021). Interoperability between Real and Virtual Environments Connected by a GAN for the Path-Planning Problem. Applied Sciences, 11(21), 10445. https:\/\/doi.org\/10.3390\/app112110445\n"},{"Title":"Batik AR ver.1.0: Augmented Reality application as gamification of batik design using waterfall method","DOI":"10.1088\/1742-6596\/1987\/1\/012021","Publication year":2021,"Abstract":"During the Covid-19 pandemic, it's time to implement asynchronous learning. The problem is, elementary school students have difficulty learning cultural arts, especially batik design. This study aims to create an application base on augmented reality (AR) with batik design content that can be used to introduce batik design as cultural art to elementary school students. The method used in developing this application is a waterfall which consists of (1) feasibility study, (2) requirements, (3) System design, (4) Encoding, (5) Testing system, (6) Acceptance Test. The results of this study were the validators rated more than 85% of media validation consisting of the quality of visual and auditory perceptions, ease of interaction, ease of interaction, and ease of use, while user responses consisted of 80% efficiency, 85% usability, 78% cognitive absorption, and enjoyment. 87%. This implies that the AR application can provide benefits for students who learn cultural sessions, especially batik design.","LowLevel":"cognition;computer aided instruction;diseases;educational institutions;epidemics;serious games","MidLevel":"education;training;human factors;simulation;medical","HighLevel":"industries;use cases;end users and user experience","Venue":"J. Phys., Conf. Ser. (UK)","Top20AbsAndTags":[114,405,87,171,163,275,320,369,236,336,136,9,128,306,196,39,16,134,262,51],"Top20Abs":[114,405,163,275,369,87,128,9,136,39,16,320,336,207,77,88,306,418,118,410],"Top20Tags":[113,196,204,236,171,51,391,124,405,262,363,16,87,320,400,61,33,306,117,150],"Citation":"Sobandi, B., Wibawa, S. C., Triyanto, T., Syakir, S., Pandanwangi, A., Suryadi, S., Nursalim, A., & Santosa, H. (2021). Batik AR ver.1.0: Augmented Reality application as gamification of batik design using waterfall method. Journal of Physics: Conference Series, 1987(1), 012021. https:\/\/doi.org\/10.1088\/1742-6596\/1987\/1\/012021\n"},{"Title":"Prototype of augmented reality technology for orthodontic bracket positioning: an in vivo study","DOI":"10.3390\/app11052315","Publication year":2021,"Abstract":"To improve the accuracy of bracket placement in vivo, a protocol and device were introduced, which consisted of operative procedures for accurate control, a computer-aided design, and an augmented reality-assisted bracket navigation system. The present study evaluated the accuracy of this protocol. Methods: Thirty-one incisor teeth were tested from four participators. The teeth were bonded by novice and expert orthodontists. Compared with the control group by Boone gauge and the experiment group by augmented reality-assisted bracket navigation system, our study used for brackets measurement. To evaluate the accuracy, deviations of positions for bracket placement were measured. Results: The augmented reality-assisted bracket navigation system and control group were used in the same 31 cases. The priority of bonding brackets between control group or experiment group was decided by tossing coins, and then the teeth were debonded and the other technique was used. The medium vertical (incisogingival) position deviation in the control and AR groups by the novice orthodontist was 0.90 &#177; 0.06 mm and 0.51 &#177; 0.24 mm, respectively (p &lt; 0.05), and by the expert orthodontist was 0.40 &#177; 0.29 mm and 0.29 &#177; 0.08 mm, respectively (p &lt; 0.05). No significant changes in the horizontal position deviation were noted regardless of the orthodontist experience or use of the augmented reality-assisted bracket navigation system. Conclusion: The augmented reality-assisted bracket navigation system increased the accuracy rate by the expert orthodontist in the incisogingival direction and helped the novice orthodontist guide the bracket position within an acceptable clinical error of approximately 0.5 mm.","LowLevel":"biomechanics;dentistry;medical computing;orthotics;patient treatment","MidLevel":"medical","HighLevel":"industries","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[152,320,303,43,139,70,49,63,183,171,383,91,186,326,432,191,33,488,277,165],"Top20Abs":[152,303,43,320,139,49,70,171,383,183,63,326,277,165,91,186,488,33,191,432],"Top20Tags":[397,417,210,91,294,186,108,42,394,376,47,342,140,152,61,384,262,278,413,9],"Citation":"Lo, Y.-C., Chen, G.-A., Liu, Y.-C., Chen, Y.-H., Hsu, J.-T., & Yu, J.-H. (2021). Prototype of Augmented Reality Technology for Orthodontic Bracket Positioning: An In Vivo Study. Applied Sciences, 11(5), 2315. https:\/\/doi.org\/10.3390\/app11052315\n"},{"Title":"CultReal&#8212;A Rapid Development Platform for AR Cultural Spaces, with Fused Localization","DOI":"10.3390\/s21196618","Publication year":2021,"Abstract":"Virtual and augmented reality technologies have known an impressive market evolution due to their potential to provide immersive experiences. However, they still have significant difficulties to enable fully fledged, consumer-ready applications that can handle complex tasks such as multi-user collaboration or time-persistent experiences. In this context, CultReal is a rapid creation and deployment platform for augmented reality (AR), aiming to revitalize cultural spaces. The platform's content management system stores a representation of the environment, together with a database of multimedia objects that can be associated with a location. The localization component fuses data from beacons and from video cameras, providing an accurate estimation of the position and orientation of the visitor's smartphone. A mobile application running the localization component displays the augmented content, which is seamlessly integrated with the real world. The paper focuses on the series of steps required to compute the position and orientation of the user's mobile device, providing a comprehensive evaluation with both virtual and real data. Pilot implementations of the system are also described in the paper, revealing the potential of the platform to enable rapid deployment in new cultural spaces. Offering these functionalities, CultReal will allow for the fast development of AR solutions in any location.","LowLevel":"content management;groupware;mobile computing","MidLevel":"telecommunication;collaboration;developers","HighLevel":"industries;technology;use cases","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[31,197,316,17,440,51,302,203,279,139,156,151,82,16,142,386,419,132,109,41],"Top20Abs":[316,440,31,259,51,251,158,16,302,197,425,41,365,17,82,63,194,139,210,142],"Top20Tags":[279,29,219,17,203,119,104,331,483,303,304,93,240,164,131,199,239,36,42,200],"Citation":"Morar, A., B\u0103lu\u021boiu, M.-A., Moldoveanu, A., Moldoveanu, F., & Butean, A. (2021). CultReal\u2014A Rapid Development Platform for AR Cultural Spaces, with Fused Localization. Sensors, 21(19), 6618. https:\/\/doi.org\/10.3390\/s21196618\n"},{"Title":"Assessment of a Location-Based Mobile Augmented-Reality Game by Adult Users with the ARCS Model","DOI":"10.3390\/app11146448","Publication year":2021,"Abstract":"In mobile augmented reality (MAR) games, learning by doing is important to supplement the theoretical knowledge with practical exercise in order to maximize the learning outcome. However, in many fields, the users are not able to apply their knowledge in practical ways, despite having achieved a good understanding of the theoretical fundamentals and this is even more important to adult learners. The aim of this research is to examine young, middle-aged and elderly adults' opinions about the location-based MAR game Ingress, by applying John Keller's \"ARCS learning motivation model\" (Attention, Relevance, Confidence and Satisfaction). The users' responses to closed questions related to Ingress were collected from 45 adult players aged 20-60 from Greece and were subsequently analyzed by means of pre- and post-quantitative measures of the four ARCS factors. The results show that: (a) game training improves all the factors of ARCS, primarily attention and satisfaction; (b) the responses of young people (20-35) agree more with those of elderly adults (&gt;52) than with those of the intermediate age group of 36-51. Our findings, therefore, highlight the potential and the applicability of the ARCS model in MAR games.","LowLevel":"computer aided instruction;computer games;human factors;mobile computing;mobile handsets","MidLevel":"training;human factors;telecommunication;liberal arts;input","HighLevel":"end users and user experience;technology;industries;use cases","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[164,36,185,124,111,83,369,30,264,22,357,236,320,400,79,196,57,113,51,72],"Top20Abs":[164,36,124,185,111,369,264,22,83,357,400,330,91,30,320,113,33,72,140,79],"Top20Tags":[164,204,410,197,3,14,51,29,111,93,30,196,254,82,145,97,320,13,339,376],"Citation":"Sdravopoulou, K., Mu\u00f1oz Gonz\u00e1lez, J. M., & Hidalgo-Ariza, M. D. (2021). Assessment of a Location-Based Mobile Augmented-Reality Game by Adult Users with the ARCS Model. Applied Sciences, 11(14), 6448. https:\/\/doi.org\/10.3390\/app11146448\n"},{"Title":"Toward Baggage-free airport terminals: a case study of London City Airport","DOI":"10.3390\/su14010212","Publication year":2021,"Abstract":"Nowadays, the aviation industry pays more attention to emission reduction toward the net-zero carbon goals. However, the volume of global passengers and baggage is exponentially increasing, which leads to challenges for sustainable airports. A baggage-free airport terminal is considered a potential solution in solving this issue. Removing the baggage operation away from the passenger terminals will reduce workload for airport operators and promote passengers to use public transport to airport terminals. As a result, it will bring a significant impact on energy and the environment, leading to a reduction of fuel consumption and mitigation of carbon emission. This paper studies a baggage collection network design problem using vehicle routing strategies and augmented reality for baggage-free airport terminals. We use a spreadsheet solver tool, based on the integration of the modified Clark and Wright savings heuristic and density-based clustering algorithm, for optimizing the location of logistic hubs and planning the vehicle routes for baggage collection. This tool is applied for the case study at London City Airport to analyze the impacts of the strategies on carbon emission quantitatively. The result indicates that the proposed baggage collection network can significantly reduce 290.10 tonnes of carbon emissions annually.","LowLevel":"air pollution control;airports;logistics;optimization;pattern clustering;public transport;spreadsheet programs;sustainable development;vehicle routing","MidLevel":"cultural heritage;policy;automotive;computer vision;data;aviation and aerospace;transportation;business performance metrics;developers;other;logistics","HighLevel":"industries;technology;business;other","Venue":"Sustainability (Switzerland)","Top20AbsAndTags":[334,22,310,258,374,311,469,106,261,179,264,268,236,67,24,307,404,447,158,458],"Top20Abs":[22,310,258,469,374,268,179,307,261,106,447,458,158,24,86,401,426,294,67,84],"Top20Tags":[374,28,334,269,288,346,355,254,236,67,426,286,106,264,378,64,101,15,166,156],"Citation":"Jiang, Y., Yang, R., Zang, C., Wei, Z., Thompson, J., Tran, T. H., Encinas-Oropesa, A., & Williams, L. (2021). Toward Baggage-Free Airport Terminals: A Case Study of London City Airport. Sustainability, 14(1), 212. https:\/\/doi.org\/10.3390\/su14010212\n"},{"Title":"Multi-Robot Preemptive Task Scheduling with Fault Recovery: A Novel Approach to Automatic Logistics of Smart Factories","DOI":"10.3390\/s21196536","Publication year":2021,"Abstract":"This paper presents a novel approach for Multi-Robot Task Allocation (MRTA) that introduces priority policies on preemptive task scheduling and considers dependencies between tasks, and tolerates faults. The approach is referred to as Multi-Robot Preemptive Task Scheduling with Fault Recovery (MRPF). It considers the interaction between running processes and their tasks for management at each new event, prioritizing the more relevant tasks without idleness and latency. The benefit of this approach is the optimization of production in smart factories, where autonomous robots are being employed to improve efficiency and increase flexibility. The evaluation of MRPF is performed through experimentation in small-scale warehouse logistics, referred to as Augmented Reality to Enhanced Experimentation in Smart Warehouses (ARENA). An analysis of priority scheduling, task preemption, and fault recovery is presented to show the benefits of the proposed approach.","LowLevel":"logistics;mobile robots;multi-robot systems;scheduling;task analysis;warehouse automation","MidLevel":"education;human factors;robotics;developers;manufacturing;logistics","HighLevel":"industries;technology;business;end users and user experience","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[334,19,264,174,257,195,307,242,388,399,360,371,322,467,361,94,352,432,469,172],"Top20Abs":[19,334,264,307,174,257,242,195,388,371,94,360,352,399,172,432,361,151,303,107],"Top20Tags":[334,378,60,282,399,355,273,102,19,322,487,478,467,156,140,379,205,120,396,429],"Citation":"Kalempa, V. C., Piardi, L., Limeira, M., & de Oliveira, A. S. (2021). Multi-Robot Preemptive Task Scheduling with Fault Recovery: A Novel Approach to Automatic Logistics of Smart Factories. Sensors, 21(19), 6536. https:\/\/doi.org\/10.3390\/s21196536\n"},{"Title":"The role of augmented reality-based unplugged computer programming approach in the effectiveness of computational thinking","DOI":"10.1504\/IJMLO.2021.116506","Publication year":2021,"Abstract":"Recently, pedagogical approaches to promote computational thinking among 21st century learners have increasingly become a topic of interest in educational research. Studies revealed that unplugged computer programming, learning coding without a computer, can enhance students' computational thinking and positive attitudes towards computer programming. However, when dealing with complex computer science concepts, they face limitations. This study therefore developed a solution adopting unplugged coding using flowcharts integrated with Augmented Reality (AR) technology to help scaffold students (called AR semi-unplugged coding). It recruited 120 secondary students for data collection, divided into an experimental group and a control group (unplugged coding). It showed that although both exhibited improved computational thinking and increased self-efficacy after participating in their assigned activity, those in the experimental group outperformed their counterparts statistically. Moreover, the developed solution attracted a greater level of perceived usefulness by users which shows its potential to become an effective approach for promoting computational thinking.","LowLevel":"computer aided instruction;computer science education;educational courses;human factors","MidLevel":"education;human factors;developers;training","HighLevel":"industries;technology;use cases;end users and user experience","Venue":"Int. J. Mob. Learn. Organ. (Switzerland)","Top20AbsAndTags":[185,171,232,320,369,128,383,62,88,342,410,312,87,163,57,134,272,204,47,306],"Top20Abs":[185,369,171,232,128,383,320,312,342,410,88,62,163,87,115,82,188,272,306,204],"Top20Tags":[88,232,62,124,57,87,185,128,134,196,204,383,33,114,195,241,145,405,147,410],"Citation":"Threekunprapa, A., & Yasri, P. (2021). The role of augmented reality-based unplugged computer programming approach in the effectiveness of computational thinking. International Journal of Mobile Learning and Organisation, 15(3), 233. https:\/\/doi.org\/10.1504\/ijmlo.2021.116506\n"},{"Title":"A UWB-Driven Self-Actuated Projector Platform for Interactive Augmented Reality Applications","DOI":"10.3390\/app11062871","Publication year":2021,"Abstract":"With the rapid development of interactive technology, creating systems that allow users to define their interactive envelope freely and provide multi-interactive modalities is important to build up an intuitive interactive space. We present an indoor interactive system where a human can customize and interact through a projected screen utilizing the surrounding surfaces. An ultra-wideband (UWB) wireless sensor network was used to assist human-centered interaction design and navigate the self-actuated projector platform. We developed a UWB-based calibration algorithm to facilitate the interaction with the customized projected screens, where a hand-held input device was designed to perform mid-air interactive functions. Sixteen participants were recruited to evaluate the system performance. A prototype level implementation was tested inside a simulated museum environment, where a self-actuated projector provides interactive explanatory content for the on-display artifacts under the user's command. Our results depict the applicability to designate the interactive screen efficiently indoors and interact with the augmented content with reasonable accuracy and relatively low workload. Our findings also provide valuable user experience information regarding the design of mobile and projection-based augmented reality systems, with the ability to overcome the limitations of other conventional techniques.","LowLevel":"calibration;museums;optical projectors;wireless sensor networks","MidLevel":"cultural heritage;networks;sensors;optics","HighLevel":"industries;technology;displays","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[109,422,433,55,118,66,393,215,419,224,472,440,175,53,316,267,16,409,75,338],"Top20Abs":[109,55,419,224,440,118,175,215,422,248,75,267,53,309,410,412,409,338,433,205],"Top20Tags":[220,302,105,229,18,472,77,433,293,109,66,173,16,393,236,415,465,23,445,450],"Citation":"Elsharkawy, A., Naheem, K., Koo, D., & Kim, M. S. (2021). A UWB-Driven Self-Actuated Projector Platform for Interactive Augmented Reality Applications. Applied Sciences, 11(6), 2871. https:\/\/doi.org\/10.3390\/app11062871\n"},{"Title":"BIM-Based Digital Twin and XR Devices to Improve Maintenance Procedures in Smart Buildings: A Literature Review","DOI":"10.3390\/app11156810","Publication year":2021,"Abstract":"In recent years, the use of digital twins (DT) to improve maintenance procedures has increased in various industrial sectors (e.g., manufacturing, energy industry, aerospace) but is more limited in the construction industry. However, the operation and maintenance (O&amp;M) phase of a building's life cycle is the most expensive. Smart buildings already use BIM (Building Information Modeling) for facility management, but they lack the predictive capabilities of DT. On the other hand, the use of extended reality (XR) technologies to improve maintenance operations has been a major topic of academic research in recent years, both through data display and remote collaboration. In this context, this paper focuses on reviewing projects using a combination of these technologies to improve maintenance operations in smart buildings. This review uses a combination of at least three of the terms \"Digital Twin\", \"Maintenance\", \"BIM\" and \"Extended Reality\". Results show how a BIM can be used to create a DT and how this DT use combined with XR technologies can improve maintenance operations in a smart building. This paper also highlights the challenges for the correct implementation of a BIM-based DT combined with XR devices. An example of use is also proposed using a diagram of the possible interactions between the user, the DT and the application framework during maintenance operations.","LowLevel":"building information modelling;construction industry;maintenance engineering","MidLevel":"manufacturing;construction","HighLevel":"industries","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[122,86,181,146,315,261,35,131,412,374,179,360,190,395,257,314,308,67,106,63],"Top20Abs":[122,315,86,181,261,146,131,374,35,179,360,395,412,257,190,63,314,67,376,106],"Top20Tags":[35,146,86,181,412,190,261,131,144,179,314,308,459,370,106,68,360,305,237,67],"Citation":"Coupry, C., Noblecourt, S., Richard, P., Baudry, D., & Bigaud, D. (2021). BIM-Based Digital Twin and XR Devices to Improve Maintenance Procedures in Smart Buildings: A Literature Review. Applied Sciences, 11(15), 6810. https:\/\/doi.org\/10.3390\/app11156810\n"},{"Title":"Virtual Reality (VR) Simulation and Augmented Reality (AR) Navigation in Orthognathic Surgery: A Case Report","DOI":"10.3390\/app11125673","Publication year":2021,"Abstract":"VR and AR technology have gradually developed to the extent that they could help operators in the surgical field. In this study, we present a case of VR simulation for preoperative planning and AR navigation applied to orthognathic surgery. The average difference between the preplanned data and the post-operative results was 3.00 mm, on average, and the standard deviation was 1.44 mm. VR simulation could provide great advantages for 3D medical simulations, with accurate manipulation and immersiveness. AR navigation has great potential in medical application; its advantages include displaying real time augmented 3D models of patients. Moreover, it is easily applied in the surgical field, without complicated 3D simulations or 3D-printed surgical guides.","LowLevel":"medical computing;surgery","MidLevel":"medical","HighLevel":"industries","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[294,326,356,186,234,464,210,69,212,91,168,301,200,189,49,139,463,191,120,60],"Top20Abs":[294,326,356,212,234,186,49,301,464,200,139,210,463,189,69,316,60,433,87,120],"Top20Tags":[168,356,294,326,108,376,91,234,384,186,42,408,133,61,210,262,47,397,9,140],"Citation":"Jo, Y.-J., Choi, J.-S., Kim, J., Kim, H.-J., & Moon, S.-Y. (2021). Virtual Reality (VR) Simulation and Augmented Reality (AR) Navigation in Orthognathic Surgery: A Case Report. Applied Sciences, 11(12), 5673. https:\/\/doi.org\/10.3390\/app11125673\n"},{"Title":"Augmented Reality Visualization of Modal Analysis Using the Finite Element Method","DOI":"10.3390\/app11031310","Publication year":2021,"Abstract":"Modal analysis provides the dynamic behavior of an object or structure, and is often undertaken using the Finite Element Method (FEM) due to its ability to deal with arbitrary geometries. This article investigates the use of Augmented Reality (AR) to provide the in situ visualization of a modal analysis for an aluminum impeller. Finite Element Analysis (FEA) software packages regularly use heat maps and shape deformation to visualize the outcomes of a given simulation. AR allows the superimposition of digital information on a view of the real-world environment, and provides the opportunity to overlay such simulation results onto real-world objects and environments. The presented modal analysis undertaken herein provides natural frequencies and the corresponding deformation of an aluminum impeller. The results indicate the ability for the design part and finite element analysis results to be viewed on the physical part. A mobile AR-FEA-based system was developed for Modal Analysis result visualization. This study offers designers and engineers a new way to visualize such simulation results.","LowLevel":"data visualization;deformation;finite element analysis;impellers;modal analysis","MidLevel":"graphics;data;developers;other","HighLevel":"technology;other","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[301,7,402,178,212,142,120,157,406,440,136,139,214,294,4,144,69,35,288,374],"Top20Abs":[7,301,402,212,178,142,120,157,440,139,136,294,406,214,49,144,207,164,374,82],"Top20Tags":[92,234,72,422,4,410,71,48,356,415,99,309,300,130,345,370,98,198,80,101],"Citation":"Yavuz Erkek, M., Erkek, S., Jamei, E., Seyedmahmoudian, M., Stojcevski, A., & Horan, B. (2021). Augmented Reality Visualization of Modal Analysis Using the Finite Element Method. Applied Sciences, 11(3), 1310. https:\/\/doi.org\/10.3390\/app11031310\n"},{"Title":"Creation of auditory augmented rea lity using a position-dynamic binaural synthesis system&#8212;technical components, psychoacoustic needs, and perceptual evaluation","DOI":"10.3390\/app11031150","Publication year":2021,"Abstract":"For a spatial audio reproduction in the context of augmented reality, a position-dynamic binaural synthesis system can be used to synthesize the ear signals for a moving listener. The goal is the fusion of the auditory perception of the virtual audio objects with the real listening environment. Such a system has several components, each of which help to enable a plausible auditory simulation. For each possible position of the listener in the room, a set of binaural room impulse responses (BRIRs) congruent with the expected auditory environment is required to avoid room divergence effects. Adequate and efficient approaches are methods to synthesize new BRIRs using very few measurements of the listening room. The required spatial resolution of the BRIR positions can be estimated by spatial auditory perception thresholds. Retrieving and processing the tracking data of the listener's head-pose and position as well as convolving BRIRs with an audio signal needs to be done in real-time. This contribution presents work done by the authors including several technical components of such a system in detail. It shows how the single components are affected by psychoacoustics. Furthermore, the paper also discusses the perceptive effect by means of listening tests demonstrating the appropriateness of the approaches.","LowLevel":"acoustic signal processing;architectural acoustics;audio signal processing;hearing;signal synthesis;transient response","MidLevel":"human factors;audio;data;medical;construction;sensors;other","HighLevel":"industries;technology;other;end users and user experience","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[267,299,59,217,337,316,259,208,425,139,287,250,437,120,145,170,162,433,194,440],"Top20Abs":[267,59,208,259,299,139,425,316,217,287,337,162,291,440,433,170,142,437,226,120],"Top20Tags":[299,217,230,455,428,253,250,384,337,417,165,108,178,90,13,284,18,127,130,421],"Citation":"Werner, S., Klein, F., Neidhardt, A., Sloma, U., Schneiderwind, C., & Brandenburg, K. (2021). Creation of Auditory Augmented Reality Using a Position-Dynamic Binaural Synthesis System\u2014Technical Components, Psychoacoustic Needs, and Perceptual Evaluation. Applied Sciences, 11(3), 1150. https:\/\/doi.org\/10.3390\/app11031150\n"},{"Title":"Scalable Extended Reality: A Future Research Agenda","DOI":"10.3390\/bdcc6010012","Publication year":2021,"Abstract":"Extensive research has outlined the potential of augmented, mixed, and virtual reality applications. However, little attention has been paid to scalability enhancements fostering practical adoption. In this paper, we introduce the concept of scalable extended reality (XRS), i.e., spaces scaling between different displays and degrees of virtuality that can be entered by multiple, possibly distributed users. The development of such XRSspaces concerns several research fields. To provide bidirectional interaction and maintain consistency with the real environment, virtual reconstructions of physical scenes need to be segmented semantically and adapted dynamically. Moreover, scalable interaction techniques for selection, manipulation, and navigation as well as a world-stabilized rendering of 2D annotations in 3D space are needed to let users intuitively switch between handheld and head-mounted displays. Collaborative settings should further integrate access control and awareness cues indicating the collaborators' locations and actions. While many of these topics were investigated by previous research, very few have considered their integration to enhance scalability. Addressing this gap, we review related previous research, list current barriers to the development of XRSspaces, and highlight dependencies between them.","LowLevel":"groupware;helmet mounted displays;human computer interaction;mobile computing","MidLevel":"telecommunication;collaboration;wearables;human-computer interaction;display technology","HighLevel":"displays;industries;use cases;end users and user experience","Venue":"Big Data Cogn. Comput. (Switzerland)","Top20AbsAndTags":[80,440,75,304,424,419,248,243,203,267,410,237,38,364,301,436,252,211,17,205],"Top20Abs":[304,440,80,419,75,248,252,410,388,243,226,237,177,333,436,64,228,208,205,424],"Top20Tags":[80,259,29,75,424,21,364,180,69,337,238,206,203,38,104,267,217,339,17,239],"Citation":"Memmesheimer, V. M., & Ebert, A. (2022). Scalable Extended Reality: A Future Research Agenda. Big Data and Cognitive Computing, 6(1), 12. https:\/\/doi.org\/10.3390\/bdcc6010012\n"},{"Title":"Ultrasound for Gaze Estimation&#8212;A Modeling and Empirical Study","DOI":"10.3390\/s21134502","Publication year":2021,"Abstract":"Most eye tracking methods are light-based. As such, they can suffer from ambient light changes when used outdoors, especially for use cases where eye trackers are embedded in Augmented Reality glasses. It has been recently suggested that ultrasound could provide a low power, fast, light-insensitive alternative to camera-based sensors for eye tracking. Here, we report on our work on modeling ultrasound sensor integration into a glasses form factor AR device to evaluate the feasibility of estimating eye-gaze in various configurations. Next, we designed a benchtop experimental setup to collect empirical data on time of flight and amplitude signals for reflected ultrasound waves for a range of gaze angles of a model eye. We used this data as input for a low-complexity gradient-boosted tree machine learning regression model and demonstrate that we can effectively estimate gaze (gaze RMSE error of 0.965 &#177; 0.178 degrees with an adjusted R2 score of 90.2 &#177; 4.6).","LowLevel":"cameras;eye;gradient methods;human computer interaction;learning algorithms;regression analysis","MidLevel":"artificial intelligence;human-computer interaction;medical;input","HighLevel":"end users and user experience;technology;industries","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[32,259,250,123,237,48,413,243,201,72,173,433,260,323,376,481,182,111,332,141],"Top20Abs":[32,259,250,123,201,237,323,376,433,173,182,413,48,220,481,141,445,111,260,486],"Top20Tags":[32,172,259,72,144,243,439,214,321,71,415,425,339,230,308,58,422,352,379,396],"Citation":"Golard, A., & Talathi, S. S. (2021). Ultrasound for Gaze Estimation\u2014A Modeling and Empirical Study. Sensors, 21(13), 4502. https:\/\/doi.org\/10.3390\/s21134502\n"},{"Title":"Applications of Smart Glasses in Applied Sciences: A Systematic Review","DOI":"10.3390\/app11114956","Publication year":2021,"Abstract":"The aim of this study is to review academic papers on the applications of smart glasses. Among 82 surveyed papers, 57 were selected through filtering. The papers were published from January 2014 to October 2020. Four research questions were set up using the systematic review method, and conclusions were drawn focusing on the research trends by year and application fields; product and operating system; sensors depending on the application purpose; and data visualization, processing, and transfer methods. It was found that the most popular commercial smart glass products are Android-based Google products. In addition, smart glasses are most often used in the healthcare field, particularly for clinical and surgical assistance or for assisting mentally or physically disabled persons. For visual data transfer, 90% of the studies conducted used a camera sensor. Smart glasses have mainly been used to visualize data based on augmented reality, in contrast with the use of mixed reality. The results of this review indicate that research related to smart glasses is steadily increasing, and technological research into the development of smart glasses is being actively conducted.","LowLevel":"computer aided instruction;data visualization;handicapped aids;health care;internet;medical computing;mobile computing;surgery","MidLevel":"training;telecommunication;medical;data;networks","HighLevel":"industries;technology;use cases","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[182,89,228,310,281,49,5,33,224,110,294,388,4,38,352,277,168,56,170,46],"Top20Abs":[182,89,310,281,49,228,224,277,110,412,33,5,388,18,352,38,12,389,370,429],"Top20Tags":[42,93,410,82,9,89,140,327,262,390,117,163,164,56,222,367,147,134,330,47],"Citation":"Kim, D., & Choi, Y. (2021). Applications of Smart Glasses in Applied Sciences: A Systematic Review. Applied Sciences, 11(11), 4956. https:\/\/doi.org\/10.3390\/app11114956\n"},{"Title":"Solving Tourism Management Challenges by Means of Mobile Augmented Reality Applications","DOI":"10.4018\/IJWLTT.293280","Publication year":2021,"Abstract":"The purpose of the present article was to examine the use of mobile augmented reality technologies in the process of planning and organizing tourist activities. The analysis of the attitude of TUI Showroom's clients and managers of Russian travel agencies to the application of immersive technologies in travel consulting unveiled several possibilities for the practical use of mobile augmented reality applications in the travel business. The study concludes with the opinion that stimulation of the client's interest in the historical and cultural context of the tour by providing additional argumentation and high-quality information on the marketing proposal in a new, unusual manner forms the cultural, epistemic, and educational values of augmented reality, necessary in sales, personnel training, and interaction with business partners.","LowLevel":"mobile computing;travel industry","MidLevel":"telecommunication;transportation","HighLevel":"industries","Venue":"Int. J. Web-Based Learn. Teach. Technol. (USA)","Top20AbsAndTags":[421,368,1,16,8,56,267,102,105,207,320,103,166,241,289,78,244,264,30,31],"Top20Abs":[421,368,16,1,207,30,267,289,56,8,102,105,103,320,436,244,31,155,264,78],"Top20Tags":[392,90,73,109,93,64,139,82,42,440,166,376,8,164,13,56,129,327,348,142],"Citation":"Ghandour, A., Kintonova, A., Demidchik, N., & Sverdlikova, E. (2021). Solving Tourism Management Challenges by Means of Mobile Augmented Reality Applications. International Journal of Web-Based Learning and Teaching Technologies, 16(6), 1\u201316. https:\/\/doi.org\/10.4018\/ijwltt.293280\n"},{"Title":"A Novel Mixed Reality Solution Based on Learning Environment for Sutures in Minor Surgery","DOI":"10.3390\/app11052335","Publication year":2021,"Abstract":"Minor Surgery Sutures is a fundamental skill for healthcare professionals. However, in the educational field, the practice of suturing is sometimes limited and reduced, with more theoretical than practical study. In order to facilitate learning, our goal is to develop an immersive and interactive educational tool that complements theoretical study, called Suture MR. This application could enhance suture procedural skills in the fields of nursing and medicine. Applying Mixed Reality techniques, we generate a 3D model of an arm with a full-scale wound. Realistically, the user will simulate the suture movements as part of the learning process. The application has surgical clamps and a needle holder that are virtually visualized in the user's hands, allowing gestures and movements faithful to the real ones. In this article, we want to demonstrate the usability of our environment and the feasibility of using Mixed Reality learning experiences in clinical practical training as a complement to theoretical training. The results of the study reveal a greater perception of learning and the willingness of students to use this methodology.","LowLevel":"computer aided instruction;surgery","MidLevel":"training;medical","HighLevel":"industries;use cases","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[163,128,408,171,207,47,400,436,103,306,375,46,147,57,87,320,62,112,88,414],"Top20Abs":[163,128,408,207,171,88,375,306,103,147,400,87,436,188,46,62,442,124,116,414],"Top20Tags":[9,376,204,93,33,34,145,405,367,336,3,82,320,17,23,222,262,230,232,46],"Citation":"Rojo, A., Raya, L., & Sanchez, A. (2021). A Novel Mixed Reality Solution Based on Learning Environment for Sutures in Minor Surgery. Applied Sciences, 11(5), 2335. https:\/\/doi.org\/10.3390\/app11052335\n"},{"Title":"Hybrid Simulation and Planning Platform for Cryosurgery with Microsoft HoloLens","DOI":"10.3390\/s21134450","Publication year":2021,"Abstract":"Cryosurgery is a technique of growing popularity involving tissue ablation under controlled freezing. Technological advancement of devices along with surgical technique improvements have turned cryosurgery from an experimental to an established option for treating several diseases. However, cryosurgery is still limited by inaccurate planning based primarily on 2D visualization of the patient's preoperative images. Several works have been aimed at modelling cryoablation through heat transfer simulations; however, most software applications do not meet some key requirements for clinical routine use, such as high computational speed and user-friendliness. This work aims to develop an intuitive platform for anatomical understanding and pre-operative planning by integrating the information content of radiological images and cryoprobe specifications either in a 3D virtual environment (desktop application) or in a hybrid simulator, which exploits the potential of the 3D printing and augmented reality functionalities of Microsoft HoloLens. The proposed platform was preliminarily validated for the retrospective planning\/simulation of two surgical cases. Results suggest that the platform is easy and quick to learn and could be used in clinical practice to improve anatomical understanding, to make surgical planning easier than the traditional method, and to strengthen the memorization of surgical planning.","LowLevel":"biological tissues;biothermics;diseases;freezing;medical image processing;surgery","MidLevel":"computer vision;medical;data;other","HighLevel":"industries;technology;other","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[294,152,326,234,69,186,418,168,475,401,397,82,488,464,285,178,463,244,403,376],"Top20Abs":[294,152,326,418,234,401,475,69,397,186,82,488,463,433,285,403,464,465,168,267],"Top20Tags":[326,234,71,294,69,152,186,48,178,108,425,168,143,112,417,376,255,230,148,370],"Citation":"Condino, S., Cutolo, F., Cattari, N., Colangeli, S., Parchi, P. D., Piazza, R., Ruinato, A. D., Capanna, R., & Ferrari, V. (2021). Hybrid Simulation and Planning Platform for Cryosurgery with Microsoft HoloLens. Sensors, 21(13), 4450. https:\/\/doi.org\/10.3390\/s21134450\n"},{"Title":"Smart Sensors for Augmented Electrical Experiments","DOI":"10.3390\/s22010256","Publication year":2021,"Abstract":"With the recent increase in the use of augmented reality (AR) in educational laboratory settings, there is a need for new intelligent sensor systems capturing all aspects of the real environment. We present a smart sensor system meeting these requirements for STEM (science, technology, engineering, and mathematics) experiments in electrical circuits. The system consists of custom experiment boxes and cables combined with an application for the Microsoft HoloLens 2, which creates an AR experiment environment. The boxes combine sensors for measuring the electrical voltage and current at the integrated electrical components as well as a reconstruction of the currently constructed electrical circuit and the position of the sensor box on a table. Combing these data, the AR application visualizes the measurement data spatially and temporally coherent to the real experiment boxes, thus fulfilling demands derived from traditional multimedia learning theory. Following an evaluation of the accuracy and precision of the presented sensors, the usability of the system was evaluated with n=20 pupils in a German high school. In this evaluation, the usability of the system was rated with a system usability score of 94 out of 100.","LowLevel":"computer aided instruction;electrical engineering computing;electrical engineering education;intelligent sensors;laboratories;multimedia computing;stem","MidLevel":"education;training;sensors;engineering","HighLevel":"industries;technology;use cases","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[89,136,182,412,118,171,445,18,134,83,262,109,220,342,332,45,180,57,372,125],"Top20Abs":[89,136,412,182,18,445,118,171,220,109,134,45,262,332,342,180,175,207,24,267],"Top20Tags":[125,39,89,284,6,204,33,18,262,229,17,9,272,83,93,246,171,52,196,342],"Citation":"Kapp, S., Lauer, F., Beil, F., Rheinl\u00e4nder, C. C., Wehn, N., & Kuhn, J. (2021). Smart Sensors for Augmented Electrical Experiments. Sensors, 22(1), 256. https:\/\/doi.org\/10.3390\/s22010256\n"},{"Title":"Dielectric Metalens: Properties and Three-Dimensional Imaging Applications","DOI":"10.3390\/s21134584","Publication year":2021,"Abstract":"Recently, optical dielectric metasurfaces, ultrathin optical skins with densely arranged dielectric nanoantennas, have arisen as next-generation technologies with merits for miniaturization and functional improvement of conventional optical components. In particular, dielectric metalenses capable of optical focusing and imaging have attracted enormous attention from academic and industrial communities of optics. They can offer cutting-edge lensing functions owing to arbitrary wavefront encoding, polarization tunability, high efficiency, large diffraction angle, strong dispersion, and novel ultracompact integration methods. Based on the properties, dielectric metalenses have been applied to numerous three-dimensional imaging applications including wearable augmented or virtual reality displays with depth information, and optical sensing of three-dimensional position of object and various light properties. In this paper, we introduce the properties of optical dielectric metalenses, and review the working principles and recent advances in three-dimensional imaging applications based on them. The authors envision that the dielectric metalens and metasurface technologies could make breakthroughs for a wide range of compact optical systems for three-dimensional display and sensing.","LowLevel":"lenses;light polarisation;nanophotonics;optical arrays;optical design techniques;optical focusing;optical metamaterials;plasmonics;skin;three-dimensional displays","MidLevel":"other;human factors;medical;chemical;input;optics;human-computer interaction;metals and mining;display technology","HighLevel":"displays;end users and user experience;industries;technology;other","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[317,158,278,169,450,444,235,225,431,141,228,291,74,70,422,445,76,255,332,105],"Top20Abs":[158,317,278,169,450,444,431,235,228,141,225,74,291,445,76,422,393,344,255,324],"Top20Tags":[293,422,225,98,107,332,255,364,235,218,201,70,158,149,143,300,278,71,415,105],"Citation":"Kim, S.-J., Kim, C., Kim, Y., Jeong, J., Choi, S., Han, W., Kim, J., & Lee, B. (2021). Dielectric Metalens: Properties and Three-Dimensional Imaging Applications. Sensors, 21(13), 4584. https:\/\/doi.org\/10.3390\/s21134584\n"},{"Title":"Personalized Augmented Reality Based Tourism System: Big Data and User Demographic Contexts","DOI":"10.3390\/app11136047","Publication year":2021,"Abstract":"A lack of required data resources is one of the challenges of accepting the Augmented Reality (AR) to provide the right services to the users, whereas the amount of spatial information produced by people is increasing daily. This research aims to design a personalized AR that is based on a tourist system that retrieves the big data according to the users' demographic contexts in order to enrich the AR data source in tourism. This research is conducted in two main steps. First, the type of the tourist attraction where the users interest is predicted according to the user demographic contexts, which include age, gender, and education level, by using a machine learning method. Second, the correct data for the user are extracted from the big data by considering time, distance, popularity, and the neighborhood of the tourist places, by using the VIKOR and SWAR decision making methods. By about 6%, the results show better performance of the decision tree by predicting the type of tourist attraction, when compared to the SVM method. In addition, the results of the user study of the system show the overall satisfaction of the participants in terms of the ease-of-use, which is about 55%, and in terms of the systems usefulness, about 56%.","LowLevel":"big data;decision making;decision trees;learning algorithms;support vector machines;travel industry","MidLevel":"artificial intelligence;human factors;medical;data;transportation","HighLevel":"end users and user experience;technology;industries","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[155,4,90,70,160,385,486,136,163,289,79,410,86,208,118,178,102,382,109,96],"Top20Abs":[155,4,90,70,486,385,136,178,208,160,86,410,163,79,102,181,118,479,419,45],"Top20Tags":[352,4,289,426,368,378,315,64,96,90,416,76,13,485,392,230,308,172,373,282],"Citation":"Rezaee, S., Sadeghi-Niaraki, A., Shakeri, M., & Choi, S.-M. (2021). Personalized Augmented Reality Based Tourism System: Big Data and User Demographic Contexts. Applied Sciences, 11(13), 6047. https:\/\/doi.org\/10.3390\/app11136047\n"},{"Title":"P300 Brain-Computer Interface-Based Drone Control in Virtual and Augmented Reality","DOI":"10.3390\/s21175765","Publication year":2021,"Abstract":"Since the emergence of head-mounted displays (HMDs), researchers have attempted to introduce virtual and augmented reality (VR, AR) in brain-computer interface (BCI) studies. However, there is a lack of studies that incorporate both AR and VR to compare the performance in the two environments. Therefore, it is necessary to develop a BCI application that can be used in both VR and AR to allow BCI performance to be compared in the two environments. In this study, we developed an opensource-based drone control application using P300-based BCI, which can be used in both VR and AR. Twenty healthy subjects participated in the experiment with this application. They were asked to control the drone in two environments and filled out questionnaires before and after the experiment. We found no significant (p &gt; 0.05) difference in online performance (classification accuracy and amplitude\/latency of P300 component) and user experience (satisfaction about time length, program, environment, interest, difficulty, immersion, and feeling of self-control) between VR and AR. This indicates that the P300 BCI paradigm is relatively reliable and may work well in various situations.","LowLevel":"brain-computer interfaces;electroencephalography;helmet mounted displays;medical signal processing","MidLevel":"human factors;medical;data;wearables;sensors;farming and natural science;display technology","HighLevel":"displays;technology;industries;end users and user experience","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[200,210,384,455,237,383,60,398,238,207,163,64,75,91,399,414,69,87,152,306],"Top20Abs":[200,398,383,210,60,237,399,91,87,207,306,163,261,419,414,33,64,238,316,152],"Top20Tags":[455,384,250,417,108,80,217,180,279,21,206,253,230,238,38,75,364,424,301,69],"Citation":"Kim, S., Lee, S., Kang, H., Kim, S., & Ahn, M. (2021). P300 Brain\u2013Computer Interface-Based Drone Control in Virtual and Augmented Reality. Sensors, 21(17), 5765. https:\/\/doi.org\/10.3390\/s21175765\n"},{"Title":"AR in the Architecture Domain: State of the Art","DOI":"10.3390\/app11156800","Publication year":2021,"Abstract":"Augmented reality (AR) allows the real and digital worlds to converge and overlap in a new way of observation and understanding. The architectural field can significantly benefit from AR applications, due to their systemic complexity in terms of knowledge and process management. Global interest and many research challenges are focused on this field, thanks to the conjunction of technological and algorithmic developments from one side, and the massive digitization of built data. A significant quantity of research in the AEC and educational fields describes this state of the art. Moreover, it is a very fragmented domain, in which specific advances or case studies are often described without considering the complexity of the whole development process. The article illustrates the entire AR pipeline development in architecture, from the conceptual phase to its application, highlighting each step's specific aspects. This storytelling aims to provide a general overview to a non-expert, deepening the topic and stimulating a democratization process. The aware and extended use of AR in multiple areas of application can lead a new way forward for environmental understanding, bridging the gap between real and virtual space in an innovative perception of architecture.","LowLevel":"knowledge management;software architecture","MidLevel":"construction;developers;human resources","HighLevel":"industries;technology;business","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[233,54,87,372,67,2,228,41,134,448,279,370,122,47,178,82,410,184,207,146],"Top20Abs":[87,54,372,67,41,228,233,178,82,207,370,122,448,159,279,134,2,410,184,47],"Top20Tags":[17,334,34,44,140,68,87,403,392,105,231,289,406,370,84,94,78,146,335,132],"Citation":"Russo, M. (2021). AR in the Architecture Domain: State of the Art. Applied Sciences, 11(15), 6800. https:\/\/doi.org\/10.3390\/app11156800\n"},{"Title":"Employing Emerging Technologies to Develop and Evaluate In-Vehicle Intelligent Systems for Driver Support: Infotainment AR HUD Case Study","DOI":"10.3390\/app11041397","Publication year":2021,"Abstract":"The plurality of current infotainment devices within the in-vehicle space produces an unprecedented volume of incoming data that overwhelm the typical driver, leading to higher collision probability. This work presents an investigation to an alternative option which aims to manage the incoming information while offering an uncluttered and timely manner of presenting and interacting with the incoming data safely. The latter is achieved through the use of an augmented reality (AR) head-up display (HUD) system, which projects the information within the driver's field of view. An uncluttered gesture recognition interface provides the interaction with the AR visuals. For the assessment of the system's effectiveness, we developed a full-scale virtual reality driving simulator which immerses the drivers in challenging, collision-prone, scenarios. The scenarios unfold within a digital twin model of the surrounding motorways of the city of Glasgow. The proposed system was evaluated in contrast to a typical head-down display (HDD) interface system by 30 users, showing promising results that are discussed in detail.","LowLevel":"gesture recognition;head up displays;probability","MidLevel":"video;input;human factors;display technology","HighLevel":"displays;technology;end users and user experience","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[288,149,444,242,121,0,118,172,180,32,248,21,432,348,69,79,38,66,80,412],"Top20Abs":[288,149,444,0,242,248,118,121,486,120,432,412,45,172,38,440,4,177,82,175],"Top20Tags":[172,32,288,198,65,180,12,242,213,66,366,53,415,7,329,118,348,144,195,69],"Citation":"Charissis, V., Falah, J., Lagoo, R., Alfalah, S. F. M., Khan, S., Wang, S., Altarteer, S., Larbi, K. B., & Drikakis, D. (2021). Employing Emerging Technologies to Develop and Evaluate In-Vehicle Intelligent Systems for Driver Support: Infotainment AR HUD Case Study. Applied Sciences, 11(4), 1397. https:\/\/doi.org\/10.3390\/app11041397\n"},{"Title":"School of the Future: A Comprehensive Study on the Effectiveness of Augmented Reality as a Tool for Primary School Children's Education","DOI":"10.3390\/app11115277","Publication year":2021,"Abstract":"With the emerging technologies of augmented reality (AR) and virtual reality (VR), the learning process in today's classroom is much more effective and motivational. Overlaying virtual content into the real world makes learning methods attractive and entertaining for students while performing activities. AR techniques make the learning process easy, and fun as compared to traditional methods. These methods lack focused learning and interactivity between the educational content. To make learning effective, we propose to use handheld marker-based AR technology for primary school students. We developed a set of four applications based on students' academic course of primary school level for learning purposes of the English alphabet, decimal numbers, animals and birds, and an AR Globe for knowing about different countries around the world. These applications can be played wherever and whenever a user wants without Internet connectivity, subject to the availability of a tablet or mobile device and the required target images. These applications have performance evaluation quizzes (PEQs) for testing students' learning progress. Our study investigates the effectiveness of AR-based learning materials in terms of learning performance, motivation, attitude, and behavior towards different methods of learning. Our activity results favor AR-based learning techniques where students' learning motivation and performance are enhanced compared to the non-AR learning methods.","LowLevel":"computer aided instruction;internet;learning algorithms","MidLevel":"training;artificial intelligence;medical;networks","HighLevel":"industries;technology;use cases","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[320,369,87,114,185,57,39,306,207,383,262,134,244,62,296,115,124,342,136,82],"Top20Abs":[369,87,39,320,185,306,114,383,88,207,115,262,57,124,62,244,296,342,134,77],"Top20Tags":[330,251,376,244,159,363,51,268,320,298,195,232,204,9,364,262,34,10,93,185],"Citation":"Afnan, Muhammad, K., Khan, N., Lee, M.-Y., Imran, A., & Sajjad, M. (2021). School of the Future: A Comprehensive Study on the Effectiveness of Augmented Reality as a Tool for Primary School Children\u2019s Education. Applied Sciences, 11(11), 5277. https:\/\/doi.org\/10.3390\/app11115277\n"},{"Title":"Deep-Learning-Based Adaptive Advertising with Augmented Reality","DOI":"10.3390\/s22010063","Publication year":2021,"Abstract":"In this work we describe a system composed of deep neural networks that analyzes characteristics of customers based on their face (age, gender, and personality), as well as the ambient temperature, with the purpose of generating a personalized signal to potential buyers who pass in front of a beverage establishment; faces are automatically detected, displaying a recommendation using deep learning methods. In order to present suitable digital posters for each person, several technologies were used: Augmented reality, estimation of age, gender, and estimation of personality through the Big Five test applied to an image. The accuracy of each one of these deep neural networks is measured separately to ensure an appropriate precision over 80%. The system has been implemented into a portable solution, and is able to generate a recommendation to one or more people at the same time.","LowLevel":"advertising data processing;deep learning (artificial intelligence);face recognition","MidLevel":"artificial intelligence;other;human factors;data;medical;liberal arts;input;sales and marketing","HighLevel":"end users and user experience;business;industries;technology;other","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[381,460,453,255,283,387,144,166,12,401,95,116,81,18,379,102,125,425,143,115],"Top20Abs":[460,381,453,255,283,144,166,12,387,401,116,164,190,458,95,447,102,437,240,379],"Top20Tags":[95,18,379,368,415,115,428,387,13,308,81,255,425,381,271,143,280,182,451,40],"Citation":"Moreno-Armend\u00e1riz, M. A., Calvo, H., Duchanoy, C. A., Lara-C\u00e1zares, A., Ramos-Diaz, E., & Morales-Flores, V. L. (2021). Deep-Learning-Based Adaptive Advertising with Augmented Reality. Sensors, 22(1), 63. https:\/\/doi.org\/10.3390\/s22010063\n"},{"Title":"Augmented Reality Applications in Industry 4.0 Environment","DOI":"10.3390\/app11125592","Publication year":2021,"Abstract":"New technologies, such as cloud computing, the Internet of Things, wireless communications, etc., have already become part of our daily lives. This paper provides an insight into one of the new technologies, i.e., augmented reality (AR), as part of the manufacturing paradigm Industry 4.0 (I4.0). The aim of this paper is to contribute to the current state in the field of AR by assessing the main areas of the application of AR, the used devices and the tracking methods in support of the digitalization of the industry. Searches via Science Direct, Google Scholar and the Internet in general have resulted in the collection of a large number of papers. The examined works are classified according to several criteria and the most important data resulting from them are presented here. A comprehensive analysis of the literature has indicated the main areas of application of AR in I4.0 and, among these, those that stand out are maintenance, assembly and human robot collaboration. Finally, a roadmap for the application of AR in companies is proposed and the most promising future areas of research are listed.","LowLevel":"human-robot interaction;internet;production engineering computing","MidLevel":"manufacturing;networks;robotics;engineering","HighLevel":"industries;technology","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[448,315,449,19,205,447,110,289,2,174,352,179,429,45,359,227,122,137,5,298],"Top20Abs":[315,2,448,110,447,449,179,131,49,370,429,137,281,139,289,19,93,361,204,359],"Top20Tags":[407,19,205,255,174,70,298,330,289,388,449,370,352,251,163,429,411,175,227,122],"Citation":"Relji\u0107, V., Milenkovi\u0107, I., Dudi\u0107, S., \u0160ulc, J., & Baj\u010di, B. (2021). Augmented Reality Applications in Industry 4.0 Environment. Applied Sciences, 11(12), 5592. https:\/\/doi.org\/10.3390\/app11125592\n"},{"Title":"BlocklyXR: An Interactive Extended Reality Toolkit for Digital Storytelling","DOI":"10.3390\/app11031073","Publication year":2021,"Abstract":"Traditional in-app virtual reality (VR)\/augmented reality (AR) applications pose a challenge of reaching users due to their dependency on operating systems (Android, iOS). Besides, it is difficult for general users to create their own VR\/AR applications and foster their creative ideas without advanced programming skills. This paper addresses these issues by proposing an interactive extended reality toolkit, named BlocklyXR. The objective of this research is to provide general users with a visual programming environment to build an extended reality application for digital storytelling. The contextual design was generated from real-world map data retrieved from Mapbox GL. ThreeJS was used for setting up, rendering 3D environments, and controlling animations. A block-based programming approach was adapted to let users design their own story. The capability of BlocklyXR was illustrated with a use case where users were able to replicate the existing PalmitoAR utilizing the block-based authoring toolkit with fewer efforts in programming. The technology acceptance model was used to evaluate the adoption and use of the interactive extended reality toolkit. The findings showed that visual design and task technology fit had significantly positive effects on user motivation factors (perceived ease of use and perceived usefulness). In turn, perceived usefulness had statistically significant and positive effects on intention to use, while there was no significant impact of perceived ease of use on intention to use. Study implications and future research directions are discussed.","LowLevel":"computer aided instruction;computer games;cultural aspects;data visualization;human factors;mobile computing;rendering;visual programming","MidLevel":"training;human factors;policy;telecommunication;data;graphics;liberal arts;developers","HighLevel":"end users and user experience;use cases;business;industries;technology","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[185,62,30,57,312,45,87,53,246,216,118,306,135,414,79,145,279,200,154,195],"Top20Abs":[185,30,62,45,87,57,312,246,135,53,306,118,200,70,208,128,145,79,166,154],"Top20Tags":[147,204,164,376,51,93,82,254,236,56,327,22,196,89,42,320,97,145,13,154],"Citation":"Jung, K., Nguyen, V. T., & Lee, J. (2021). BlocklyXR: An Interactive Extended Reality Toolkit for Digital Storytelling. Applied Sciences, 11(3), 1073. https:\/\/doi.org\/10.3390\/app11031073\n"},{"Title":"VR and AR Restoration of Urban Heritage: A Virtual Platform Mediating Disagreement from Spatial Conflicts in Korea","DOI":"10.3390\/buildings11110561","Publication year":2021,"Abstract":"This study sought to uncover (1) the disagreement of spatial conflict between urban heritage and surrounding urban structure using two case studies from Korea&#8212;the main gate of the royal palace (Gwanghwamun) and the urban park containing celebrity graves (Hyoch'ang Park)&#8212;and (2) whether digital heritage restoration may mediate spatial conflict. A historical literature review and field surveys were conducted, with three main findings. First, the place identity of Gwanghwamun and Hyoch'ang Park, rooted in the Joso&#774;n Dynasty, was seriously damaged during the Japanese colonial period. Although there were national attempts to recover the place identities of these sites during the modern period, limitations existed. Second, the restoration of Gwanghwamun's Wo&#774;ltae (podium) and the relocation of U&#774;iyo&#774;lsa (the shrine of Hyoch'ang Park), which involved spatial transformation based on heritage, emerged in conflict with their surrounding urban structures&#8212;we identify a spatial conflict between local residents and stakeholders' memories and the histories of these sites. Third, Donu&#774;imun (the west gate of the city wall of the Joso&#774;n Dynasty) digital restoration is a case mediating the conflict by restoring a sense of place in a virtual space and activating the cultural memory of the public by showcasing properties.","LowLevel":"cultural aspects;history","MidLevel":"liberal arts;policy","HighLevel":"industries;business","Venue":"Buildings (Switzerland)","Top20AbsAndTags":[60,6,218,35,305,475,236,125,261,392,105,199,385,197,132,302,398,224,16,329],"Top20Abs":[60,218,6,305,475,125,392,35,261,199,398,385,246,224,329,236,132,31,310,251],"Top20Tags":[16,410,302,236,6,197,218,105,109,10,405,319,354,27,229,305,67,310,372,3],"Citation":"Youn, H.-C., & Ryoo, S.-L. (2021). VR and AR Restoration of Urban Heritage: A Virtual Platform Mediating Disagreement from Spatial Conflicts in Korea. Buildings, 11(11), 561. https:\/\/doi.org\/10.3390\/buildings11110561\n"},{"Title":"WARM: Wearable AR and Tablet-Based Assistant Systems for Bus Maintenance","DOI":"10.3390\/app11041443","Publication year":2021,"Abstract":"This paper shows two developed digital systems as an example of intelligent garage and maintenance that targets the applicability of augmented reality and wearable devices technologies to the maintenance of bus fleets. Both solutions are designed to improve the maintenance process based on verification of tasks checklist. The main contribution of the paper focuses on the implementation of the prototypes in the company's facilities in an operational environment with real users and address the difficulties inherent in the transfer of a technology to a real work environment, such as a mechanical workshop. The experiments have been conducted in real operation thanks to the involvement of the public transport operator DBUS, which operates public transport buses in the city of Donostia&#8212;San Sebastian (Spain). Two solutions have been developed and compared against the traditional process: one based on Tablet and another one based on Microsoft HoloLens. The results show objective metrics (Key Performance Indicators, KPI) as well as subjective metrics based on questionnaires comparing the two technological approaches against the traditional one based on manual work and paper.","LowLevel":"automotive engineering;maintenance engineering;road vehicles","MidLevel":"automotive;manufacturing","HighLevel":"industries","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[360,389,131,315,179,84,395,412,35,414,388,190,136,137,106,213,246,421,267,51],"Top20Abs":[360,389,315,131,179,84,414,395,412,136,388,213,165,371,137,439,63,106,35,440],"Top20Tags":[35,389,190,131,144,86,179,181,146,395,412,85,356,92,106,211,267,268,360,258],"Citation":"Borro, D., Suescun, \u00c1., Braz\u00e1lez, A., Gonz\u00e1lez, J. M., Ortega, E., & Gonz\u00e1lez, E. (2021). WARM: Wearable AR and Tablet-Based Assistant Systems for Bus Maintenance. Applied Sciences, 11(4), 1443. https:\/\/doi.org\/10.3390\/app11041443\n"},{"Title":"Review: Development and Technical Design of Tangible User Interfaces in Wide-Field Areas of Application","DOI":"10.3390\/s21134258","Publication year":2021,"Abstract":"A tangible user interface or TUI connects physical objects and digital interfaces. It is more interactive and interesting for users than a classic graphic user interface. This article presents a descriptive overview of TUI's real-world applications sorted into ten main application areas&#8212;teaching of traditional subjects, medicine and psychology, programming, database development, music and arts, modeling of 3D objects, modeling in architecture, literature and storytelling, adjustable TUI solutions, and commercial TUI smart toys. The paper focuses on TUI's technical solutions and a description of technical constructions that influences the applicability of TUIs in the real world. Based on the review, the technical concept was divided into two main approaches: the sensory technical concept and technology based on a computer vision algorithm. The sensory technical concept is processed to use wireless technology, sensors, and feedback possibilities in TUI applications. The image processing approach is processed to a marker and markerless approach for object recognition, the use of cameras, and the use of computer vision platforms for TUI applications.","LowLevel":"computer vision;graphical user interfaces;haptic interfaces;human computer interaction;image processing;object recognition;teaching;user interfaces","MidLevel":"education;human factors;computer vision;data;graphics;input;human-computer interaction","HighLevel":"industries;technology;end users and user experience","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[53,96,41,246,220,261,414,87,284,48,410,119,434,112,487,189,116,419,12,76],"Top20Abs":[53,96,41,220,410,284,261,87,246,116,189,207,290,434,48,487,412,12,120,194],"Top20Tags":[422,55,72,126,112,167,419,144,99,76,415,53,105,32,98,424,381,198,242,455],"Citation":"Krestanova, A., Cerny, M., & Augustynek, M. (2021). Review: Development and Technical Design of Tangible User Interfaces in Wide-Field Areas of Application. Sensors, 21(13), 4258. https:\/\/doi.org\/10.3390\/s21134258\n"},{"Title":"Mixed Reality-Enhanced Intuitive Teleoperation with Hybrid Virtual Fixtures for Intelligent Robotic Welding","DOI":"10.3390\/app112311280","Publication year":2021,"Abstract":"This paper presents an integrated scheme based on a mixed reality (MR) and haptic feedback approach for intuitive and immersive teleoperation of robotic welding systems. By incorporating MR technology, the user is fully immersed in a virtual operating space augmented by real-time visual feedback from the robot working space. The proposed robotic tele-welding system features imitative motion mapping from the user's hand movements to the welding robot motions, and it enables the spatial velocity-based control of the robot tool center point (TCP). The proposed mixed reality virtual fixture (MRVF) integration approach implements hybrid haptic constraints to guide the operator's hand movements following the conical guidance to effectively align the welding torch for welding and constrain the welding operation within a collision-free area. Onsite welding and tele-welding experiments identify the operational differences between professional and unskilled welders and demonstrate the effectiveness of the proposed MRVF tele-welding framework for novice welders. The MRVF-integrated visual\/haptic tele-welding scheme reduced the torch alignment times by 56% and 60% compared to the MRnoVF and baseline cases, with minimized cognitive workload and optimal usability. The MRVF scheme effectively stabilized welders' hand movements and eliminated undesirable collisions while generating smooth welds.","LowLevel":"collision avoidance;control engineering computing;force feedback;haptic interfaces;intelligent robots;motion control;robotic welding;telerobotics","MidLevel":"engineering;automotive;robotics;input;manufacturing;human-computer interaction;other","HighLevel":"industries;technology;other;end users and user experience","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[465,167,371,174,366,240,407,439,118,375,208,19,7,322,276,43,84,64,205,424],"Top20Abs":[167,465,371,174,240,375,366,208,407,439,118,19,43,335,7,64,357,276,15,303],"Top20Tags":[371,399,401,366,167,174,126,398,112,205,419,422,455,32,120,421,240,347,424,122],"Citation":"Su, Y.-P., Chen, X.-Q., Zhou, T., Pretty, C., & Chase, G. (2021). Mixed Reality-Enhanced Intuitive Teleoperation with Hybrid Virtual Fixtures for Intelligent Robotic Welding. Applied Sciences, 11(23), 11280. https:\/\/doi.org\/10.3390\/app112311280\n"},{"Title":"Using Mixed Reality (MR) to Improve On-Site Design Experience in Community Planning","DOI":"10.3390\/app11073071","Publication year":2021,"Abstract":"In recent years, designing in existing environments has been consistently emphasized in community planning. However, practicing such on-site design is not easy for designers, because the current technical conditions do not allow virtual design objects into real environments for 3D visualization and interaction. Thus, designers' intuitive design perceptions, accurate design judgments, and convenient design decisions are hardly supported. This paper explores the possibilities of using mixed reality (MR) technology to improve designers' on-site design experiences in community planning. For this, we introduced an MR design support system (MR-DSS) for the interactive on-site 3D visualization of virtual design objects. With the MR-DSS, we performed a design experiment with sixteen participants in a typical on-site design scene of community planning. The results showed that the MR technology could provide designers with intuitive design perceptions, accurate design judgments, and convenient design decisions, thus effectively improving their on-site design experiences.","LowLevel":"data visualization;design;solid modelling;town and country planning","MidLevel":"smart cities;data;business planning and management;manufacturing;human-computer interaction","HighLevel":"end users and user experience;use cases;business;industries;technology","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[407,439,365,227,224,114,395,419,329,397,424,319,412,246,411,400,58,388,197,178],"Top20Abs":[407,224,419,365,439,114,395,227,329,397,424,197,58,246,319,92,6,400,356,388],"Top20Tags":[181,310,329,411,178,135,280,366,53,345,111,336,380,412,161,136,397,340,227,385],"Citation":"Dan, Y., Shen, Z., Zhu, Y., & Huang, L. (2021). Using Mixed Reality (MR) to Improve On-Site Design Experience in Community Planning. Applied Sciences, 11(7), 3071. https:\/\/doi.org\/10.3390\/app11073071\n"},{"Title":"ATARI: a graph convolutional neural network approach for performance prediction in next-generation WLANs","DOI":"10.3390\/s21134321","Publication year":2021,"Abstract":"IEEE 802.11 (Wi-Fi) is one of the technologies that provides high performance with a high density of connected devices to support emerging demanding services, such as virtual and augmented reality. However, in highly dense deployments, Wi-Fi performance is severely affected by interference. This problem is even worse in new standards, such as 802.11n\/ac, where new features such as Channel Bonding (CB) are introduced to increase network capacity but at the cost of using wider spectrum channels. Finding the best channel assignment in dense deployments under dynamic environments with CB is challenging, given its combinatorial nature. Therefore, the use of analytical or system models to predict Wi-Fi performance after potential changes (e.g., dynamic channel selection with CB, and the deployment of new devices) are not suitable, due to either low accuracy or high computational cost. This paper presents a novel, data-driven approach to speed up this process, using a Graph Neural Network (GNN) model that exploits the information carried in the deployment's topology and the intricate wireless interactions to predict Wi-Fi performance with high accuracy. The evaluation results show that preserving the graph structure in the learning process obtains a 64% increase versus a naive approach, and around 55% compared to other Machine Learning (ML) approaches when using all training features.","LowLevel":"channel allocation;computer network performance evaluation;graph theory;learning algorithms;neural networks;telecommunication network topology;wireless lan","MidLevel":"artificial intelligence;telecommunication;medical;business performance metrics;input;geospatial;networks;other","HighLevel":"industries;technology;business;other","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[231,265,446,423,426,469,268,341,325,404,163,484,476,172,215,144,267,316,116,209],"Top20Abs":[231,265,423,469,446,426,268,341,325,484,404,267,476,215,172,480,163,316,318,369],"Top20Tags":[265,231,428,271,426,209,28,268,423,266,427,385,18,160,446,346,95,163,116,144],"Citation":"Soto, P., Camelo, M., Mets, K., Wilhelmi, F., G\u00f3ez, D., Fletscher, L. A., Gaviria, N., Hellinckx, P., Botero, J. F., & Latr\u00e9, S. (2021). ATARI: A Graph Convolutional Neural Network Approach for Performance Prediction in Next-Generation WLANs. Sensors, 21(13), 4321. https:\/\/doi.org\/10.3390\/s21134321\n"},{"Title":"VR Education Support System&#8212;A Case Study of Digital Circuits Design","DOI":"10.3390\/en15010277","Publication year":2021,"Abstract":"Areas of experience allow for the acquisition and consolidation of both existing knowledge and skills. These are significant factors in the training of staff members for companies in the Industry 4.0 area. One of the currently available modern tools used in the teaching process is virtual reality (VR) technology. This technology, due to its high level of immersion and involvement of the different senses, and the need to focus on the performed activities, allows one to develop skills in solving various tasks and problems. The extended VR environment enables the creation of diverse teaching scenarios adapted to the needs of industry. This paper presents the possibility of building training scenarios in the field of digital techniques. The software solution, developed and presented by the authors, uses elements of computer game mechanics and is designed to familiarize students with the idea of digital circuits, their construction, logical implementation and application. This paper also presents a comparison of the features of different forms of education used in teaching digital techniques, as well as a comparison of these forms, from the point of view of the student and his\/her perceptions.","LowLevel":"computer aided instruction;computer displays;computer games;teaching;user interfaces","MidLevel":"education;training;liberal arts;human-computer interaction;display technology","HighLevel":"displays;industries;use cases;end users and user experience","Venue":"Energies (Switzerland)","Top20AbsAndTags":[363,372,136,87,306,232,312,51,82,204,150,112,47,457,342,196,320,246,128,17],"Top20Abs":[306,312,207,188,128,457,87,363,232,261,82,196,46,372,150,146,10,47,136,204],"Top20Tags":[204,164,33,238,72,51,363,405,372,410,147,244,82,248,17,236,59,145,3,222],"Citation":"Paszkiewicz, A., Salach, M., Strza\u0142ka, D., Budzik, G., Nikodem, A., W\u00f3jcik, H., & Witek, M. (2021). VR Education Support System\u2014A Case Study of Digital Circuits Design. Energies, 15(1), 277. https:\/\/doi.org\/10.3390\/en15010277\n"},{"Title":"Development of interactive learning multimedia for mathematics subjects for grade 5 elementary schools","DOI":"10.1088\/1742-6596\/1987\/1\/012017","Publication year":2021,"Abstract":"The learning process during a pandemic requires teachers to be more interactive in making online learning media. The government has designed the Covid Emergency Curriculum, which is that educational units in their current condition can choose one of the several components needed in the learning process, namely still referring to the national curriculum, using the emergency curriculum, or making curriculum simplification independently. For curriculum simplification, only the main material in learning activities needs to be conveyed, while for practice it can be independently or through an application media. This study aims to create an augmented reality-based learning media on geometry in mathematics subjects for grade 5 elementary school. This media can make it easier for students to understand the shapes with augmented reality and there is a learning video feature that also explains the material presented by the teacher. With the black box white box method, the application is developed by adjusting the curriculum simplification planned by the government. Features in the application include objectives and materials, learning videos, augmented reality simulations and integrated with Google Classroom as a learning evaluation platform. The results of this research are digital applications that can be accessed from the web or android applications.","LowLevel":"computer aided instruction;educational courses;educational institutions;epidemics;medical computing","MidLevel":"education;medical;training","HighLevel":"industries;use cases","Venue":"J. Phys., Conf. Ser. (UK)","Top20AbsAndTags":[39,134,114,163,77,342,9,320,185,207,244,369,442,171,306,161,296,145,124,87],"Top20Abs":[39,114,134,163,77,342,9,185,207,320,442,244,369,161,306,296,87,171,116,150],"Top20Tags":[320,145,241,376,9,47,61,171,390,89,134,56,93,391,405,124,103,3,136,82],"Citation":"Firmansyah, F. H., Sari, I. P., Permana, F. C., & Rinjani, D. (2021). Development of interactive learning multimedia for mathematics subjects for grade 5 elementary schools. Journal of Physics: Conference Series, 1987(1), 012017. https:\/\/doi.org\/10.1088\/1742-6596\/1987\/1\/012017\n"},{"Title":"Network Analysis for Learners' Concept Maps While Using Mobile Augmented Reality Gaming","DOI":"10.3390\/app11219929","Publication year":2021,"Abstract":"Using mobile augmented reality games in education combines situated and active learning with pleasure. The aim of this research is to analyze the responses expressed by young, middle-aged, and elderly adults about the location-based mobile augmented reality (MAR) games using methods of content analysis, concept maps, and social network analysis (SNA). The responses to questions related to MAR game Ingress were collected from 36 adult players, aged 20-60, from Greece, and subsequently analyzed by means of content analysis, concept maps, and social network analysis. Our findings show that for question 1 (How do you feel when you endow the geographical space with personal preferences?), there was a differentiation of the answers between age groups with age groups agreeing in pairs, the first two and the last two, while for question 2 (Do you think that the game offers opportunities for learning and teaching geography, building on your previous geographical knowledge?), there was an overlap in responses of participants among age groups. It was also revealed that the MAR games foster a constructivism approach of learning, as their use learning becomes an active, socially supported process of knowledge construction.","LowLevel":"computer aided instruction;computer games;mobile computing;social networking;teaching","MidLevel":"education;training;telecommunication;collaboration;liberal arts","HighLevel":"industries;use cases","Venue":"Appl. Sci. (Switzerland)","Top20AbsAndTags":[147,36,111,369,83,185,264,320,115,113,400,236,30,246,79,114,363,414,163,383],"Top20Abs":[147,36,111,369,83,264,185,400,115,113,163,383,320,79,246,302,102,26,236,134],"Top20Tags":[147,204,363,51,3,82,410,93,193,56,14,89,414,222,249,103,17,33,219,376],"Citation":"Sdravopoulou, K., Mu\u00f1oz Gonz\u00e1lez, J. M., & Hidalgo-Ariza, M. D. (2021). Network Analysis for Learners\u2019 Concept Maps While Using Mobile Augmented Reality Gaming. Applied Sciences, 11(21), 9929. https:\/\/doi.org\/10.3390\/app11219929\n"},{"Title":"A Framework for Automatic Generation of Augmented Reality Maintenance &amp; Repair Instructions based on Convolutional Neural Networks","DOI":"10.1016\/j.procir.2020.04.130","Publication year":2020,"Abstract":"In modern manufacturing world, MRO (Maintenance and Repair Operations) are the cornerstone for keeping industrial equipment in near-optimum condition. Successful completion of MRO has been benefited from Augmented Reality (AR), by considerably decreasing MTTR (Mean Time to Repair). To that end, AR delivers the digital tools that help on-the-field technicians to perform MRO easily and intuitively, however intense development is required for generating AR instructions. The latest advances in computer technologies, concretely in Convolutional Neural Networks, have enabled advanced computer vision. This research paper presents a framework for generating AR maintenance instructions, based on advanced computer vision and Convolutional Neural Networks (CNN). The applicability of the framework is tested in-vitro in a lab-based machine shop. [All rights reserved Elsevier].","LowLevel":"building management systems;computer vision;machine shops;maintenance engineering;neural networks","MidLevel":"education;artificial intelligence;computer vision;construction;manufacturing;other","HighLevel":"industries;technology;other","Venue":"Procedia CIRP (Netherlands)","Top20AbsAndTags":[131,144,110,179,35,389,53,137,315,86,261,220,160,135,2,360,51,69,453,45],"Top20Abs":[315,131,137,389,179,110,53,135,35,2,51,144,220,261,160,69,395,86,45,447],"Top20Tags":[144,86,35,370,389,131,115,172,261,90,476,179,157,116,387,106,220,480,334,110],"Citation":"Mourtzis, D., Angelopoulos, J., & Panopoulos, N. (2020). A Framework for Automatic Generation of Augmented Reality Maintenance &amp; Repair Instructions based on Convolutional Neural Networks. Procedia CIRP, 93, 977\u2013982. https:\/\/doi.org\/10.1016\/j.procir.2020.04.130\n"},{"Title":"Compact optical system displaying mid-air images movable in depth by rotating light source and mirror","DOI":"10.1016\/j.cag.2020.08.006","Publication year":2020,"Abstract":"Mid-air imaging technology enables computer graphics (CG) images to move around in the real world. However, the big form-factor of large-format mid-air displays makes them bulky and limits their applicability. To reduce the size of such displays, we propose an optical design that realizes mid-air image movement by rotating the reflecting surface and the light source with motors and thus moving the virtual image of the light source. We made a prototype based on the calculation of the mirror and display angles and the distance between them and evaluated the luminance, sharpness, and position of the mid-air image of this prototype. We confirmed that the size of the prototype was smaller than that produced by the previous method, which will allow a smaller form-factor for creating mid-air images for different applications. [All rights reserved Elsevier].","LowLevel":"computer graphics;human computer interaction;interactive devices;light sources;mirrors;optical design techniques;reflectivity;display screens","MidLevel":"graphics;chemical;input;optics;human-computer interaction;display technology","HighLevel":"industries;technology;displays;end users and user experience","Venue":"Comput. Graph. (Netherlands)","Top20AbsAndTags":[393,66,153,106,169,141,437,413,332,248,319,201,76,74,344,343,225,235,278,99],"Top20Abs":[393,66,106,141,153,169,437,201,248,76,225,332,413,74,344,278,99,20,235,430],"Top20Tags":[72,293,343,126,167,421,419,32,424,415,98,99,148,248,48,313,112,255,263,379],"Citation":"Osato, Y., & Koizumi, N. (2020). Compact optical system displaying mid-air images movable in depth by rotating light source and mirror. Computers &amp; Graphics, 91, 290\u2013300. https:\/\/doi.org\/10.1016\/j.cag.2020.08.006\n"},{"Title":"Data-driven and AR assisted intelligent collaborative assembly system for large-scale complex products","DOI":"10.1016\/j.procir.2020.04.041","Publication year":2020,"Abstract":"With the continuous improvement of function diversity, quality and reliability, the complexity and scale of significant products such as aerospace equipment are increasing. This trend poses severe challenges to the assembly process. The large scale of products requires the fixed-position assembly with multiple roles of workers in parallel, and it increases the difficulty of multi-person assembly. The high complexity of products makes the assembly process tedious. It is time-consuming and laborious for workers to understand the 2D documents, and it easily leads to mis-operation. The assembly process of complex products can produce huge amounts of data, which have the potential for improving efficiency and quality but remain underutilized. The high reliability of products puts forward high requirements for real-time detection of the assembly process. With the development of information technology, data driven approach and Augmented Reality (AR) provide an efficient way to change the above situation. Therefore, this paper proposes an intelligent collaboration assembly system (ICAS) combining real-time data driven method with AR technology. In the ICAS, real-time data driven method is used to realize the human-to-human and human-to-machine collaboration under the fixed-position assembly, and AR is introduced to the system to assist the assembly of large-scale complex products. The intelligent detection method in the ICAS combines 3D reconstruction with sensors measurement data, and the detection result is displayed in an intuitive way by AR. The proposed system is verified in the assembly process of a large-scale space deployable mechanism, and it paves the way for the further development of intelligent assembly in the future. [All rights reserved Elsevier].","LowLevel":"assembling;image reconstruction;production engineering computing","MidLevel":"manufacturing;construction;computer vision;engineering","HighLevel":"industries;technology","Venue":"Procedia CIRP (Netherlands)","Top20AbsAndTags":[137,429,12,81,157,172,159,486,252,406,178,227,49,233,376,345,116,205,340,311],"Top20Abs":[137,429,157,81,12,486,159,172,49,252,178,406,376,4,345,227,205,311,175,116],"Top20Tags":[388,12,407,318,360,143,225,45,130,77,159,340,181,205,449,73,419,411,379,179],"Citation":"Liu, X., Zheng, L., Shuai, J., Zhang, R., & Li, Y. (2020). Data-driven and AR assisted intelligent collaborative assembly system for large-scale complex products. Procedia CIRP, 93, 1049\u20131054. https:\/\/doi.org\/10.1016\/j.procir.2020.04.041\n"},{"Title":"A fire reconnaissance robot based on SLAM position, thermal imaging technologies, and AR display","DOI":"10.3390\/s19225036","Publication year":2019,"Abstract":"Due to hot toxic smoke and unknown risks under fire conditions, detection and relevant reconnaissance are significant in avoiding casualties. A fire reconnaissance robot was therefore developed to assist in the problem by offering important fire information to fire fighters. The robot consists of three main systems, a display operating system, video surveillance, and mapping and positioning navigation. Augmented reality (AR) goggle technology with a display operating system was also developed to free fire fighters' hands, which enables them to focus on rescuing processes and not system operation. Considering smoke disturbance, a thermal imaging video surveillance system was included to extract information from the complicated fire conditions. Meanwhile, a simultaneous localization and mapping (SLAM) technology was adopted to build the map, together with the help of a mapping and positioning navigation system. This can provide a real-time map under the rapidly changing fire conditions to guide the fire fighters to the fire sources or the trapped occupants. Based on our experiments, it was found that all the tested system components work quite well under the fire conditions, while the video surveillance system produces clear images under dense smoke and a high-temperature environment; SLAM shows a high accuracy with an average error of less than 3.43%; the positioning accuracy error is 0.31 m; and the maximum error for the navigation system is 3.48%. The developed fire reconnaissance robot can provide a practically important platform to improve fire rescue efficiency to reduce the fire casualties of fire fighters.","LowLevel":"emergency management;emergency services;fires;infrared imaging;navigation;position control;rescue robots;robot vision;slam robotics;smoke;video surveillance","MidLevel":"emergency response;computer vision;robotics;security;navigation;sensors;input;human-computer interaction","HighLevel":"industries;technology;use cases;end users and user experience","Venue":"Sensors (Switzerland)","Top20AbsAndTags":[139,326,70,63,274,472,432,488,174,49,345,101,123,43,19,371,186,76,322,433],"Top20Abs":[139,174,472,326,488,123,432,371,345,274,43,267,63,70,131,101,19,137,76,433],"Top20Tags":[396,220,381,385,425,399,70,274,173,130,72,63,300,379,120,358,398,370,277,420],"Citation":"Li, Feng, Niu, Shi, Wu, & Song. (2019). A Fire Reconnaissance Robot Based on SLAM Position, Thermal Imaging Technologies, and AR Display. Sensors, 19(22), 5036. https:\/\/doi.org\/10.3390\/s19225036\n"}]